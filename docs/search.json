[
  {
    "objectID": "PSYC121/Week8.html",
    "href": "PSYC121/Week8.html",
    "title": "Related-samples t-tests, plotting means and SE bars",
    "section": "",
    "text": "Today we will take a look at summarising means and standard errors (SEs) from our data. We will look at how we plot these together on the one graph (using ggplot() commands that allow us to share mappings between different geoms. We will explore our data on the famous “Stroop Task” and we will use a related-samples t-test to examine the differences between the means of our different conditions in this task."
  },
  {
    "objectID": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "href": "PSYC121/Week8.html#pre-lab-work-online-tutorial",
    "title": "Related-samples t-tests, plotting means and SE bars",
    "section": "Pre-lab work: online tutorial",
    "text": "Pre-lab work: online tutorial\nOnline tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\n\nCreate a folder for Week 8 and download the Week_8.zip file and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week8.html#rstudio-tasks",
    "href": "PSYC121/Week8.html#rstudio-tasks",
    "title": "Related-samples t-tests, plotting means and SE bars",
    "section": "RStudio Tasks",
    "text": "RStudio Tasks\n\nCalculating means and SEs\nThe “Stroop Effect” is a classic demonstration of automaticity of behaviour. Participants have to say the colour a word is printed in, which is an easy task for a “compatible” stimulus like GREEN, and a much more difficult task for an “incompatible” stimulus like BLUE. We can’t help but read the text - it has seemingly become an automatic process.\n In this task we will calculate the means and standard errors of the means and then we will then plot them using ggplot(). First though, we’ll need to inspect the data and maybe do a bit of data wrangling by using our filter() command.\n\nOpen the script “Week_8_script.R” (see prep work)\nRun the library and add a read_csv line to read in the data set. Call your data something meaningful (perhaps data_w8 or data_stroop but maybe not bestest_most_fantastic_data_on_the_stroop_test_eva_init)\nView the data with View(data). You will see that the data are a little different from the data we have worked with previously. We have an pID variable, which gives a unique number for each person and each person has 3 rows. This is because the different conditions of the Stroop task reflect a within-subjects variable (related samples). For data like this it is often useful to have them arranged in what is referred to as “long format”, with multiple rows for each response the participant provides. For the current data that means we have a variable called condition, which is our IV, and one called time which is our DV.\nLet’s look at the distribution of time (our DV) as a function of condition. Complete the next chunk of code by mapping x to time and fill to condition for our geom_density() plot. You can play around with the alpha parameter, setting it to a value between 0 and 1 - note that this is done OUTSIDE of the aes() command.\nWe seem to have some outlier values at both the high and the low ends. It’s probably best if we remove data for the whole participant if their average time is unusual. To do that, we’ll need to create a new column. Here we will introduce you to a new function called mutate(). This function will create a new variable (column) from an old one. Run this next block of code (you don’t need to edit this one) to create the new column, avg_time. We are using this in combination with group_by(pID) because we want to calculate the average time for each participant. Use view() to take a look at the data to check the column has been created correctly.\nEdit the geom_histogram() code to plot the distribution of values in the new avg_time column.\nWe now need the filter out the values in our data that we feel are unusual. Like last week, we will do this (for now) in a fairly unprincipled manner, by “eyeballing” the data (next week we’ll consider something a bit more “scientific”). Complete the filter command so that it removes both the very low values in the avg_time column, and also those that are very high. Because you want to filter out low AND high values, you are using an AND expression (&). You will therefore need to enter in two numerical values, based on your assessment of the histogram produced for Q6. Note that you need to think about how you are storing the result of this filter process. Do you want to create a new object, or overwrite the existing object?\nCheck the new distribution of average times after this filter has been applied to the data.\n\n\n\nRunning related samples t-tests\nWe have seen in our density plots that the reaction times (DV) look different in the three different Stroop conditions (our IV). But now we need to look at whether there are statistically significant differences between the means of the three conditions.\nTo do this, we will first summarise the mean time taken by each condition in the Stroop task. Remember from Week 3 that we can use group_by() and summarise() to get summary stats (e.g., mean, sd) at each level of the IV. That’s what we want to do now:\n\nEdit the group_by() code to specify the IV and the summarise() to calculate the mean() of our DV. If you do this correctly, you’ll get three values - a mean value for each level (condition) of our IV. Do these means reflect what you would expect in the Stroop task?\nNext we need to test if these differences between our means are real. To do that, we can run a related samples t-test; remember that the data for each level of the IV in this experiment came from the same person. We must use a filter() to restrict the data to just two levels of the IV: the condition column/variable in the data. This is because a related samples t-test looks at the difference between two means (and only two), so the column we use for the t-test needs to have just two levels of the IV (two of the conditions).\nThe filter command is already set up to restrict the data to two of the conditions. Note that the filter uses an “|” symbol, which means “or”, because we want the data that is the same as (==) one condition OR is the same as (==) the other condition.\nRun the t-test on this selection of data, to compare the means from these two levels of the IV. Is the result significant? Note the t-value and the p-value.\nWrite out a statement to express this result. Here’s a template you can use, where you need to edit the bits in the []:\n\n\n“There [was a / was no] significant difference between the [describe the variables that were compared], t([degrees of freedom here]) = [t value here], p &lt; [p value here].”\n\n\nWith 3 levels to the IV condition there are 3 possible comparisons we can make (1 vs. 2; 1 vs. 3; 2 vs. 3). Complete all three tests, by copying and pasting the commands, editing each to make a different filter selection, and then to run the t-test. Write out a report statement (Q5) for each of your comparisons.\n\n\n\nPlotting the means and SEs\n\nIn Task 2 you calculated the means for each condition in the Stroop task. We’ve seen in lectures that “standard error” provides an estimate of how variable that mean will be across the samples we collect. A very typical way to plot a mean value is to plot it with the standard error of the mean (SEM):\n\n\n\nThe code from Task 2-Q1 will give the mean. We will now complete the second line of this code to give the standard error values. TO do this, you simply need to add the correct variable (DV) to the sd() command to calculate the SE values (note that you don’t need to put anything in n(), as this simply calculates how many rows there are).\nView the new object summary object you have created. Check that the means and SEs are different for the 3 conditions. If they are the same, you probably summarised the wrong column!\nWe will now plot these 3 mean values in a figure. Let’s use geom_point() so that our means and SEs look a bit like the figure above. Complete the ggplot command to plot our summarised value called stroop_mean (y), as a function of the IV, condition (x).\nNow we need to add some “error bars” which provide a visual guide as to how much uncertainty we have in our mean value. Edit the code for the ggplot() command to plot both geom_point() (same as Q4) and geom_errorbar. You will need to calculate a ymin and a ymax value. Use the illustration above to work out how to combine the mean value and the SE value (hint: add or subtract) to create the right ymin and ymax.\n\nEXTRA: These next steps can be completed to practice customising your plot\n\nAdd a labs() layer to the plot to change the axis titles, and the title of the plot.\nChange the theme of the plot (e.g., theme_void())\nMap the fill aesthetic to the variable stroop_condition. You can do this for geom_point or geom_errorbar or both at once by putting it in the aes() within the ggplot() command.\nManually change the colours of the columns with using scale_fill_brewer(). Take a look at the Week 6 (6.3.7) for instructions on setting colours.\nTry changing your geom_point() to geom_col.\n\n\n\nSaving your work\nScripts: By now you are hopefully getting used to editing and working within the script. As you know, to save a script, you simply click the save icon, or press ctrl+S (cmd+s on a mac).\nPlots: To save a graph you have produced, click the “Export” button in the plot window, then “Save as Image”. You can resize the graph and give it an appropriate filename. If you’ve set the working directory correctly, then the new file should appear in the current folder.\nData: The data objects you create (in the Environment) only exist within RStudio, and are temporary (with a script and the csv file, you can always redo the analysis). But what if you want to use the data elsewhere? For example you may want to share the data with your project (PEP?) supervisor. To do this, we need to write the data to a csv file (like those we use to import the data). You can do this with the following command: write_csv(the_data_object, \"the_filename.csv\").\nExporting from RStudio: The above save operations save files to a folder within RStudio Server. At some stage you will need to get these files out of RStudio Server, for example if you need a graph for your report, or you need to share the data or the scripts. Or maybe you want to make the csv file available to other researchers. To get files out of RStudio, simply select the files you want in the Files pane, click “More” and then “Export”. Selecting multiple files will produce a “.zip” file, which will need to be “unzipped” on your computer to access the individual files (instructions for Windows and instructions for Mac)"
  },
  {
    "objectID": "PSYC121/Week8.html#week-8-quiz",
    "href": "PSYC121/Week8.html#week-8-quiz",
    "title": "Related-samples t-tests, plotting means and SE bars",
    "section": "Week 8 Quiz",
    "text": "Week 8 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/Week9.html",
    "href": "PSYC121/Week9.html",
    "title": "Unrelated-samples t-test and Power",
    "section": "",
    "text": "Online tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\nCreate a folder for Week 9. and download the week_9.zip file and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week9.html#pre-lab-work-online-tutorial",
    "href": "PSYC121/Week9.html#pre-lab-work-online-tutorial",
    "title": "Unrelated-samples t-test and Power",
    "section": "",
    "text": "Online tutorial: You must make every attempt to complete this before the lab! To access the pre-lab tutorial click here (on campus, or VPN required)\nGetting ready for the lab class\nCreate a folder for Week 9. and download the week_9.zip file and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week9.html#rstudio-tasks",
    "href": "PSYC121/Week9.html#rstudio-tasks",
    "title": "Unrelated-samples t-test and Power",
    "section": "RStudio tasks",
    "text": "RStudio tasks\n\nExploring the data\nIn this class we will be exploring some data on people’s estimations on aspects of the UK population. We asked people 4 different questions:\n\nOut of every 100 people, about how many do you think are:\n\n\n\nChristian?\nMuslim?\nOver the age of 65?\n\n\nWe also asked a related question about immigration:\n\nWhat percentage of the UK population do you think are immigrants to this country? (i.e. not born in UK)\n\n\nRead in the data “data_wk9.csv” using read_csv().\nTake a look at the summary statistics for all of the columns in our data using summary()\nAt this stage you could try using ggplot() with geom_histogram() (hint: not sure? look at last week) or geom_density() to explore the different columns.\nYou may be tempted at this stage to apply the filter() commands to remove any outliers. If so, yes, that’s entirely sensible. But note that we are going to remove outliers a little further into our tasks after we’ve first done some visualisation.\n\n\n\nVisualising relationships in the data\nTo what extent are people’s estimations of these population parameters related? Let’s look at this by plotting these data as geom_point():\n\nComplete the code to add one numeric column in the data to x and another numeric column to y. You can pick any of the columns you like, but it’s important that you understand what research question you are asking with your choice. For example, you might be asking “Do people who estimate there are more Christians in the population also think there are more people over the age of 65?”\nNote the general pattern in the data. Is there a postive relationship: do people who give high estimations for one variable tend to give high estimations for the other variable? Or is there a negative relationship: do people who give high estimations for one variable tend to give lower estimations for the other variable? Or is there no discernable relationship at all?\nCopy the code and edit it to explore relationships between the other numeric variables, each time noting the research question you are asking, and discussing on your table what kind of relationship you can see in the data.\n\n\n\nUsing z-scores to remove outliers\n\nYou may have noticed that there are some fairly extreme values in some of these numeric estimations of the population. As we’ve discussed in previous weeks, these outlier values can be problematic when we run our statistical tests, so (like last week) we probably want to control their influence by removing them. As you saw in your online tutorial, we can convert the data to z scores, and then remove z values above and below certain values.\nLet’s create a “z-transform column” called z_imm for the estimates of the percentage of immigrants. Complete the code you have been given by adding the relevant variable (column) name to the code. You may want to create a new data object at this point.\nView the new data object to check this column has been created correctly. Like in the online tutorial, it would be a good idea to calculate some descriptive statistics for these new columns to check it conforms to what we know about z-scores (e.g., mean() should be very close to 0, sd() = 1). Note, if you want to change the output in r from scientific notation to non-scientific notation, you can run the command options(scipen=999).\nWe know from our lectures on the z distribution that values of greater than 2 (or less than -2) reflect around 5% of the distribution, and values greater than 3 (or less than -3) represent less than 1% of the distribution:\n\n\n\nLet’s consider an outlier any value that has a z of 2.5 (a conventional cutoff). Plot a histogram of the z_imm column in order to inspect whether there are data that are above 2.5 or below -2.5.\nAdd a filter command to remove the values in the z_imm column are greater than 2.5 or less than -2.5."
  },
  {
    "objectID": "PSYC121/Week9.html#unrelated-samples-t-test",
    "href": "PSYC121/Week9.html#unrelated-samples-t-test",
    "title": "Unrelated-samples t-test and Power",
    "section": "Unrelated samples t-test",
    "text": "Unrelated samples t-test\nWe have also included a categorical variable in our data this week, which is one you have seen before in our analysis classes: the home location in the UK of the respondent, home_location_in_UK. For this data object we have included only those responses from those people from the “North” (NW and NE) and those from the “South” (SW and SE). Other respondents from elsewhere have been removed from the data. We can therefore look at whether people’s home location determines their population estimations.\n\nFirst we will look at the mean population estimations, split by home location. To do this, complete the group_by() and summarise() commands to give the mean estimates of the proportion of muslims in the population by home location. You don’t need to edit the N = n() line - this provides the number of participants at each level of the home_location_in_UK variable.\nWhat do the means suggest? Do people in the North and South give different estimations?\nLet’s test if these differences are real. First, it is worth noting that many more respondents originate from the North than from the South (see the N column in the summary). We have unequal sample sizes, and potentially unequal variances. Run the var.test() code to check if the variances of the two samples are similar (homogeneity of variance). If this test produces a p value less that .05, then the variances in the two samples are unequal. That will have consequences for how we run the t-test() in the next step.\nNow let’s run the t-test. This week we are comparing data from different samples of participants (those who are from the North and South). We need to tell the t-test that the data are NOT paired (paired = FALSE). The result of the var.test() in the last step will tell you whether the var.equal value should be TRUE or FALSE. Set var.equal = FALSE or var.equal = TRUE depending on whether the variances are equal. When you’re happy with the parameters, run the t-test. What result did you get and what does this mean? Discuss this with your table, or with the staff in the lab.\nIn that t-test we looked at the pop_est_muslim variable, but we can do this test for all of our population estimates. Copy the code to run another var.test() and t.test(), for the pop_est_christian variable. Note the t and p values; What do these tell us about the relationship between music preference and social media use? You can also run two further tests on the other population estimations."
  },
  {
    "objectID": "PSYC121/Week9.html#power-and-effect-size-d-calculations",
    "href": "PSYC121/Week9.html#power-and-effect-size-d-calculations",
    "title": "Unrelated-samples t-test and Power",
    "section": "Power and effect size (d) calculations",
    "text": "Power and effect size (d) calculations\n\nWe saw in last week’s lab tasks that there was a significant effect in our Stroop task data: participants were faster to say the colour names of the compatible list compared to the incompatible list (there were significant differences with the control list too). We will now use these data to calculate an effect size (Cohen’s d) for the t-statistic that we observed in that test.\nImport the stroop data. We have reduced the data down to just the compatible and incompatible conditions.\nRun the cohens_d() code to calculate the effect size, which is reported as effsize. You can ignore any negative sign, taking note of the absolute value.\nWe already know that this large effect size was significant with our large sample of participants. What might we have expected with a much smaller sample size? Use the pwr.t.test() function to add in the effect size that you calculated (Cohen’s d) and set the N to 20. What power would we have achieved with this sample size, to detect this large effect? Discuss with your table, or staff, what this power means.\nLet’s say we wanted our next experiment to have an 80% chance of finding an effect at least as large as the one we found. Complete the pwr.t.test() function to work out the minimum sample size we would need to achieve power of .8, with this effect size.\nLet’s say we are looking to run a new experiment in which we give people a stressful task to complete simultaneously. We will ask them to put their hands in a bucket of ice cold water while doing the Stroop task (this is a real “stressor task” people use!). We are unsure of what consequence this will have for our effect size, but we want to estimate the effect size that could be detected in our experiment. We decide to run 40 participants, and want to achieve a power of .90 (90% chance to find an effect at least this large). What is the minimum effect size we could hope to detect under these conditions?"
  },
  {
    "objectID": "PSYC121/Week9.html#week-9-quiz",
    "href": "PSYC121/Week9.html#week-9-quiz",
    "title": "Unrelated-samples t-test and Power",
    "section": "Week 9 Quiz",
    "text": "Week 9 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/Week1.html",
    "href": "PSYC121/Week1.html",
    "title": "Introduction to PSYC121",
    "section": "",
    "text": "For each lab session that you have been scheduled to attend, please come “prepared’. By this we mean\n\nYou have watched the lecture video, made notes, and developed questions where you have them.\nYou have worked through the online “prelab” preparation materials - Here is the one for Week 1\nYou read the lab sheet (this one) to get a sense of what we’re going to be doing, and you anticipate potential problems so that you can focus on these in the session.\n\nThe lecture is designed to deliver important ideas and procedures for learning about analysis. Pre-lab material is then designed to help consolidate this learning, or enhance, expand and apply it in ways that set the scene for the lab session activity. We want to prepare you to be ready to go in the session itself and make the most of your time there.\nWant additional support? Keep in mind that the Department has endorsed and will use the Statistics textbook by David Howell called “Fundamental statistics for the behavioral sciences”\nThe book covers principles of statistics as well as some tutorials on using R. You can access a library copy from the library pages"
  },
  {
    "objectID": "PSYC121/Week1.html#analysis-labs-and-pre-lab-activity",
    "href": "PSYC121/Week1.html#analysis-labs-and-pre-lab-activity",
    "title": "Introduction to PSYC121",
    "section": "",
    "text": "For each lab session that you have been scheduled to attend, please come “prepared’. By this we mean\n\nYou have watched the lecture video, made notes, and developed questions where you have them.\nYou have worked through the online “prelab” preparation materials - Here is the one for Week 1\nYou read the lab sheet (this one) to get a sense of what we’re going to be doing, and you anticipate potential problems so that you can focus on these in the session.\n\nThe lecture is designed to deliver important ideas and procedures for learning about analysis. Pre-lab material is then designed to help consolidate this learning, or enhance, expand and apply it in ways that set the scene for the lab session activity. We want to prepare you to be ready to go in the session itself and make the most of your time there.\nWant additional support? Keep in mind that the Department has endorsed and will use the Statistics textbook by David Howell called “Fundamental statistics for the behavioral sciences”\nThe book covers principles of statistics as well as some tutorials on using R. You can access a library copy from the library pages"
  },
  {
    "objectID": "PSYC121/Week1.html#activities-for-this-week",
    "href": "PSYC121/Week1.html#activities-for-this-week",
    "title": "Introduction to PSYC121",
    "section": "Activities for this week",
    "text": "Activities for this week"
  },
  {
    "objectID": "PSYC121/Week1.html#task-1---check-in-with-the-university-attendance-register",
    "href": "PSYC121/Week1.html#task-1---check-in-with-the-university-attendance-register",
    "title": "Introduction to PSYC121",
    "section": "Task 1 - check-in with the University attendance register",
    "text": "Task 1 - check-in with the University attendance register\nWhen you arrive, make sure you have checked-in to your Analysis session in the Levy lab. All students are required by the University to confirm attendance at taught session\nStaff will remind you of this in your class."
  },
  {
    "objectID": "PSYC121/Week1.html#task-2---getting-dicy",
    "href": "PSYC121/Week1.html#task-2---getting-dicy",
    "title": "Introduction to PSYC121",
    "section": "Task 2 - Getting dicy",
    "text": "Task 2 - Getting dicy\n\nHere’s a simple task for you to complete as a group around each of the workstations;\nYou will be given a pair of dice\n\nWorking in pairs, one person rolls both dice.\nAdd up the total on each of them and have someone record that total (if you don’t have some spare paper or a pen, use your computer)\nRepeat those steps 20 times.\nThen swap over your roles (the person rolling the dice, the person recording the outcome)\nOnce everyone at the workstation has had a turn at this, each person should attempt to work out (a) the mean and (b) the median of their dice roll total.\nCheck each others working, and discuss any differences or problems you have.\n\nAre all your answers the same? Why / why not? If not, are they very different or very similar?"
  },
  {
    "objectID": "PSYC121/Week1.html#task-3---using-rstudio",
    "href": "PSYC121/Week1.html#task-3---using-rstudio",
    "title": "Introduction to PSYC121",
    "section": "Task 3 - Using RStudio",
    "text": "Task 3 - Using RStudio\n\nIntroducing R Studio\nR and RStudio is the software that we will be using to explore and learn about analysis in your Psychology degree. It’s a computational engine: a very snazzy calculator that you should see as your friend and ally in the journey to understand and appreciate psychology. It sits alongside what we teach about the concepts and interpretation of statistical analysis.\nR is the core software, RStudio is the interface for interacting with it. Put another way, R is the engine, RStudio is the cockpit.\nLike even a simplest calculator, it just does what you ask (at least when you ask nicely!) but it requires the user to know what they want from it and to understand what it is telling you. A calculator can’t help a kid get the right answer to a multiplication problem if they don’t know the difference between multiplication and division and addition etc. And whilst a calculator is brilliant at doing the number crunching (and as a bonus, R Studio can help with turning the numbers into beautiful graphs and images too), even a calculator requires a thoughtful person to take the answers and make sensible interpretations from them.\nTherefore, we need to learn both about the concepts of statistical analysis on the one hand, and the processing of statistical information -through R- on the other. The lectures will provide the starting point and the direction for statistical concepts, whilst these analysis labs provide the more practical experiences in how to use R, and how to make R your ally. Over the next year, in these labs we will increasingly be using RStudio to focus on the latter, processing side, which will allow you to focus your energies on the conceptual side and its relevance for appreciating psychology.\n\n\nGetting started with RStudio\nFor Lancaster University Psychology Students in 2022, we will be learning about R Studio through a simple but powerful web server architecture. That is, through the power of the internet, you can access and use R Studio by logging into a free account that we have provided and we will maintain for your use.\n\nHere’s a little secret: There are several different ways to access RStudio. For example, you can download a copy of the software onto your computer, or use a Virtual Machine set up to run a copy. There’s nothing to stop you having your local copy, but please note - we can’t support your own version through lab classes. We’re using the web server to make sure everyone has the same, controlled experience.\n\nYou will have received an email with your account information to log onto From a computer on the campus wifi, you can access R Studio at:\nLancaster Psychology R Studio Server\nAt the login screen, use your university username (e.g., bloggsj)\nYour password for R Studio is: [password here]\nAs it says in the subject line, please keep your account details safe!\n\n\nWhat does RStudio look like?\nWhen RStudio starts, it will look something like this: \nRStudio has three panels or windows: there are tabs for Console (taking up the left hand side), Environment (and History top right) , Current file (bottom right). You will also see a 4th window for a script or set of commands you develop, also (on the left hand side).\n\n\nLet’s get started!\nThe first thing we want to do in RStudio is to create a folder for this week so that we can put the relevant material there and keep it tidy.\nFrom the lower-right panel of RStudio, click the files tab.\nSelect the “new folder” option and create a new folder (eg “week 1”)\nClick on that folder to open it\nNext, we’ve prepared some instructions for RStudio to use - this is called a “script”. So we need to get this script into the server for you to explore and play with\n\nDownload the “zip” file by clicking this link\nFind the location of the file on your computer and check it is saved as a “.zip” file\nReturn to RStudio\nClick “Upload”\nClick choose file and find the file on your computer.\nSelect the file and click “Open”. Click “OK”\n\nYou should now see the files extracted in the directory. If you receive an “unexpected server error” please try this process in a different browser. If you still have trouble, send your username to t.beesley@lancaster.ac.uk for support.\nYou should now have the script available in RStudio.\nUse “Save…As” to create a new version of the script. By doing this, you’ll be able to have a “before” and “after” version of the script and can go back over the changes\nIn the script, select or highlight the first line of text, which is this one:\n\nRun your first ever R instruction!\n\n5 + 5\n\nand “run” this line. That tells RStudio to carry out the instruction.\nYou should see that in the console tab, RStudio calculates the answer to this incredibly hard maths challenge! (amazing huh? OK, maybe not *that* amazing…).\n\n\nModify your first ever R instruction!\nUse your imagination – add a new line to the script and ask a different simple arithmetic question of your own choosing! What happens?\n\n\nCalculate descriptive stats in R for the first time!\nIn this week’s analysis lecture, we looked at measures of central tendency and how to calculate them. So let’s get R to do these calculations also!\nFirst, we tell R about the data used in the lecture. We’ve already created the instruction that will do exacly this and it is in the script, so run this line from the script\n\nweek_1_lecture_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nThis creates an “object” called week_1_lecture_data. We can then perform calculations on this object. For example, we can find the mean by running the following command (use the script to do this)\n\nmean(week_1_lecture_data)\n\nCheck the answer is the same we found in the lecture (it should be 6!).\nNext, let’s ask for the median by running this line from the script:\n\nmedian(week_1_lecture_data)\n\nThis also should be the answer from the lecture (7)\nR doesn’t have a single corresponding command for the mode, but we can use the block of code in the script for this that starts and ends with the “getmode” text (there are 6 lines of text)\nThis is just a bit of clever jiggery-pokery that gets the mode. What does R say the mode is?\n\n\nYour challenge\nHow can you get RStudio to verify / check the dice calculations that you attempted earlier? Think about how you might solve this problem, on the basis of what we have covered so far.\nWe will discuss this in class and attempt to get RStudio to check your answers. In doing so, annotate the script (add notes for you - not RStudio) using the “#” command"
  },
  {
    "objectID": "PSYC121/Week1.html#before-you-finish",
    "href": "PSYC121/Week1.html#before-you-finish",
    "title": "Introduction to PSYC121",
    "section": "Before you finish",
    "text": "Before you finish\n\nMake sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\nEnd your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\nThere is a red “power” button near the top right of the R studio window (do ask for help if you can’t find it). It’s a good habit to get into to turn the session off\n\n\nExtra content for outside the lab class\n\nIn the Howell text book on statistics, there’s some R code on descriptive statistics. It is included in the script for you to look at and play with.\nin your own time and think about the following:\n\nIn R, “&lt;-” is the assignment operator as in the command we used:\n\nPSYC121_week_1_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nWe create the variable label on the left (Analysis_week1_data) and we give it those numbers on the right. The nameAnalysis_week1_data` is largely arbitrary: try use a variable of your own naming (your own name?) instead - and then use that alternative name for the other commands.\n\nThroughout this year, we’ll use the convention of the “underscore” to separate words in labels (it_makes_them_easier_to_read than ifyoudidn’thaveanyspaces)"
  },
  {
    "objectID": "PSYC121/Week1.html#task-4-review-the-learnr-sample-practice-questions",
    "href": "PSYC121/Week1.html#task-4-review-the-learnr-sample-practice-questions",
    "title": "Introduction to PSYC121",
    "section": "Task 4 – Review the learnr sample / practice questions",
    "text": "Task 4 – Review the learnr sample / practice questions\nAfter every block of teaching in part-1 analysis (specifically, we mean in week 5, week 10, week 15 and week 20) there will be a class test. This will assess your knowledge and your understanding of the material that has been covered.\nThe class test will comprise a set of Multiple Choice Questions (and the set of questions will be different for each student, as the test will involve random selection from a larger pool) under timed conditions.\nIn order to help you get (a) a broad or basic feel for the sort of questions you might get in the class test (b) self-review your progress through the term, we will provide MCQs each week for you to attempt.\nSo these are for your benefit… you can take the questions when you choose to, and the learnr quiz will provide feedback on the answers your provide. Just bear in mind:\n\nWe place a set of questions at the end of the learnr pages so that you can attempt these at the end of each week, after you’ve completed lab activities, follow-up work, weekly Q&As etc. But it’s up to you when you answer the questions\nThese are meant as indicative questions. There’s no point in learning/ memorising these questions (they won’t be on the quiz!) and our advice is to reflect on how the teaching and content links to the sorts of questions that get posed."
  },
  {
    "objectID": "PSYC121/Week1.html#task-5-data-collection-exercise",
    "href": "PSYC121/Week1.html#task-5-data-collection-exercise",
    "title": "Introduction to PSYC121",
    "section": "Task 5 – Data collection exercise",
    "text": "Task 5 – Data collection exercise\nIn order to learn about psychology and data analysis techniques, we need data! Rather than rely too much on artificial data (certainly it is sometimes useful to say “Here are a bunch of numbers and this is what we can do with them” – think about the R Studio example for this week’s lab) for the most part, we prefer to draw on datasets that are a bit more engaging and meaningful that you have a stake in yourself! By using a common data set, that we can return to over the year, we can also build up familiarity and confidence in the data and remove a potential obstacle to thinking about the more important analysis part.\nSo a key task will be for everyone to have a go at taking our online survey, and contribute to a dataset that can be used throughout the year.\nWe would like you to complete the survey via your Department Sona system account. This way, you will receive a research credit for doing so, to “jump start” your credit account.\nThe sona system is can be accessed by following this link"
  },
  {
    "objectID": "PSYC121/Week2.html",
    "href": "PSYC121/Week2.html",
    "title": "Descriptive statistics in RStudio",
    "section": "",
    "text": "Last week, among other things, we asked you to\n\nRoll some dice, carry out some (relatively) straightforward ‘hand’ calculations of central tendency\nConnect to the RStudio server and create a folder\nWithin RStudio upload and run a script - a set of instructions, and explore annotations\nAdapt the script commands to perform calculations on the dice rolls within RStudio\nComplete a survey so that we can collect data for analysis teaching\n\nYour progress was great! We start with small steps and build up - but this is a nice start and we’re pleased how things went!\nBefore the lab, make sure you have worked through the material in the week2 learnr tutorial. The link is here\n\n\nFor a reminder of how to start RStudio, see week 1 lab sheet\n(remember: off campus, you will need to be on the VPN)\n\nA word of advice (from David Howell’s statistics book: “One more word of advice”I can’t resist adding what is perhaps the best advice I have. If there is something that you don’t understand, just remember that “Google is your friend.” She certainly is mine. (Well, maybe Google is getting a bit pushy, but there are many other search sites.) If you don’t understand what Fisher’s Exact Test is, or you don’t like my explanation, go to Google and type in Fisher’s Exact Test. I just did that and had 260,000 hits. You can’t tell me that there isn’t going to be something useful in there.)”\n\n\n\n\nIn week 1, we had a tiny dataset (relatively speaking) that we entered into R through a script line. That worked for what is was. But it’s going to become painful and tedious when (a) we want to work with larger datasets (b) we have data more complex than a 1-dimensional list of numbers (think about some 2-dimensional data sheets you might have encountered in excel for example)\nR can handle data files, and this week we’re going to explore them. Within R, we can specify ‘data frames’ which can have, essentially, multiple columns of data, and we can link data files to data frames for processing\nTo make things straightforward, each week we’ll provide students with a “zip” file that contains the script to start from (which you can expand and annotate etc, and save on your file space). We’ll also provide a data file or data files for you to use in the zip file. R can then import these files into the RStudio environment. So when you upload the zip file, you can import the data AND you can open up the script"
  },
  {
    "objectID": "PSYC121/Week2.html#pre-lab-work",
    "href": "PSYC121/Week2.html#pre-lab-work",
    "title": "Descriptive statistics in RStudio",
    "section": "",
    "text": "Last week, among other things, we asked you to\n\nRoll some dice, carry out some (relatively) straightforward ‘hand’ calculations of central tendency\nConnect to the RStudio server and create a folder\nWithin RStudio upload and run a script - a set of instructions, and explore annotations\nAdapt the script commands to perform calculations on the dice rolls within RStudio\nComplete a survey so that we can collect data for analysis teaching\n\nYour progress was great! We start with small steps and build up - but this is a nice start and we’re pleased how things went!\nBefore the lab, make sure you have worked through the material in the week2 learnr tutorial. The link is here\n\n\nFor a reminder of how to start RStudio, see week 1 lab sheet\n(remember: off campus, you will need to be on the VPN)\n\nA word of advice (from David Howell’s statistics book: “One more word of advice”I can’t resist adding what is perhaps the best advice I have. If there is something that you don’t understand, just remember that “Google is your friend.” She certainly is mine. (Well, maybe Google is getting a bit pushy, but there are many other search sites.) If you don’t understand what Fisher’s Exact Test is, or you don’t like my explanation, go to Google and type in Fisher’s Exact Test. I just did that and had 260,000 hits. You can’t tell me that there isn’t going to be something useful in there.)”\n\n\n\n\nIn week 1, we had a tiny dataset (relatively speaking) that we entered into R through a script line. That worked for what is was. But it’s going to become painful and tedious when (a) we want to work with larger datasets (b) we have data more complex than a 1-dimensional list of numbers (think about some 2-dimensional data sheets you might have encountered in excel for example)\nR can handle data files, and this week we’re going to explore them. Within R, we can specify ‘data frames’ which can have, essentially, multiple columns of data, and we can link data files to data frames for processing\nTo make things straightforward, each week we’ll provide students with a “zip” file that contains the script to start from (which you can expand and annotate etc, and save on your file space). We’ll also provide a data file or data files for you to use in the zip file. R can then import these files into the RStudio environment. So when you upload the zip file, you can import the data AND you can open up the script"
  },
  {
    "objectID": "PSYC121/Week2.html#lab-exercises---descriptive-information-in-r-studio",
    "href": "PSYC121/Week2.html#lab-exercises---descriptive-information-in-r-studio",
    "title": "Descriptive statistics in RStudio",
    "section": "Lab exercises - descriptive information in R Studio",
    "text": "Lab exercises - descriptive information in R Studio\nSome years ago, a large group of participants gave an estimate of the weight of Penelope the cow. Just over 17,000 guesses. And the distribution of guesses was something like this: \nWhat we can see from this graph is that:\n\nGuesses formed a roughly normal distribution. There is a bit of a skew with a right-hand tail, but this is inevitable as a weight of less than 0 is physically impossible, but there is no limit of the semantics of a large guess.\nThe mean guess weight (1,287 lbs) is very close to the actual (true) weight of the cow (1,355 lbs). So even though lots of people were inaccurate, a central tendency measure has a pretty good alignment with the true weight. This is known as the Wisdom of Crowds phenomenon, first identified by Galton in 1907 (though he suggested using the median weight)\n\nLet’s look at (a sample of) the PSYC121 student data collected on guessing the weight of Penelope, and ask whether it resembles the properties of this large dataset.\n\nLoading the data\nUsing the instructions and advice given on Moodle, get the week2.zip file and bring the data and R script into R Studio. The week 2 zip file is here\n\n\nUsing the R script\nStep 1. Download the week_2.zip file (If you are using a mac, there is a video guide on Moodle to explain how to download the zip file as is)\nStep 2. Open the appropriate folder on the RStudio server\nStep 3. Upload the zip file\nLet’s start working with our data, by opening up (clicking on) the script “Week_2.R” file.\nThe first command is to load a library of functions:\n\nlibrary(tidyverse)\n\nTo run this, simply click anywhere on line 1 of the R script to put the cursor there, and press ctrl+enter (cmd+enter on a mac) or click the button called run. You will see a number of messages appear in the console. Don’t worry about these, or worry too much about what exactly this command is doing. Essentially this is giving us some useful tools for our analysis. We will introduce the features of the tidyverse gradually during this course.\n(side note: If you were using a local version of R studio on your computer, it might not have the ‘tidyverse’ library already installed. You would need to install the package first)\nThe data are on the RStudio server if you have followed all the lab sheet to this point. Note that when you imported the data into the R environment, a command line was generated at the console\n\ncows &lt;- read_csv(\"~/penelope22.csv\")\n\nWhat this command accomplished was to read the spreadsheet called ‘penelope22’ into an object in R called cows. You could use any object label, but it’s important to then keep that name consistent in what you do next.\nThe command was also generated\n\n View(cows)\n\nwhich presents the data in a window of RStudio. Note that “NA” means not available or missing data. Does this file structure make some sense to you?\n\n\nFinding the mean and median estimates\nUse the data to answer the following questions…\n\nWhat is the mean weight estimates?\nWhat is the standard deviation of the estimates?\nWhat is the median weight of the estimates?\nWhich of these central tendency measures is the more accurate measure of the true cow weight? (make a judgement)\nWhat is the mean weight estimate (and standard deviation) for female respondents and non-female (male / non-binary /prefer not to say) respondents?\n\nYou may be thinking, how do I possibly do any of this?! Well this week most of the commands you need are contained in the R script you have downloaded. Also, remember from last week, we explored the R command:\n\nmean(week_1_lecture_data)\n\nThat gave us the mean of the small dataset week_1_lecture_data. This time, we want to explore the penelope dataset. But also, the lecture_data was just a single list of numbers. The penelope22 object is more like a datasheet. So we need to tell R Studio which column we are interested in. RStudio uses the format data$column. So run the followinbg line in the script\n\nmean(cows$estimate) \n\n\nsd(cows$estimate)\n\nSo from this, can you work out what you would do to get the median value (remember from last week how we got the median value?)? Part fo the command is given to you, can you change the text so that it works?\n\n\nCalculations from a range of columns\nWe have seen that:\n\nmean(cows$estimate) \n\nwill provide a mean of the column “estimate”. In the third column, named “female_estimate”, we have the estimates of just the female respondents. In the fourth column, named “other_estimate”, we have the estimates of the “other” respondents (males and non-binary and prefer not to say).\nSo can you now figure out how you might get information about the estimate from the female data (only) or the non-female data? Try it, based on what you have just done. Does it work?\nYou will find that the result of the this command produces an “NA” result. This means that the answer is “Not Available”, or in other words, is a “missing value”. This is because some of the values in this column are NA, and the mean of a column with NAs will always lead to the result NA.\nInstead, try change the script so it looks like this:\n\nmean(cows$female_estimate, na.rm = TRUE )\n\nAny different? The na.rm = TRUE instruction tells RStudio that missing data can be ignored in this mean calculation. (in technical language, na.rm is a parameter of the function mean that removes the NAs if set to TRUE)\n\n\nSimple graphs\nRStudio can be used to create graphical data plots that can help interpret datasets\nThe first thing we can do is create a histogram distribution of guesses from the sample student data to compare with the previous large sample study (i.e. the 17,000 guesses):\n\nhist(cows$estimate)\n\nOne way to alter or adjust the histogram is to change the width of the bars, the intervals, between each plot section. Try run this line from the script\n\nhist(cows$estimate, breaks = ??)\n\nDoes it work? No? What you need to do is replace the two question marks in the script (or better still, create a new instruction line in which you amend this to have a numerical value representing the number of different plot bars. Try at least 3 different values. Look at and think about how this affects the visual distribution.\nWe can also create a “box and whisker plot”. Here’s a general simple description of a box-and-whisker plot as a graphical representation of data:\n\n\n\nAdditional challenge\nIn the zip file, we also provide data on the estimates of the percentage of immigrants in the UK. This will allow you to explore the data discussed in the analysis lecture, and create visualisations of the data and its spread. Can you apply the analysis of the penelope data to the immigration data?"
  },
  {
    "objectID": "PSYC121/Week3.html",
    "href": "PSYC121/Week3.html",
    "title": "DVs and IVs in RStudio",
    "section": "",
    "text": "Last week we asked you to\n\nUse a script to run instructions in RStudio\nPut data into RStudio form a data file and explore how to run descriptive stats\n\nThis week - again, there’s a learnr tutorial to follow and help prep for this week’s activities You can find it here"
  },
  {
    "objectID": "PSYC121/Week3.html#pre-lab-work",
    "href": "PSYC121/Week3.html#pre-lab-work",
    "title": "DVs and IVs in RStudio",
    "section": "",
    "text": "Last week we asked you to\n\nUse a script to run instructions in RStudio\nPut data into RStudio form a data file and explore how to run descriptive stats\n\nThis week - again, there’s a learnr tutorial to follow and help prep for this week’s activities You can find it here"
  },
  {
    "objectID": "PSYC121/Week3.html#r-studio-tasks",
    "href": "PSYC121/Week3.html#r-studio-tasks",
    "title": "DVs and IVs in RStudio",
    "section": "R Studio tasks",
    "text": "R Studio tasks\nFor the “penelope22” data in week 2, we provided you with estimates data, and you were able to generate descriptive statistics for the estimates (if not, please go back and work through that part of the week 2 lab sheet again). You also found the weight estimates for the female and non-female guesses, right?\nHowever, in order to find the estimates separately for gender identity, we needed to have a column for each gender category. Whilst that worked, it could get cumbersome over time always to work with data created like that.\nThere is a better way…\n\nTask 1 - More with the penelope22 data\nStep 1. This week, we again want to explore commands from the tidyverse library (toolkit) which can help us do more powerful things more elegantly. So let’s get R to work with the tidyverse library\nlibrary(tidyverse)\nStep 2. Explore help() commands. R can give you more information about how it works.\nStep 3. Creating a project and a folder, and setting the working directory\nStep 4. Bring the week3.zip file into R Studio server. Like last week, upload the zip file, and launch the R script.You can get the file here\nStep 5. Read or load the penelope data into R. Have a look at it using the View() command in the script\nThis time, let’s ask for the estimate data arranged by identity:\naggregate(x = *MISSING*2$estimate, by = list(*MISSING*$identity), FUN = mean)\nFirst, let’s try this (you will need to use your object name in place of MISSING). What do you get? Does this match what we did last week when we calculated the mean for the female and for the other (i.e., non-female) group?\nSecond, let’s look at what is happening here:\naggregate This is a command to call for descriptive statistics\nx= This defines what column we are analyzing\nby=list Now we tell R how to group the estimate data, and which column does that\nFUN=mean Specifies which descriptive function is being asked for Can you explore whether you can call on alternate measures?)\n\ngroup_by()\nThere’s another way that also allows us to group scores by a (nominal) variable. This is explored in the learnr tutorial, which should help you create the command the get weight estimates broken down by gender identity. You need to define the data frame for the estimates data, and the gender IV and the estimates DV\n*MISSING* %&gt;% group_by(*MISSING*) %&gt;% summarise(mean_estimate = mean(*MISSING*))\nFirst, try this command and see what you get. If you run this command as entered, it won’t work. So now use your experience at skills from the above and the learnr tutorial to work out what is required.\nNote\n%&gt;% This is called a “pipe operator”, basically take the output from the left and feed it into the requests on the right. Summarise Provide summary statistics information for the specified variable as specified (whether mean, median etc)\n\n\nThe assignment operator\nAs well as learning about the pipe operator, we want to remind you /draw attention explicitly to another important element of the R command line syntax: the assignment operator. Uing a command such as\ncows &lt;- read_csv(\"penelope22.csv\")\nlooks for the csv datafile called ‘penelope22’, and assign it to an object or variable called ‘cows’\nWe could create any object name we wanted (within limits of names already known to RStudio). It isn’t just for reading in data files, we can perform a whole range of functions and assign them to an object.\n\n\nTask 2 - Salary data\nUsing aggregate and summarise may not seem like much progress, because they are just replicating what we had already done with mean() is week 2. However (a) this emphasizes that there are often several ways to get at the same thing in R (b) now we know about grouping, about working with 2-dimensional data, we can start to do more efficient and informative things.\nNow, let’s turn to the guesses made about median salary in the UK. We can get the data from the file “wages” in the week_3.zip file (you will need to adapt the code in the script for the penelope data so that it will load in the wages data)\nLet’s take a peek at the dataset with\nglimpse(wages)\nGlimpse pretty much does what you might think from the meaning of the word – it just gives us a data sample (handy because this is a much bigger dataset) and shows that we have 3 columns; uk_region (where someone lives, note ‘other’ probably equals Northern Ireland, Europe, China, etc), family_position (age relationship with siblings), and salary (estimate).\nBy the way, the govt statistics say the actual median income in 2019 was approx. £30,350 see this link\n\nWriting into your script, use the working “aggregate” commands from task 1 with the penelope weight data to find out the salary guesses as a function of where someone lives? That is, can you adapt that code for this problem? First, make sure you read in the data file into an R object.\nCan you use the aggregate command to find out salary guess as a function of family relationships? (if you are the youngest child maybe you have older siblings earning money that changes your evaluation?)\nCan you get a breakdown of guess as a function of BOTH UK region AND family relationship together?\nCan you use the group by() command to display salary guesses as a function of where someone lives? Check this this gives you the same answer\n\n\n\n\nTask 3 - Phone Use data\n\nDatset 3: Use the dataset of phone screen time usage to further explore and consolidate the group_by() command (ie we’ll drop the aggregate command for this task to focus on group_by(). Adapt script lines from the above two tasks to read in and calculate screen time usage as a function of the type of phone.\nUse RStudio to figure out the overall mean salary estimate and the standard deviation. Calculate by hand what salary estimate would have a z score of z=-1.5?\n\n\n\nTask 4 - Final challenge: visualisation\nCan you find a way to visualise the screentime usage data that you have been working with above? The script provides two ways to consider doing this - boxplots (which we have looked at in script commands already) and ggplot, which we have spent less time with but is an extremely powerful engine for creating plots. We’ve provided the start of the code in each case, leaving you to work out the specifics. Remember to annotate your creations!\n\n\nNow you’re finished …\nRemember to complete your notes in the script, save the script file, and end the server session."
  },
  {
    "objectID": "PSYC121/Week7.html",
    "href": "PSYC121/Week7.html",
    "title": "Filtering data and testing means (one-sample t-test)",
    "section": "",
    "text": "Today we will look in a bit more detail at people’s estimates of the average UK salary. We will first plot this data using geom_histogram() and also geom_boxplot(). When we do this, we’ll see that there are some unusual values, and we’ll need to do a bit of data wrangling to remove them, using the filter() command. We’ll then turn to the conceptual ideas of the lecture - how can we tell if the mean of our sample is unusual, or whether we would actually expect this mean value under the null hypothesis? Finally, we’ll continue to develop our skills in data visualisation by exploring geom_density() plots."
  },
  {
    "objectID": "PSYC121/Week7.html#pre-lab-work",
    "href": "PSYC121/Week7.html#pre-lab-work",
    "title": "Filtering data and testing means (one-sample t-test)",
    "section": "Pre-lab work",
    "text": "Pre-lab work\n\nThere is an online tutorial. Please make every attempt to complete this before the lab!\nCreate a folder for Week 7\nDownload the Week_7.zip and upload it into this new folder in RStudio Server."
  },
  {
    "objectID": "PSYC121/Week7.html#rstudio-tasks",
    "href": "PSYC121/Week7.html#rstudio-tasks",
    "title": "Filtering data and testing means (one-sample t-test)",
    "section": "RStudio tasks",
    "text": "RStudio tasks\n\nPlotting and filtering\n\nOpen the Week_7_script and run the library, options commands. The options command is new. It is very cryptic and you don’t need to worry too much about this - it is making sure that the values in the graphs are displayed as regular numbers and not as scientific notation. Add a read_csv() command to get the data - you’ve done this every week, so this should be familiar.\nComplete the geom_histogram() code to plot the distribution of salary data. Try setting bins=20 within your geom_histogram() code. Play around with the number of bins for the histogram.\nOK - we’ve got some pretty funky values here! Some people think the average salary is over £400,000!!! Well, maybe they just added too many zeros (let’s give them the benefit of the doubt). Quite often when we get our “raw” data, it contains weird values like this that we need to consider removing. Let’s run the arrange() code now to see what exactly those high values are.\nWe’ll need to remove these high values to get a better sense of the distribution. Let’s use a filter() command to do this. We need to make a decision about what values to exclude. In later labs we’ll look at a more systematic process of removing outliers, but for now, let’s just remove any that are over £200,000. Edit the filter() command to keep only those estimates that are below £200,000 (&lt;). Remember that the filter command keeps the data that is TRUE according to the expression.\nSTOP! OK, this next bit is very important. If you completed that last step correctly you’ll see an output in the console showing a “tibble” (a data frame) which has 194 rows and 5 columns. However, your object has not actually changed in the environment. This will still be showing as 197 observations (row). So the filter command will take out those large estimations, but we haven’t actually changed the data object. To do this we need to assign (&lt;-)the result of our filter command to the object. To do this, you need to put in some code at the start of this small chunk of code you’ve just run, so that the result of your commands will be assigned to the object (you can make this the same object name you’ve been using -overwrite it-, or call it a new name).\nWhen you run this command again you should see the (new) object has changed to 194 rows. We can now plot the data again as a histogram. To do this, copy the earlier code for the histogram and edit it as necessary (if you’re using a new object for the filtered data, you’ll need to edit the code to reflect that). .\nAnd as you know, we can also look at the distribution as a boxplot, a violin, or a density plot. Feel free to add in your own code for other visualisations you might like to try.\n\n\n\nOne-sample t-test\nWe now want to know if the salary estimates are different to the actual average salary in the UK (which is approx. £30,000). Our hypothesis might be that people are inaccurate - they overestimate or underestimate the average UK salary. Let’s test that.\n\nFirst, let’s calculate the mean and sd of the column.\nNow we can compute a t-statistic and check whether this mean is significantly different from the expected mean. We do this with t.test(). Edit the code on this line to conduct a one-sample t-test. You need to provide the sample of data on which you want to conduct the test, and the expected mean under the null hypothesis. Remember our hypothesis is that people are not accurate. Your calculation of the mean should tell you whether they numerically overestimated or underestimated. But would we expect such a result under the null hypothesis? Run the t-test and note the p value. How likely is it that we would see this sample of data (this mean value and the distribution of data - the SD) under the null hypothesis? The p value ranges from 0 to 1. If it is very low - typically we say p &lt; .05 - then we conclude our result is unlikely under the null hypothesis and it is therefore a significant result.\nWhat is the critical value of t in the t-distribution table, for this sample size? Degrees of freedom is N - 1.\n\n\n\n\nPractising filtering\n\nFiltering can also be useful for selecting certain sub-sets of our data. In the script we have given you an example of how we select a sub-set of data based on two conditions from two different columns:\n\ndata_set_name %&gt;% filter(home_location_in_UK == \"NW\" & sibling_order == \"oldest\")\nWe have given you a few different columns to look at and to use in practicing your filter commands:\nsibling_order: what position in age was the respondent within their siblings home_location: UK / Asia / Europe, etc home_location_in_UK: NW, NE, etc (NA is non-UK residents) attention_check: respondents were asked “click strongly agree to show you are paying attention” - some people failed this!!!\nComplete the following filters. We’ve put in () the number of rows you should see in the resulting object\n\nJust those people who come from the North East (16 rows)\nThose people who come from Wales and are the middle child within their siblings (2 rows)\nThose people passed the attention check, are from the UK, and are an only child (21 rows)\nThose people who are NOT from the North West; you’ll need to use != (84 rows)\nThose people who failed the attention check (that is, did not say strongly agree) (14 rows)\nThose people who are from the South East or (|) the South West (38 rows)"
  },
  {
    "objectID": "PSYC121/Week7.html#sample-size-size-of-effect-and-the-one-sample-t-test",
    "href": "PSYC121/Week7.html#sample-size-size-of-effect-and-the-one-sample-t-test",
    "title": "Filtering data and testing means (one-sample t-test)",
    "section": "Sample size, size of effect, and the one sample t-test",
    "text": "Sample size, size of effect, and the one sample t-test\nIn the lecture this week, Tom used an application to show the process of sampling data. You can access this application at the link below. There are three “parameters” you can change in this:\n\nThe true mean of the effect: Think of this like the bias that was set up in your deck of cards last week. There is some true state out there in the world, and we are going to draw samples from a distribution of data that has a mean that equals this value. If you make this 100, then the true mean is equal to that under the null hypothesis (there is no effect).\nThe standard deviation of the data: This sets how variable the data are in this population. If the data are more variable, then our samples will produce estimations that are less accurate of the true mean value.\nThe sample size: How many observations are drawn in the sample. These are represented by the yellow circles in the plot.\n\nEach time you draw a sample the data points are plotted in yellow and the mean of the sample is marked with the red line. The application also runs a one-sample t-test against the expected mean under the null hypothesis, of 100. The null hypothesis is also represented by the static distribution presented in grey, centred on 100.\nThings to try:\n\nStart with a sample size of 10, and a mean of the effect of 110 (SD = 15). How often do you get a significant result (p &lt; .05) when you draw a new sample?\nNow try changing the mean of the effect to 120. Does this increase or decrease the likelihood of getting significant results? What about changing to 130?\nNow keep the mean effect constant (say 110), but increase the sample size. Try 5, then 10, 15, and so on. Does this increase or decrease the likelihood of getting significant results?\nSet the mean of the effect to 100 and the sample size to 10. Keep drawing new samples, noting each time the p value. You will evenutally get a p value of &lt; .05. What type of error is this?\n\nClick here for the one-sample t-test application"
  },
  {
    "objectID": "PSYC121/Week7.html#week-7-quiz",
    "href": "PSYC121/Week7.html#week-7-quiz",
    "title": "Filtering data and testing means (one-sample t-test)",
    "section": "Week 7 Quiz",
    "text": "Week 7 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/Week6.html",
    "href": "PSYC121/Week6.html",
    "title": "Sampling, probability and binomial tests",
    "section": "",
    "text": "Ensure you have watched the lecture for Week 6.\nThere is a learnr tutorial to complete which will help you to think about binomial tests. You : You can find it here.\nSet up a new folder in RStudio and upload the files from the Week 6 zip file\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week6.html#pre-lab-work",
    "href": "PSYC121/Week6.html#pre-lab-work",
    "title": "Sampling, probability and binomial tests",
    "section": "",
    "text": "Ensure you have watched the lecture for Week 6.\nThere is a learnr tutorial to complete which will help you to think about binomial tests. You : You can find it here.\nSet up a new folder in RStudio and upload the files from the Week 6 zip file\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week6.html#card-sampling-task",
    "href": "PSYC121/Week6.html#card-sampling-task",
    "title": "Sampling, probability and binomial tests",
    "section": "Card sampling task",
    "text": "Card sampling task\n\nIn the first task this week we will look at the sampling of events and we will apply the basic statistical test of the binomial test: binom.test()\nEach table has a set of cards. These will be 13 red cards and 13 black cards - please check your set to ensure you have the right number of each colour (it doesn’t matter what suit the cards are).\nIn this task we will be drawing samples from the deck, and trying to tell whether the deck is biased towards red or black. Think of the cards as the population of scores out there in world, and when you can only see back of the cards, that data is unobserved.\nAs an experimenter we are trying to estimate what is true about the world, and in order to do this we need to draw a sample. So each time you draw a card, you are observing one data point from the population, and based on a collection of data (the sample) you are going to draw an inference about what is true about the population.\n\nSet up and instructions:\n\nOne person on each table should act as the “world” (the dealer). Congratulations, you are God. That is, this person will determine what is true about the state of things in the world. In this example, that means they control what is contained in the deck of cards. For each experiment, secretly remove some cards from the deck and place those face down. It’s important that no one sees what these cards are. For example, you might take out 5 red cards, so that 13 black and 8 red are left; this deck is now biased to black. Or you might take out 3 red and 3 black; this deck is not biased.\nThe remaining people (1 or more) will act as the experimenters. Your job is to draw samples and work out whether you think the deck is biased or not towards either red or black.\nFor each experiment, go through the following steps:\n\n\nThe world removes some cards from the full deck (the number and colour of the cards removed is up to them) and they place these face down on the table. They shuffle the deck ready to start the “experiment”.\nThe experimenters “pre-register” their sample size. That is, they state how many cards they are going to draw.\nDraw samples one at a time. Importantly - make sure you replace the cards each time and the world/dealer should give the pack a quick shuffle.\nMark down whether the card was red or black in your logbook\nThe world should shuffle the cards after each draw. Repeat 2.3 and 2.4 until you have reached your sample size.\nAt the end of each experiment, the experimenters should draw a conclusion based initially on their own “gut feeling” about the data. Do you think the deck was biased towards red, black, or was it unbiased?\nAs a group, use RStudio to run a binom.test() to provide a statistical result (you can do this in the console, or create a new script to save your tests and results - it’s up to you). Was this result unusual? How likely were the data given the null hypothesis? Note down the p value that this test gives you.\nThe “world” can then reveal the hidden cards. Was the deck actually biased or not? How does this sit with a) your initial conclusions, and b) the result of the binomial test?\n\n\nRepeat all steps in part 3 again for a new experiment, making sure that you try different parameters for the experiment. So vary a) how many cards are removed from the deck, b) the combination of cards removed from the deck, and c) the pre-registered sample size. Feel free to swap the roles around.\nOnce you’ve conducted a few experiments, discuss on your table the results you found. It might be useful to think about the following things:\n\n\nwere there times when your intuitions were different to the statistical result? For example, you were sure there was a bias, but in fact the statistics told you this was not that unusual (p was &gt; .05)?\nwere there times when the deck was actually biased, but you failed to prove this with your experiment (you failed to see p &lt; .05)? Do you remember what this type of error is called?\nwere there times when the deck was not biased, but the test result suggested it was (p &lt; .05)? Do you remember what type of error this is called?"
  },
  {
    "objectID": "PSYC121/Week6.html#rstudio-tasks",
    "href": "PSYC121/Week6.html#rstudio-tasks",
    "title": "Sampling, probability and binomial tests",
    "section": "RStudio tasks",
    "text": "RStudio tasks\nFor the second exercise today we will take a look at the phone data again. If you haven’t already, set up a new folder for Week 6 and upload the data from the Week 6 zip (see pre-lab instructions).\n\nRead in the csv data file\nYou should know how to do this by now. But if not, try searching “csv” at the top. Remember that what you name your data set is important for the following commands.\n\n\nTake a look at the data\nThere are lots of ways to get a quick look at the data. Here are a few useful ones (some you’ve come across, some that might be new): glimpse(), summary(), View(), head().\n\n\nCreate a boxplot of the phone data\nComplete the code to create a boxplot of the estimated phone use. Note that you can put the boxes on either the x or you y axis. Copy and paste the code and edit it so you can also plot the actual phone use.\n\n\nCreate a density plot and/or hisotgram of the phone data\nIt’s very easy to convert the boxplot code into either a density plot (geom_density()) or a histogram (geom_histogram()). Have a play around with these different types of graphs. Which one communicates the spread of the data most clearly? Is it better to plot these on the x or y axis?\n\n\nPlot the relationship between estimated and actual phone use\nSo far we’ve looked at the phone use estimate and the actual phone use separately. But if people are at all accurate in their estimates, we’d expect these two things to be related (those people who use their phone more probably know they do). Let’s use a scatter plot to see if this relationship exists in our data. In ggplot() we can do with with geom_point(). Each point on the graph needs an x and y value, so with the code you’ve been given, you just need to add in the two variables we want to plot.\n\nIs there a relationship between these variables?\nHow would you describe this relationship in words?\n\n\n\nBinomial test of the accuracy of phone use estimates\nSo did people overestimate or underestimate their phone use on average? We have given you a column called accuracy which simply says whether each participant underestimated or overestimated. Add your dataset name to this code to use the count() function to get the total who overestimated and underestimated. With these totals, you have enough information to run a binomial test:\n\nfor this test focus on the totals for “underestimate” and “overestimate” (for simplicity we can ignore people who were “accurate” and the NA values)\nThese give you your first two parameters for binom.test()\nto get the probability, we consider the null hypothesis\nthat is what’s the probability of overestimating or underestimating under the null hypothesis?\nwhat is the result of the binomial test?\nYou can also include the UK_region within your count, and then run binomial tests on these sub-groups. Do you find any interesting reuslts? What issues might we have with running the binom.test() on these sub groups?\n\n\n\nMore things to try with your scatter plot\nWe can customise our plot even further:\n\nTry adding/changing the size = within geom_point() to make the points bigger or smaller (values from .1 to 30)\nTry adding/changing the alpha = within geom_point() to make the points transparent (try values between 0.1 and 1)\ntry adding colour = within ggplot(aes( )) to map the colour to the ‘UK_region’ variable.\nyou can change the colours used by ggplot by adding + scale_colour_brewer() to your plot code. Within this, try setting the pallete parameter to one of these options (e.g., scale_colour_brewer(palette = \"Set3\"))\n\n\n\nremember you can add labels using +labs()\nremember you can set a new theme, such as + theme_minimal()"
  },
  {
    "objectID": "PSYC121/Week6.html#week-6-quiz",
    "href": "PSYC121/Week6.html#week-6-quiz",
    "title": "Sampling, probability and binomial tests",
    "section": "Week 6 Quiz",
    "text": "Week 6 Quiz\nYou can access a set of quiz questions related to this week here."
  },
  {
    "objectID": "PSYC121/Week4.html",
    "href": "PSYC121/Week4.html",
    "title": "Customisation of graphs, and z-scores",
    "section": "",
    "text": "Complete materials from sessions in previous week (materials from week 1 /2 / 3 remain fully available for anyone who wants them). Consolidate what we have already covered.\nThis week - again, there’s a learnr tutorial to follow and help prep for what we are covering: You can find it here.\nMake sure you have access to the week4.zip file for the RStudio server.You can get the file here.\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week4.html#pre-lab-work",
    "href": "PSYC121/Week4.html#pre-lab-work",
    "title": "Customisation of graphs, and z-scores",
    "section": "",
    "text": "Complete materials from sessions in previous week (materials from week 1 /2 / 3 remain fully available for anyone who wants them). Consolidate what we have already covered.\nThis week - again, there’s a learnr tutorial to follow and help prep for what we are covering: You can find it here.\nMake sure you have access to the week4.zip file for the RStudio server.You can get the file here.\nIf you create a folder and upload the file into RStudio before the lab class you’ll be even more ready to follow along!"
  },
  {
    "objectID": "PSYC121/Week4.html#r-studio-tasks",
    "href": "PSYC121/Week4.html#r-studio-tasks",
    "title": "Customisation of graphs, and z-scores",
    "section": "R Studio tasks",
    "text": "R Studio tasks\nLast week we introduced two different ways to get descriptive information about a variable / column of scores investigated as a function of a separate piece of information. In others words, describe the DV a function of an IV\nStudents were generally very good at utilising each of these;\naggregate(x = DV, by = list(IV), FUN = mean)\nand\ntibble %&gt;% group_by(IV) %&gt;% summarise(mean_estimate = mean(DV))\n[tibble = technical name for the data in R]}\nThis week, we’re focusing on how you can edit or customise a graph to be more useful to a viewer."
  },
  {
    "objectID": "PSYC121/Week4.html#section-1---customisation-of-data-plots",
    "href": "PSYC121/Week4.html#section-1---customisation-of-data-plots",
    "title": "Customisation of graphs, and z-scores",
    "section": "Section 1 - Customisation of data plots",
    "text": "Section 1 - Customisation of data plots\nStep 1. Set up a folder for this week in the R Project that you created last week.\nStep 2. Bring the week4.zip file into R Studio server. Like last week, upload the zip file, and load in the data file. Launch the week_4 R script as before.\n{If you’ve done Step 1 &2 already as a pre-lab preparation, super, pat yourself on the back, skip these steps an move on)}\nStep 3. Once again, we’re gong to be using commands from the tidyverse library (the pipe operator is one example) so we need to ensure that it’s active. Run the command\nlibrary(tidyverse)\nStep 4. Read in the datafiles that will be on the server. There’s already a script line for this, you just need to change the file name (see the comments for advice on this)\nStep 5. We’ve provided a suggestion of how you can complete the visualisation challenge task from week 3.\nStep 6. Customize you graph work. We’ve provided some suggestions about adding titles and labels for your graph. Edit and play with the script lines to make them useful to you and to understand how they work.\n\nTry change the text, the colours, and so on of the graphs.\nAdd comments for yourself about what the different commands do. The idea is to learn by trying different things out (changing values, taking out elements of the command, putting other is) and record for yourself.\nIf you are struggling or not sue, try look at help files."
  },
  {
    "objectID": "PSYC121/Week4.html#section-2-z-scores",
    "href": "PSYC121/Week4.html#section-2-z-scores",
    "title": "Customisation of graphs, and z-scores",
    "section": "Section 2: z-scores",
    "text": "Section 2: z-scores\n\nHint / Reminder: Sketch a normal (z score) distribution and mark the mean/mode, and mark off the relevant parts of the question so you know what you are trying to achieve and how to interpret any calculations you make.\n\n\nHint/ Reminder 2. For questions 6 & 7, remember that from the week 4 lecture material, typically in psychology we use the 5% level as a cutoff to decide, in broadly described terms, whether something is extreme or unlikely vs. at least somewhat plausible or likely.\n\n\nz-scores 1\nz-score distributions\nQ1. What is the relationship between the sign of a z-score and its position in a distribution?\nQ2. If a distribution has a mean of 100 and a standard deviation of 10, what is the raw score equivalent to a z-score of 1.96?\nQ3. If a distribution has a mean of 157 and a standard deviation of 19, what is the raw score equivalent to a z-score of 1?\n\n\nz-scores 2 Using z-score tables\nQ4. What proportion of scores lie between the mean and a z-score of 0.5?\nQ5. What is the combined proportion of scores lying between z=-1.2 and z=.85?\n\n\nz-scores 3 Applying z-scores to inferential problems\nQ6. A Neuropsychologist has presented a test of face recognition to 200 neurotypical participants and finds that the scores are normally distributed with a mean of 85 and the standard deviation of 12. Two brain-damaged patients are also given the test. The one with right hemisphere brain damage scored 58 and the one with left hemisphere damage scored 67.\n\nWhat is the z score of the right hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower that this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\nWhat is the z score of the left hemisphere patient when compared to the neurotypical group?\nWhat proportion of neurotypical participants score lower than this patient?\nIs this patient likely to belong to the population of neurotypical participants? (justify your answer)\n\n\n\nExtra activity\nCome back to this afterwards for some extra practice if you want:\nQ7. Tom Bunion has completed a huge research study and measured the foot size of men and women and found each to be normally distributed. The men have a mean size of 55 with a standard deviation of 5 and the women a mean of 33 and a standard deviation of 5. Joanna Toes has foolishly measured two individuals but forgotten to note their gender. These have foot sizes of 37 and 47. To which gender is each more likely to belong? What evidence is there for this?"
  },
  {
    "objectID": "PSYC121/Week5.html",
    "href": "PSYC121/Week5.html",
    "title": "Class test",
    "section": "",
    "text": "No materials!"
  },
  {
    "objectID": "PSYC401/Week8.html",
    "href": "PSYC401/Week8.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC401/Week9.html",
    "href": "PSYC401/Week9.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC401/Week1.html",
    "href": "PSYC401/Week1.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC401/Week2.html",
    "href": "PSYC401/Week2.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC401/Week3.html",
    "href": "PSYC401/Week3.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC401/Week7.html",
    "href": "PSYC401/Week7.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC401/Week6.html",
    "href": "PSYC401/Week6.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC401/Week4.html",
    "href": "PSYC401/Week4.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week8.html",
    "href": "PSYC234/Week8.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week9.html",
    "href": "PSYC234/Week9.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week1.html",
    "href": "PSYC234/Week1.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week2.html",
    "href": "PSYC234/Week2.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week3.html",
    "href": "PSYC234/Week3.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week7.html",
    "href": "PSYC234/Week7.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week6.html",
    "href": "PSYC234/Week6.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC234/Week4.html",
    "href": "PSYC234/Week4.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "Resources/index.html",
    "href": "Resources/index.html",
    "title": "Accessing the R Server",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to Lancaster University’s Department of Psychology modules on Statistics for Psychologists. Here you will find all the module worksheets, along with the data and necessary files for completion of the labs.\nYou can access the Department’s R server at psy-rstudio.lancaster.ac.uk. Remember you will need to be on campus, or access via the VPN.\n\n\n\n\n\n\n\n\n\n\n\n\nPSYC121\n\nStatistics for Psychologists\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 6\nWeek 7\nWeek 8\nWeek 9\n\n\n\n\n\n\nPSYC122\n\nStatistics for Psychologists\n\n\n\nIntroduction to Part 1\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nIntroduction to Part 2\nWeek 16\nWeek 17\nWeek 18\nWeek 19\n\n\n\n\n\n\nPSYC214\n\nStatistics for Psychologists\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 6\nWeek 7\nWeek 8\nWeek 9\n\n\n\n\n\n\nPSYC234\n\nStatistics for Psychologists\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 6\nWeek 7\nWeek 8\nWeek 9\n\n\n\n\n\n\nPSYC401\n\nStatistics for Psychologists\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 6\nWeek 7\nWeek 8\nWeek 9\n\n\n\n\n\n\nPSYC402\n\nStatistics for Psychologists\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 6\nWeek 7\nWeek 8\nWeek 9\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "PSYC122/Week18.html",
    "href": "PSYC122/Week18.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials you will work with in PSYC122 Week 18.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 18 class, we will aim to answer two research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written"
  },
  {
    "objectID": "PSYC122/Week18.html#sec-week-18-intro",
    "href": "PSYC122/Week18.html#sec-week-18-intro",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials you will work with in PSYC122 Week 18.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 18 class, we will aim to answer two research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written"
  },
  {
    "objectID": "PSYC122/Week18.html#our-learning-goals",
    "href": "PSYC122/Week18.html#our-learning-goals",
    "title": "Statistics for Psychologists",
    "section": "Our learning goals",
    "text": "Our learning goals\nIn Week 18, we aim to further develop skills in analyzing and in visualizing psychological data.\nWe will use linear models to estimate the association between predictors and outcomes. What is new, here, is that we will explore the power and flexibility of the linear model analysis method in two important aspects.\n\n\n\n\n\n\nTip\n\n\n\n\nWe will fit linear models including multiple predictors, this is why this form of analysis is also often called multiple regression. 2 We will use linear models to estimate the effects of numeric and categorical or nominal predictor variables.\n\n\n\nWhen we do these analyses, we will need to adapt how we report the results:\n\nwe need to report information about the model we specify, identifying all predictors;\nwe will need to decide if the effects of one or more predictors are significant;\nwe will report the model fit statistics (F, R-squared) as well as coefficient estimates;\nand we need to learn to write texts describing the impact of predictors.\n\nUsually, in describing the impacts of predictors, we are required to communicate:\n\nthe direction of the effect – do values of the outcome variable increase or decrease given increasing values of the predictor?\nthe size of the effect – how much do values of the outcome variable increase or decrease given increasing values of the predictor?\n\nThis task of description is enabled by producing plots of the predictions we can make:\n\nabout how we expect the outcome to change, given different values of a predictor.\n\n\n\n\n\n\n\nTip\n\n\n\nWe will aim to build skills in producing professional-looking plots for our audiences.\n\nWe can produce plots showing the effects of predictors\nAs predictions of change in outcome, given different values of the predictor variables."
  },
  {
    "objectID": "PSYC122/Week18.html#sec-w18-resources-intro",
    "href": "PSYC122/Week18.html#sec-w18-resources-intro",
    "title": "Statistics for Psychologists",
    "section": "Your resources",
    "text": "Your resources\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found under the Week 18 resources section title, on Moodle:\nLink to Moodle"
  },
  {
    "objectID": "PSYC122/Week18.html#lectures-video-recordings",
    "href": "PSYC122/Week18.html#lectures-video-recordings",
    "title": "Statistics for Psychologists",
    "section": "Lectures: video recordings",
    "text": "Lectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 19 minutes\nPart 2 of 4; about 13 minutes\nPart 3 of 4; about 15 minutes\nPart 4 of 4; about 13 minutes\nThe lectures have three main areas of focus.\n1. Working with the linear model with multiple predictors\nWe focus in-depth on how you code linear models, how you identify critical information in the results summaries, and how you report the results: the language and the style you can use in your reports.\n\n\n\n\n\n\nTip\n\n\n\n\nA small change to lm() coding releases power and flexibility in how you use the analysis method.\n\n\n\n2. Analyses are done in context so when we conduct analyses we must use contextual information\nThe power and flexibility of the linear model presents challenges. We must decide which predictor variables we specify in our model. This specification requires us to think about our theoretical assumptions and what they require us to include to make sense of the behaviours or the individual differences we observe when we do things like investigating what makes health information easy or difficult to understand.\n3. Developing critical thinking\nAs we develop conceptual understanding and practical skills, we must learn to reflect critically on our analyses, and learn to critically evaluate the analyses we read about when we read research reports in the scientific literature.\n\n\n\n\n\n\nTip\n\n\n\nCritical analysis can develop by considering\n\nvalidity\nmeasurement\ngeneralizability\n\n\n\nWe are always working in the broader context of uncertainty:\n\nuncertainty about the predictions we may make concerning outcomes of interest;\nuncertainty given the possibility that predicted effects may vary between individuals or groups;\nuncertainty given the influence of sources of randomness in how specific responses are produced.\n\n\nLinks to other classes\nIn the lecture, I sketch out the ways that the linear model can be extended. You will learn new and different analysis methods in the second year classes but, most of the time, you may understand that …\n\n\n\n\n\n\nImportant\n\n\n\nEverything is some kind of linear model."
  },
  {
    "objectID": "PSYC122/Week18.html#pre-lab-activity-1",
    "href": "PSYC122/Week18.html#pre-lab-activity-1",
    "title": "Statistics for Psychologists",
    "section": "Pre-lab activity 1",
    "text": "Pre-lab activity 1\nIn weeks 16-19, we have been working together on a research project to investigate how people vary in their response to health advice.\nCompleting the project involves collecting responses from PSYC122 students.\nTo enter your responses, we invite you to complete a short survey.\nComplete the survey by clicking on the link here\n\n\n\n\n\n\nWarning\n\n\n\nIn our week 19 class activity, we will analyze the data we collect here.\n\nThis means we will close the survey by 5pm Monday 6 March (week 18)\n\n\n\n\nPre-lab activity 1 alternative\nIf you do not want to complete the survey, we invite you to read the pre-registered research plan for the PSYC122 health advice research project.\nRead the project pre-registration"
  },
  {
    "objectID": "PSYC122/Week18.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "href": "PSYC122/Week18.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "title": "Statistics for Psychologists",
    "section": "Pre-lab activity 2: Getting ready for the lab class",
    "text": "Pre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w18_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand the code files:\n\n2022-23-PSYC122-w18-how-to.R\n2022-23-PSYC122-w18-workbook.R\n\nYou will use 2022-23-PSYC122-w18-workbook.R in the lab activity practical class.\nAlternatively, you can instead download the resources you need from the week 18 section of the Moodle page for the PSYC122 module:\nLink to Moodle\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\n\nOur aim is to make sure you can work with code, and write notes in the .R files.\n\n\n\nIn the workbook.R file you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\nIn both the .R files:\n\n2022-23-PSYC122-w18-how-to.R\n2022-23-PSYC122-w18-workbook.R\n\nwe will take things step-by-step.\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\nThe how-to guide comprises an .R file 2022-23-PSYC122-w18-how-to.R with code and advice. The code in the .R file was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 7) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity."
  },
  {
    "objectID": "PSYC122/Week18.html#sec-w18-activity",
    "href": "PSYC122/Week18.html#sec-w18-activity",
    "title": "Statistics for Psychologists",
    "section": "Lab activity",
    "text": "Lab activity\nIn the lab activity .R file 2022-23-PSYC122-w18-workbook.R, you will work with data from a study about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv\n\nThe data are similar in format to the response data we are collecting as part of the PSYC122 project.\n\nTasks\nIn the activity, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library()\nRead in the data files – using read_csv()\nInspect the data – using head() and summary()\nEstimate the way in which an outcome may vary, given different values in a predictor variable – using lm()\nDo this with multiple predictor variables\nBetter understand linear model predictions by comparing one outcome-predictor relation\nCreate boxplots to examine the potential association between variation in a continuous outcome variable and the differences between groups or levels in a categorical variable – using geom_boxplot()\nEstimate the effects of factors as well as numeric variables\nVisualize the model predictions – using ggpredict()\n\nThe 2022-23-PSYC122-w18-workbook.R file takes you through the tasks, one by one.\nIf you are unsure about what you need to do, check in the how-to guide: look at the advice in 2022-23-PSYC122-w18-how-to.R.\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the dataset or the variables to complete the tasks in the activity.\nThis process of adapting demonstration code is a process critical to data literacy and to effective problem solving in working with data in psychological science.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC122/Week19.html",
    "href": "PSYC122/Week19.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials and guidance you will work with in PSYC122 Week 19.\nWe have completed four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC122, PSYC123 and PSYC124.\nWe have been doing this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 19 class, we will answer two research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written\n\n\n\nWe will be revisiting some of the ideas and techniques you have seen introduced in previous classes. And we will be extending your development with some new ideas, to strengthen your skills.\n\n\n\n\n\n\nNote\n\n\n\nI have said that our aim in these classes is to contribute new findings from the data we collect together.\n\nThat time is now.\n\n\n\nThe PSYC122 health comprehension survey is now closed, and we will focus our practical work in week 19 on analyzing the data we have collected."
  },
  {
    "objectID": "PSYC122/Week19.html#sec-week-19-intro",
    "href": "PSYC122/Week19.html#sec-week-19-intro",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials and guidance you will work with in PSYC122 Week 19.\nWe have completed four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC122, PSYC123 and PSYC124.\nWe have been doing this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 19 class, we will answer two research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written\n\n\n\nWe will be revisiting some of the ideas and techniques you have seen introduced in previous classes. And we will be extending your development with some new ideas, to strengthen your skills.\n\n\n\n\n\n\nNote\n\n\n\nI have said that our aim in these classes is to contribute new findings from the data we collect together.\n\nThat time is now.\n\n\n\nThe PSYC122 health comprehension survey is now closed, and we will focus our practical work in week 19 on analyzing the data we have collected."
  },
  {
    "objectID": "PSYC122/Week19.html#our-learning-goals",
    "href": "PSYC122/Week19.html#our-learning-goals",
    "title": "Statistics for Psychologists",
    "section": "Our learning goals",
    "text": "Our learning goals\nIn Week 19, we aim to further develop skills in analyzing and in visualizing psychological data.\nWe will use linear models to estimate the association between predictors and outcomes in order to answer our research questions.\n\n\n\n\n\n\nTip\n\n\n\nWhat is new, here, is that we will compare the results from different studies to critically examine questions relating to results reproducibility:\n\nDo we see the same results when similar methods are used to collect data to address the same questions?\n\n\n\nWe shall be focusing our analysis work on response data contributed by PSYC122 students.\n\nBut we will critically examine whether the results of our analyses of PSYC122 data are or are not similar to the results of the analyses of data collected in other studies.\n\nWhen we do linear models, as you have seen, we usually need to report:\n\ninformation about the model;\nif the effects of the predictors are significant, and what the estimates of the effects tell us;\noverall, how well the model works to predict observed outcomes in our data.\n\nIn describing the impacts of predictors, we need to think about:\n\nthe direction of the effect – do values of the outcome variable increase or decrease given increasing values of the predictor?\nthe size of the effect – how much do values of the outcome variable increase or decrease given increasing values of the predictor?\n\n\n\n\n\n\n\nTip\n\n\n\nIn assessing results reproducibility, therefore, we may focus here on:\n\nWhether an effect is or is not significant in the datasets we are comparing;\nWhether the estimate of the coefficient for the slope of the effect has similar sign (positive or negative) or size (the value of the coefficient)."
  },
  {
    "objectID": "PSYC122/Week19.html#resources-for-you",
    "href": "PSYC122/Week19.html#resources-for-you",
    "title": "Statistics for Psychologists",
    "section": "Resources for you",
    "text": "Resources for you\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found under the Week 19 resources section title, on Moodle:\nLink to Moodle"
  },
  {
    "objectID": "PSYC122/Week19.html#lectures-video-recordings",
    "href": "PSYC122/Week19.html#lectures-video-recordings",
    "title": "Statistics for Psychologists",
    "section": "Lectures: video recordings",
    "text": "Lectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 15 minutes\nPart 2 of 4; about 13 minutes\nPart 3 of 4; about 20 minutes\nPart 4 of 4; about 20 minutes\nThe lectures have three main areas of focus.\n1. Thinking critically about how people vary, and about the robustness and generalizability of results in psychological science\nWe shall proceed to answer our research questions with the data we have collected with the help of PSYC122 students. As we do this, we should reflect on where the data come from – the fact that people vary, and results vary – and we should consider, critically, questions concerning key ideas:\n\nMethods reproducibility – Will a different researcher be able to get the same results if they analyze the same data?\nResults reproducibility – Will we get the same results if we collect new data using the same procedure?\n\nThese questions are part of our motivation for recommending open science practices.\n2. What the PSYC122 data tell us: the possible answers to our research questions\nWe have been working to develop understanding and skills using example questions, tasks and data developed within the health comprehension Clearly understood project. PSYC122 students contributed their responses to a version of the survey we have been using to collect data to find answers to our research questions. In our practical work, we will be examining the PSYC122 data but as we do so we should understand that no one study in psychological science will give us definitive answers to any interesting question about people and what they do. In the lecture, we reflect on how data from different studies – using the same methods, with the same aims – may nevertheless vary:\n\nvary in the data distributions\nand vary in the results that analyses indicate.\n\nCritically evaluating results across a series of studies, or replication attempts, is part of the process of accumulating evidence to build insight in psychological science, given measurement under uncertainty and limits in our samples.\n3. Growing in independence, working with free open R\nR is free open source software. This is critical to ensuring that when we do analyses with psychological data we can usefully share data and code in ways that help to build evidence and insight in psychological science.\nBut there is a bigger benefit: the vast, free, R knowledge ecosystem.\n\n\n\n\n\n\nTip\n\n\n\nEvery problem you ever have:\n\nsomeone has had it before\nsolved it\nand written a blog (or tweet or toot) or recorded a YouTube or TikTok about it\n\n\n\nIn the week 19 lecture, and in the practical materials, we start to look at how you can locate and use the information you need to do data analysis. But it is important that you recognize two things:\n\n\n\n\n\n\nImportant\n\n\n\n\nEverything you need is out there, you just need to learn how to find it.\nSharing knowledge about R represents a new way to share scientific information: maybe this – not books or journals – is the future.\n\n\n\n\nThe lectures: the implications of our results, the revolution in open science and being a part of the revolution\nLast, we set up questions for you to consider:\n\nWhat are the results of our analyses with the new data?\nWhat are the implications for health communication?\n\nThen we step back, and consider how we are learning to work in terms of the modern workflow, and the on-going revolution in open science, and how people now build and share knowledge."
  },
  {
    "objectID": "PSYC122/Week19.html#pre-lab-activity-1",
    "href": "PSYC122/Week19.html#pre-lab-activity-1",
    "title": "Statistics for Psychologists",
    "section": "Pre-lab activity 1",
    "text": "Pre-lab activity 1\nIn weeks 16-19, we have been working together on a research project to investigate how people vary in their response to health advice. Completing the project involved collecting responses from PSYC122 students. In our class activities, this week, we will analyze the data we collected up to Monday 7th. March.\nThe link to the survey is here, for your information:\nhere\nThe survey is now officially closed.\nAlso or your information, you can read the pre-registered research plan for the PSYC122 health advice research project, here:\nRead the project pre-registration\nLooking at the survey or at the pre-registration are entirely optional activities."
  },
  {
    "objectID": "PSYC122/Week19.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "href": "PSYC122/Week19.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "title": "Statistics for Psychologists",
    "section": "Pre-lab activity 2: Getting ready for the lab class",
    "text": "Pre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w19_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nAs well as the data compiled from 2022-23 PSYC122 students\n\n2022-23_PSYC122-subjects.csv\n\nAs well as the code files:\n\n2022-23-PSYC122-w19-how-to.R\n2022-23-PSYC122-w19-workbook.R\n\nYou will use 2022-23-PSYC122-w19-workbook.R in the lab activity practical class.\nAlternatively, you can instead download the resources you need from the week 189 section of the Moodle page for the PSYC122 module:\nLink to Moodle\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\n\nOur aim is to make sure you can work with code, and write notes in the .R files.\n\n\n\nIn the workbook.R file you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\nIn both the .R files:\n\n2022-23-PSYC122-w19-how-to.R\n2022-23-PSYC122-w19-workbook.R\n\nwe will take things step-by-step.\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\nThe how-to guide comprises an .R file 2022-23-PSYC122-w19-how-to.R with code and advice. The code in the .R file was written to work – this week – with two data files:\n\nstudy-one-general-participants.csv.\nstudy-two-general-participants.csv.\n\nThis is so that you can learn how to:\n\nanalyse data from more than one study;\ncompare the results from the analyses of data from different studies to assess the robustness or generalizability of findings.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 7) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity."
  },
  {
    "objectID": "PSYC122/Week19.html#sec-w19-activity",
    "href": "PSYC122/Week19.html#sec-w19-activity",
    "title": "Statistics for Psychologists",
    "section": "Lab activity",
    "text": "Lab activity\nIn the lab activity .R file 2022-23-PSYC122-w19-workbook.R, you will work with data from two studies about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv which you have already seen before;\n2022-23_PSYC122-subjects.csv which is new, comprising the responses contributed by 2022-23 PSYC122 students.\n\n\nTasks\nIn the activity, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library(): notice we use some new ones here\nRead in the data files – using read_csv()\nInspect the data – using head() and summary()\nPlot the distributions of some key variables\nLearn how to produce grids of plots, showing them side-by-side, for easy comparison\nWork to produce grids of plots to consolidate skills\nProduce scatterplots to examine potential associations\nProduce grids of scatterplots, showing them side-by-side to allow comparison of potential associations\nExamine the association between accuracy of understanding and rated accuracy using cor.test()\nWork out how to locate useful online information about R functions or libraries\nEstimate the way in which an outcome may vary, given different values in a predictor variable – using lm()\nDo this analysis using the 2022-23 PSYC122 data, and evaluate the robustness of results across a series of studies\nPractise visualizing model predictions using ggpredict()\n\nThe 2022-23-PSYC122-w19-workbook.R file takes you through the tasks, one by one.\nIf you are unsure about what you need to do, check in the how-to guide: look at the advice in 2022-23-PSYC122-w19-how-to.R.\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the dataset or the variables to complete the tasks in the activity.\nThis process of adapting demonstration code is a process critical to data literacy and to effective problem solving in working with data in psychological science.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC122/summary.html",
    "href": "PSYC122/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "PSYC122/Week12.html",
    "href": "PSYC122/Week12.html",
    "title": "Week 12: Correlation 2",
    "section": "",
    "text": "Written by Margriet Groen (partly adapted from materials developed by the PsyTeachR team a the University of Glasgow)\n\nToday we will continue a look at correlation as a measure of association between two numerical variables. We will review assumptions associated with correlation, discuss some issues important to be aware of when interpreting correlation results and finally, we’ll talk about intercorrelation.\n\n\nThe lecture material for this week is presented in two parts:\n\nCorrelation – Assumption, issues and intercorrelation – Theory\nCorrelation – Assumption, issues and intercorrelation – How to\n\n\n\n\nThe reading that accompanies the lectures this week is again from chapter 9 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden.\nRougly, last week we covered the material in sections 9.1 to 9.4, as well as sections 9.8 to 9.11 and section 9.15. This week, we’ll cover the material in sections 9.5 to 9.7, and sections 9.12 to 9.13. Even if a section is not mentioned here, all of chapter 9 is relevant.\n\n\n\nAfter having watched the lectures on correlation and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nData comes in lots of different formats. One of the most common formats is that of a two-dimensional table (the two dimensions being rows and columns). Usually, each row stands for a separate observation (e.g. a participant), and each column stands for a different variable (e.g. a response, category, or group). A key benefit of tabular data is that it allows you to store different types of data-numerical measurements, alphanumeric labels, categorical descriptors-all in one place.\nIt may surprise you to learn that scientists actually spend far more of time cleaning and preparing their data than they spend actually analysing it. This means completing tasks such as cleaning up bad values, changing the structure of tables, merging information stored in separate tables, reducing the data down to a subset of observations, and producing data summaries. Some have estimated that up to 80% of time spent on data analysis involves such data preparation tasks (Dasu & Johnson, 2003)!\nMany people seem to operate under the assumption that the only option for data cleaning is the painstaking and time-consuming cutting and pasting of data within a spreadsheet program like Excel. We have witnessed students and colleagues waste days, weeks, and even months manually transforming their data in Excel, cutting, copying, and pasting data. Fixing up your data by hand is not only a terrible use of your time, but it is error-prone and not reproducible. Additionally, in this age where we can easily collect massive datasets online, you will not be able to organise, clean, and prepare these by hand.\nIn short, you will not thrive as a psychologist if you do not learn some key data wrangling skills. Although every dataset presents unique challenges, there are some systematic principles you should follow that will make your analyses easier, less error-prone, more efficient, and more reproducible.\nIn the online tutorial, you will see how data science skills will allow you to efficiently get answers to nearly any question you might want to ask about your data. By learning how to properly make your computer do the hard and boring work for you, you can focus on the bigger issues.\nYou’ll be practising the select(), filter(), mutate(), arrange(), group_by() and summarise() functions from the dplyr package.\nYou’ve used these functions before, but if you’d like to quickly remind yourself what they do, watch the video (~10 min) on Data wrangling: dplyr and pipes. As the title suggests, I also explain in the video what a ‘pipe’ (this thing: %&gt;%) is and you’ll be practising with that as well.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nWorking with Tibbles Practise how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.\n\n\n\n\n\n\nDownload the 122_week12_forStudents.zip file and upload it into the new folder in RStudio Server you created (see last week’s Pre-lab activity 4 for instructions on how to do that.\n\n\n\n\n\nIn this lab, you’ll gain understanding of and practice with:\n\nconstructing and interpreting histograms and qq-plots\nconstructing and interpreting a matrix of scatterplots\nrunning intercorrelation analysis and interpret the results\ncorrect for multiple comparisons when running intercorrelation analysis\nconstructing a correlation matrix in APA format\nwhen and why to apply correlation analysis to answers questions in psychological science\n\n\n\n\n\nCorrelation would be an appropriate form on analysis for researchers interested in the relationship between:\n\nDog (breed) and height (cm) of owner\nSpeed of swimming (mph) and area of tank (cm)\nNumber of cows sitting and rain fall (mm)\nTotal llama saliva (ml) expelled and gender of visitors\nb and d\nb and c\n\n\n\n\nWhen would you use Spearman’s rho analysis instead of Pearson’s r?\n\nWhen there are clear outliers in the data\nWhen the data is not normally distributed\nWhen the relationship between X and Y is curvilinear\na and b\n\n\n\n\nUsing the histograms and qq-plots below, which of these variables satisfies the normality assumption? Explain your answers.\nHistogram non-words \nQQ-plot non-words \nHistogram words \nQQ-plot words \nHistogram vocabulary \nQQ-plot non-words \n\n\n\nWhy should correlation analysis not be conducted on variables with a curvilinear relationship?\n\n\n\n\nGreat work so far! Now we really want to see what you can do yourself. In this activity we’ll use real data on implicit and explicit attitudes towards vaping. You’ll need the data file VapingData.csv and the R-script 122_wk12_labAct2_template.R that you downloaded when completing Pre-lab activity 2.\n\n\nExplicit attitudes were measured via a questionnaire where higher scores indicated a positive attitude towards vaping (VapingQuestionaireScore). Implicit attitudes were measured through an Implicit Association Test (IAT) using images of Vaping and Kitchen utensils and associating them with positive and negative words.\nThe IAT works on the principal that associations that go together (that are congruent, e.g. warm and sun) should be quicker to respond to than associations that do not go together (that are incongruent, e.g. warm and ice). You can read up more on the procedure on the Noba Project which has a good description of the procedure under the section “Subtle/Nonsconscious Research Methods”.\nFor this exercise, you need to know that “Block 3” in the experiment tested reaction times and accuracy towards congruent associations, pairing positive words with Kitchen utensils and negative words with Vaping. “Block 5” in the experiment tested reaction times and accuracy towards incongruent associations, pairing positive words with Vaping and negative words with Kitchen Utensils. As such, if reaction times were longer in Block 5 than in Block 3 then people are considered to hold the view that Vaping is negative (i.e. congruent associations are quicker than incongruent associations). However, if reaction times were quicker in Block 5 than in Block 3 then people are considered to hold the view that Vaping is positive (i.e. incongruent associations were quicker than congruent associations). The difference between reaction times in Block5 and Block3 is called the participants’ IAT score.\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. You can do this by clicking on the broom icon at the top of the environment window, or you can use the code below.\n\nTASK: Use the code snippet below to clear the environment. TIP: If you hover your mouse over the box that includes the code snippet, a ‘copy to clipboard’ icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script. \n\n\nrm(list=ls())                            \n\n\n\n\nNow, make sure your working directory is set to the folder in which you have stored the data file (VapingData.csv).\n\nTASK: Use the code snippet below to check what you working directory is currently set to. This is the folder that R will use to look for files. Is the file path that is written to the Console after you run the code snippet the one that contains the data file? You can check by nativating to the path you can see in the Console in the ‘Files’ pane on the right. Does it contain the data file (‘VapingData.csv’)?\n\n\ngetwd()\n\nIf your working directory is not set to the folder that contains the data file, navigate to folder that contains the data file in the ‘Files’ pane, click ‘More’ and then on ‘Set as working directory’.\n\n\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, car, Hmisc, lsr and tidyverse.\n\nTASK: Load the relevant libraries. HINT: Use the library() function.\n\n\n\n\nThe data file we’ll be working with is VapingData.csv\n\nTASK: Read in the data file (using the read_csv() function) and store it in an object called dat. Have a look at the layout of the data and familiarise yourself with it. You have 8 columns. Reaction times and Accuracy scores for Blocks 3 and 5 as well as the Explicit Vaping Questionnaire scores, Sex and Age, for each participant.\n\nQUESTION 1: For how many participants do we have data?\n\n\n\nThe data are not in a shape yet that we can actually use for our analysis. We’ll have to do some ’data wrangling to knock them into shape. We need to take the following into account:\n\nAccuracy was calculated as proportion and as such can’t go above 1.Participants entered their own data so some might have made a mistake. Remove participants who had an accuracy greater than 1 in either Block 3 or Block 5 as we are unclear on the accuracy of these values.\nWe also only want participants who were paying attention so best remove anybody whose average accuracy score across Blocks 3 and 5 was less than 80%. Note - this value is arbitrary and if you wanted, in your own experiment, you could use a more relaxed or strict cut-off based on other studies or guidance. Note that these decisions should be set out at the start of your research as part of your pre-registration or as part of your Registered Report. Finally, in this instance, remember, the values are in proportions not percentages (so 80% will be .8).\nNow that we have removed data points that were the result of data entry mistakes or came participants who didn’t pay attention during the task, we create an IAT score for participants by subtracting Block 3 reaction times (RT) from Block 5 reaction times (IAT_BLOCK5_RT - IAT_BLOCK3_RT).\n\n\nTASK: Look closely at each line of code below and check you understand what it does. Copy the code to your script and for each line add a comment to describe what it does.\n\n\ndat &lt;- dat %&gt;% \n  filter(IAT_BLOCK3_Acc &lt; 1) %&gt;%\n  filter(IAT_BLOCK5_Acc &lt; 1) %&gt;%\n  mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %&gt;%\n  filter(IAT_ACC &gt; .8) %&gt;%\n  mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT)\n\nQUESTION 2: For how many participants do we have data now that we have cleaned them up?\nQUESTION 3: Use the information in the background description to understand how the scores relate to attitudes. What does a positive IAT_RT score reflect? What does a negative IAT_RT score reflect? What does a higher score on the ‘VapingQuestionnaireScore’ mean?\n\n\n\nNow that we have the variables that we need and the data cleaned up, we will create a descriptives summary of the number of people, and the means for the IAT and Vaping Questionnaire Score.\n\nTASK: Look closely at each line of code below and check you understand what it does. Copy the code to your script and for each line add a comment to describe what it does.\n\n\ndescriptives &lt;- dat %&gt;%\n  summarise(n = n(),\n            mean_IAT_ACC = mean(IAT_ACC),\n            mean_IAT_RT = mean(IAT_RT),\n            mean_VPQ = mean(VapingQuestionnaireScore, na.rm = TRUE))\n\nQUESTION 4: Why might these averages be useful? Why are averages not always useful in correlations?\n\n\n\nVariable types\nQUESTION 5: What are the variable types for the implicit (IAT_RT) and the explicit (VapingQuestionnaireScore) attitude variables?\nMissing data\n\nTASK: Remove participants with missing data. HINT: Use the filter() function and the is.na() function\n\nQUESTION 6: How many people had missing data?\nNormality\n\nTASK: Create histograms and qq-plots for the IAT_RT and VapingQuestionnaireScore variables. HINT: Use the ggplot() function with geom_histogram() and use the qqPlot() function (note the capital P)\n\nQUESTION 7: What do you conclude from the histograms and the qq-plots? Are the VapingQuestionnaireScore and the IAT_RT normally distributed?\nLinearity and homoscedasticity\n\nTASK: Plot the relationship between IAT_RT and VapingQuestionnaireScore using a scatterplot and a line of best fit. HINT: For this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels.\n\nQUESTION 8: What do you conclude from the scatterplot in terms of the homoscedasticity of the data and the linearity, direction and strength of the relationship? What does the scatterplot tell you about possible issues (outliers, range restrictions)?\n\n\n\nQUESTION 9: Do you need to calculate Pearson’s r or Spearman’s rho?\n\nTASK: Conduct a correlation analysis. HINT: Use the cor.test() function. You may want to use the pull() and the round() functions to get the numbers out.\n\nQUESTION 10: Can you write up the results including the r, df, p-value and an interpretation?\n\n\n\nFinally, let’s check whether either implicit or explicit attitude is associated with age.\nFirst, let’s create a new data frame that only includes the relevant variables. Look closely at each line of code below and check you understand what it does. Don’t forget to copy the code below to your script and run it.\n\ndat_matrix &lt;- dat %&gt;%\n  select(Age, IAT_RT, VapingQuestionnaireScore) %&gt;%\n  as.data.frame(dat_matrix) # Make sure tell R that dat is a data frame\n\n\nTASK: Now, create a matrix of scatterplots. HINT: Use the pairs() function.\n\nQUESTION 11: What do you conclude from the scatterplots?\n\nTASK: Finally, conduct intercorrelation (multiple correlations). HINT: Use the correlate() function. Do you need Pearson’s r or Spearman’s rho? Have you adjusted for multiple comparisons?\n\nQUESTION 12: What do you conclude from the results of the correlation analysis?\n\n\n\n\n\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nCorrelation would be an appropriate form on analysis for researchers interested in the relationship between\n\n\n\nDog (breed) and height (cm) of owner\n\n\nSpeed of swimming (mph) and area of tank (cm)\n\n\nNumber of cows sitting and rain fall (mm)\n\n\nTotal llama saliva (ml) expelled and gender of visitors\n\n\nb and d\n\nf. b and c All variables in b anc c are continuous. Dog breed and gender are categorical.\n\n\nWhen would you use Spearman’s rho analysis instead of Pearson’s r?\n\n\n\nWhen there are clear outliers in the data\n\nb. When the data is not normally distributed\n\nWhen the relationship between X and Y is curvilinear\n\n\na and b\n\n\n\nUsing the histograms and qq-plots below, which of these variables satisfies the normality assumption? Explain your answers. Vocabulary. Only for vocabulary does the histogram resemble a bell curve and do the data-points in the qq-plot fall within the dashed blue lines.\nWhy should correlation analysis not be conducted on variables with a curvilinear relationship? May be subject to a type 2 error  there actually is a relationship between variables yet we reject the null hypothesis. As the relationship is not linear, correlation analysis will not identify this.\n\n\n\n\nYou can download the R-script that contains the code to complete lab activity 2 here: 122_wk12_labAct2.R.\n\nFor how many participants do we have data? There are 166 observations, so we have data for 166 participants. You can see this in the Environment window in the top right. This does not tell us whether any of these participants have any missing data.\nFor how many participants do we have data now that we have cleaned them up? 104 participants\nUse the information in the background description to understand how the scores relate to attitudes. What does a positive IAT_RT score reflect? People with a positive IAT_RT are considered to hold the implicit view that vaping is negative (i.e. congruent associations are quicker than incongruent associations) What does a negative IAT_RT score reflect? People with a negative IAT_RT are considered to hold the implicit view that vaping is positive (i.e. incongruent associations were quicker than congruent associations). What does a higher score on the ‘VapingQuestionnaireScore’ mean? Higher scores indicated a positive explicit attitude towards vaping.\nWhy might these averages be useful? Why are averages not always useful in correlations? It is always worth thinking about which averages are informative and which are not. Knowing the average explicit attitude towards vaping could well be informative. In contrast, if you are using an ordinal scale and people use the whole of the scale then the average may just tell you the middle of the scale you are using - which you already know and really isn’t that informative. So it is always worth thinking about what your descriptives are calculating.\nWhat are the variable types for the implicit (IAT_RT) and the explicit (VapingQuestionnaireScore) attitude variables? Both can be considered continuous variables and at least at interval level.\nHow many people had missing data? 8. Before we removed participants with missing data, we had 104 observations, now we have 96. So there must have been 8 participants without a score on one or the other variable.\nWhat do you conclude from the histograms and the qq-plots? Are the VapingQuestionnaireScore and the IAT_RT normally distributed? Yes. Both histograms resemble a normal distribution (bell curve) and the open circles in the qq-plots fall within the blue stripy lines.\nWhat do you conclude from the scatterplot in terms of the homoscedasticity of the data and the linearity, direction and strength of the relationship? What does the scatterplot tell you about possible issues (outliers, range restrictions)? The data look like a cloud without a clear direction. This suggests the relationship might might be weak. In terms of linearity, the scatterplot doesn’t suggest any curvilinear relationships. Variance seems quite constant, but there do seem to be few people with negative IAT_RT (Implicit attitude) scores, suggesting few people held the view that vaping is positive.\nDo you need to calculate Pearson’s r or Spearman’s rho? Pearson’s r because the data do meet the assumptions.\nCan you write up the results including the r, df, p-value and an interpretation? Testing the hypothesis of a relationship between implicit and explicit attitudes towards vaping, a Pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(94) = -.02, p = .822. Overall this suggests that there is no direct relationship between implicit and explicit attitudes with regard to vaping and as such our hypothesis was not supported; we cannot reject the null hypothesis.\nWhat do you conclude from the scatterplots? The scatterplots with age suggest that age is highly skewed with only a few participants older than 25. For now, let’s say we’ll therefore calculate Spearman’s rho, rather than Pearson’s r. That is ok for now, but if you were analysing these data for a research project, you’d want to have a closer look at the age variable (think histogram, qq-plot, and think about either collecting more data from older participants or transforming the variable (more about that next year).\nWhat do you conclude from the results of the correlation analysis? No significant correlation with age was found."
  },
  {
    "objectID": "PSYC122/Week12.html#lectures",
    "href": "PSYC122/Week12.html#lectures",
    "title": "Week 12: Correlation 2",
    "section": "",
    "text": "The lecture material for this week is presented in two parts:\n\nCorrelation – Assumption, issues and intercorrelation – Theory\nCorrelation – Assumption, issues and intercorrelation – How to"
  },
  {
    "objectID": "PSYC122/Week12.html#reading",
    "href": "PSYC122/Week12.html#reading",
    "title": "Week 12: Correlation 2",
    "section": "",
    "text": "The reading that accompanies the lectures this week is again from chapter 9 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden.\nRougly, last week we covered the material in sections 9.1 to 9.4, as well as sections 9.8 to 9.11 and section 9.15. This week, we’ll cover the material in sections 9.5 to 9.7, and sections 9.12 to 9.13. Even if a section is not mentioned here, all of chapter 9 is relevant."
  },
  {
    "objectID": "PSYC122/Week12.html#pre-lab-activities",
    "href": "PSYC122/Week12.html#pre-lab-activities",
    "title": "Week 12: Correlation 2",
    "section": "",
    "text": "After having watched the lectures on correlation and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nData comes in lots of different formats. One of the most common formats is that of a two-dimensional table (the two dimensions being rows and columns). Usually, each row stands for a separate observation (e.g. a participant), and each column stands for a different variable (e.g. a response, category, or group). A key benefit of tabular data is that it allows you to store different types of data-numerical measurements, alphanumeric labels, categorical descriptors-all in one place.\nIt may surprise you to learn that scientists actually spend far more of time cleaning and preparing their data than they spend actually analysing it. This means completing tasks such as cleaning up bad values, changing the structure of tables, merging information stored in separate tables, reducing the data down to a subset of observations, and producing data summaries. Some have estimated that up to 80% of time spent on data analysis involves such data preparation tasks (Dasu & Johnson, 2003)!\nMany people seem to operate under the assumption that the only option for data cleaning is the painstaking and time-consuming cutting and pasting of data within a spreadsheet program like Excel. We have witnessed students and colleagues waste days, weeks, and even months manually transforming their data in Excel, cutting, copying, and pasting data. Fixing up your data by hand is not only a terrible use of your time, but it is error-prone and not reproducible. Additionally, in this age where we can easily collect massive datasets online, you will not be able to organise, clean, and prepare these by hand.\nIn short, you will not thrive as a psychologist if you do not learn some key data wrangling skills. Although every dataset presents unique challenges, there are some systematic principles you should follow that will make your analyses easier, less error-prone, more efficient, and more reproducible.\nIn the online tutorial, you will see how data science skills will allow you to efficiently get answers to nearly any question you might want to ask about your data. By learning how to properly make your computer do the hard and boring work for you, you can focus on the bigger issues.\nYou’ll be practising the select(), filter(), mutate(), arrange(), group_by() and summarise() functions from the dplyr package.\nYou’ve used these functions before, but if you’d like to quickly remind yourself what they do, watch the video (~10 min) on Data wrangling: dplyr and pipes. As the title suggests, I also explain in the video what a ‘pipe’ (this thing: %&gt;%) is and you’ll be practising with that as well.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nWorking with Tibbles Practise how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.\n\n\n\n\n\n\nDownload the 122_week12_forStudents.zip file and upload it into the new folder in RStudio Server you created (see last week’s Pre-lab activity 4 for instructions on how to do that."
  },
  {
    "objectID": "PSYC122/Week12.html#lab-activities",
    "href": "PSYC122/Week12.html#lab-activities",
    "title": "Week 12: Correlation 2",
    "section": "",
    "text": "In this lab, you’ll gain understanding of and practice with:\n\nconstructing and interpreting histograms and qq-plots\nconstructing and interpreting a matrix of scatterplots\nrunning intercorrelation analysis and interpret the results\ncorrect for multiple comparisons when running intercorrelation analysis\nconstructing a correlation matrix in APA format\nwhen and why to apply correlation analysis to answers questions in psychological science\n\n\n\n\n\nCorrelation would be an appropriate form on analysis for researchers interested in the relationship between:\n\nDog (breed) and height (cm) of owner\nSpeed of swimming (mph) and area of tank (cm)\nNumber of cows sitting and rain fall (mm)\nTotal llama saliva (ml) expelled and gender of visitors\nb and d\nb and c\n\n\n\n\nWhen would you use Spearman’s rho analysis instead of Pearson’s r?\n\nWhen there are clear outliers in the data\nWhen the data is not normally distributed\nWhen the relationship between X and Y is curvilinear\na and b\n\n\n\n\nUsing the histograms and qq-plots below, which of these variables satisfies the normality assumption? Explain your answers.\nHistogram non-words \nQQ-plot non-words \nHistogram words \nQQ-plot words \nHistogram vocabulary \nQQ-plot non-words \n\n\n\nWhy should correlation analysis not be conducted on variables with a curvilinear relationship?\n\n\n\n\nGreat work so far! Now we really want to see what you can do yourself. In this activity we’ll use real data on implicit and explicit attitudes towards vaping. You’ll need the data file VapingData.csv and the R-script 122_wk12_labAct2_template.R that you downloaded when completing Pre-lab activity 2.\n\n\nExplicit attitudes were measured via a questionnaire where higher scores indicated a positive attitude towards vaping (VapingQuestionaireScore). Implicit attitudes were measured through an Implicit Association Test (IAT) using images of Vaping and Kitchen utensils and associating them with positive and negative words.\nThe IAT works on the principal that associations that go together (that are congruent, e.g. warm and sun) should be quicker to respond to than associations that do not go together (that are incongruent, e.g. warm and ice). You can read up more on the procedure on the Noba Project which has a good description of the procedure under the section “Subtle/Nonsconscious Research Methods”.\nFor this exercise, you need to know that “Block 3” in the experiment tested reaction times and accuracy towards congruent associations, pairing positive words with Kitchen utensils and negative words with Vaping. “Block 5” in the experiment tested reaction times and accuracy towards incongruent associations, pairing positive words with Vaping and negative words with Kitchen Utensils. As such, if reaction times were longer in Block 5 than in Block 3 then people are considered to hold the view that Vaping is negative (i.e. congruent associations are quicker than incongruent associations). However, if reaction times were quicker in Block 5 than in Block 3 then people are considered to hold the view that Vaping is positive (i.e. incongruent associations were quicker than congruent associations). The difference between reaction times in Block5 and Block3 is called the participants’ IAT score.\n\n\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. You can do this by clicking on the broom icon at the top of the environment window, or you can use the code below.\n\nTASK: Use the code snippet below to clear the environment. TIP: If you hover your mouse over the box that includes the code snippet, a ‘copy to clipboard’ icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script. \n\n\nrm(list=ls())                            \n\n\n\n\nNow, make sure your working directory is set to the folder in which you have stored the data file (VapingData.csv).\n\nTASK: Use the code snippet below to check what you working directory is currently set to. This is the folder that R will use to look for files. Is the file path that is written to the Console after you run the code snippet the one that contains the data file? You can check by nativating to the path you can see in the Console in the ‘Files’ pane on the right. Does it contain the data file (‘VapingData.csv’)?\n\n\ngetwd()\n\nIf your working directory is not set to the folder that contains the data file, navigate to folder that contains the data file in the ‘Files’ pane, click ‘More’ and then on ‘Set as working directory’.\n\n\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we’ll need broom, car, Hmisc, lsr and tidyverse.\n\nTASK: Load the relevant libraries. HINT: Use the library() function.\n\n\n\n\nThe data file we’ll be working with is VapingData.csv\n\nTASK: Read in the data file (using the read_csv() function) and store it in an object called dat. Have a look at the layout of the data and familiarise yourself with it. You have 8 columns. Reaction times and Accuracy scores for Blocks 3 and 5 as well as the Explicit Vaping Questionnaire scores, Sex and Age, for each participant.\n\nQUESTION 1: For how many participants do we have data?\n\n\n\nThe data are not in a shape yet that we can actually use for our analysis. We’ll have to do some ’data wrangling to knock them into shape. We need to take the following into account:\n\nAccuracy was calculated as proportion and as such can’t go above 1.Participants entered their own data so some might have made a mistake. Remove participants who had an accuracy greater than 1 in either Block 3 or Block 5 as we are unclear on the accuracy of these values.\nWe also only want participants who were paying attention so best remove anybody whose average accuracy score across Blocks 3 and 5 was less than 80%. Note - this value is arbitrary and if you wanted, in your own experiment, you could use a more relaxed or strict cut-off based on other studies or guidance. Note that these decisions should be set out at the start of your research as part of your pre-registration or as part of your Registered Report. Finally, in this instance, remember, the values are in proportions not percentages (so 80% will be .8).\nNow that we have removed data points that were the result of data entry mistakes or came participants who didn’t pay attention during the task, we create an IAT score for participants by subtracting Block 3 reaction times (RT) from Block 5 reaction times (IAT_BLOCK5_RT - IAT_BLOCK3_RT).\n\n\nTASK: Look closely at each line of code below and check you understand what it does. Copy the code to your script and for each line add a comment to describe what it does.\n\n\ndat &lt;- dat %&gt;% \n  filter(IAT_BLOCK3_Acc &lt; 1) %&gt;%\n  filter(IAT_BLOCK5_Acc &lt; 1) %&gt;%\n  mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %&gt;%\n  filter(IAT_ACC &gt; .8) %&gt;%\n  mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT)\n\nQUESTION 2: For how many participants do we have data now that we have cleaned them up?\nQUESTION 3: Use the information in the background description to understand how the scores relate to attitudes. What does a positive IAT_RT score reflect? What does a negative IAT_RT score reflect? What does a higher score on the ‘VapingQuestionnaireScore’ mean?\n\n\n\nNow that we have the variables that we need and the data cleaned up, we will create a descriptives summary of the number of people, and the means for the IAT and Vaping Questionnaire Score.\n\nTASK: Look closely at each line of code below and check you understand what it does. Copy the code to your script and for each line add a comment to describe what it does.\n\n\ndescriptives &lt;- dat %&gt;%\n  summarise(n = n(),\n            mean_IAT_ACC = mean(IAT_ACC),\n            mean_IAT_RT = mean(IAT_RT),\n            mean_VPQ = mean(VapingQuestionnaireScore, na.rm = TRUE))\n\nQUESTION 4: Why might these averages be useful? Why are averages not always useful in correlations?\n\n\n\nVariable types\nQUESTION 5: What are the variable types for the implicit (IAT_RT) and the explicit (VapingQuestionnaireScore) attitude variables?\nMissing data\n\nTASK: Remove participants with missing data. HINT: Use the filter() function and the is.na() function\n\nQUESTION 6: How many people had missing data?\nNormality\n\nTASK: Create histograms and qq-plots for the IAT_RT and VapingQuestionnaireScore variables. HINT: Use the ggplot() function with geom_histogram() and use the qqPlot() function (note the capital P)\n\nQUESTION 7: What do you conclude from the histograms and the qq-plots? Are the VapingQuestionnaireScore and the IAT_RT normally distributed?\nLinearity and homoscedasticity\n\nTASK: Plot the relationship between IAT_RT and VapingQuestionnaireScore using a scatterplot and a line of best fit. HINT: For this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels.\n\nQUESTION 8: What do you conclude from the scatterplot in terms of the homoscedasticity of the data and the linearity, direction and strength of the relationship? What does the scatterplot tell you about possible issues (outliers, range restrictions)?\n\n\n\nQUESTION 9: Do you need to calculate Pearson’s r or Spearman’s rho?\n\nTASK: Conduct a correlation analysis. HINT: Use the cor.test() function. You may want to use the pull() and the round() functions to get the numbers out.\n\nQUESTION 10: Can you write up the results including the r, df, p-value and an interpretation?\n\n\n\nFinally, let’s check whether either implicit or explicit attitude is associated with age.\nFirst, let’s create a new data frame that only includes the relevant variables. Look closely at each line of code below and check you understand what it does. Don’t forget to copy the code below to your script and run it.\n\ndat_matrix &lt;- dat %&gt;%\n  select(Age, IAT_RT, VapingQuestionnaireScore) %&gt;%\n  as.data.frame(dat_matrix) # Make sure tell R that dat is a data frame\n\n\nTASK: Now, create a matrix of scatterplots. HINT: Use the pairs() function.\n\nQUESTION 11: What do you conclude from the scatterplots?\n\nTASK: Finally, conduct intercorrelation (multiple correlations). HINT: Use the correlate() function. Do you need Pearson’s r or Spearman’s rho? Have you adjusted for multiple comparisons?\n\nQUESTION 12: What do you conclude from the results of the correlation analysis?"
  },
  {
    "objectID": "PSYC122/Week12.html#answers",
    "href": "PSYC122/Week12.html#answers",
    "title": "Week 12: Correlation 2",
    "section": "",
    "text": "When you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nCorrelation would be an appropriate form on analysis for researchers interested in the relationship between\n\n\n\nDog (breed) and height (cm) of owner\n\n\nSpeed of swimming (mph) and area of tank (cm)\n\n\nNumber of cows sitting and rain fall (mm)\n\n\nTotal llama saliva (ml) expelled and gender of visitors\n\n\nb and d\n\nf. b and c All variables in b anc c are continuous. Dog breed and gender are categorical.\n\n\nWhen would you use Spearman’s rho analysis instead of Pearson’s r?\n\n\n\nWhen there are clear outliers in the data\n\nb. When the data is not normally distributed\n\nWhen the relationship between X and Y is curvilinear\n\n\na and b\n\n\n\nUsing the histograms and qq-plots below, which of these variables satisfies the normality assumption? Explain your answers. Vocabulary. Only for vocabulary does the histogram resemble a bell curve and do the data-points in the qq-plot fall within the dashed blue lines.\nWhy should correlation analysis not be conducted on variables with a curvilinear relationship? May be subject to a type 2 error  there actually is a relationship between variables yet we reject the null hypothesis. As the relationship is not linear, correlation analysis will not identify this.\n\n\n\n\nYou can download the R-script that contains the code to complete lab activity 2 here: 122_wk12_labAct2.R.\n\nFor how many participants do we have data? There are 166 observations, so we have data for 166 participants. You can see this in the Environment window in the top right. This does not tell us whether any of these participants have any missing data.\nFor how many participants do we have data now that we have cleaned them up? 104 participants\nUse the information in the background description to understand how the scores relate to attitudes. What does a positive IAT_RT score reflect? People with a positive IAT_RT are considered to hold the implicit view that vaping is negative (i.e. congruent associations are quicker than incongruent associations) What does a negative IAT_RT score reflect? People with a negative IAT_RT are considered to hold the implicit view that vaping is positive (i.e. incongruent associations were quicker than congruent associations). What does a higher score on the ‘VapingQuestionnaireScore’ mean? Higher scores indicated a positive explicit attitude towards vaping.\nWhy might these averages be useful? Why are averages not always useful in correlations? It is always worth thinking about which averages are informative and which are not. Knowing the average explicit attitude towards vaping could well be informative. In contrast, if you are using an ordinal scale and people use the whole of the scale then the average may just tell you the middle of the scale you are using - which you already know and really isn’t that informative. So it is always worth thinking about what your descriptives are calculating.\nWhat are the variable types for the implicit (IAT_RT) and the explicit (VapingQuestionnaireScore) attitude variables? Both can be considered continuous variables and at least at interval level.\nHow many people had missing data? 8. Before we removed participants with missing data, we had 104 observations, now we have 96. So there must have been 8 participants without a score on one or the other variable.\nWhat do you conclude from the histograms and the qq-plots? Are the VapingQuestionnaireScore and the IAT_RT normally distributed? Yes. Both histograms resemble a normal distribution (bell curve) and the open circles in the qq-plots fall within the blue stripy lines.\nWhat do you conclude from the scatterplot in terms of the homoscedasticity of the data and the linearity, direction and strength of the relationship? What does the scatterplot tell you about possible issues (outliers, range restrictions)? The data look like a cloud without a clear direction. This suggests the relationship might might be weak. In terms of linearity, the scatterplot doesn’t suggest any curvilinear relationships. Variance seems quite constant, but there do seem to be few people with negative IAT_RT (Implicit attitude) scores, suggesting few people held the view that vaping is positive.\nDo you need to calculate Pearson’s r or Spearman’s rho? Pearson’s r because the data do meet the assumptions.\nCan you write up the results including the r, df, p-value and an interpretation? Testing the hypothesis of a relationship between implicit and explicit attitudes towards vaping, a Pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(94) = -.02, p = .822. Overall this suggests that there is no direct relationship between implicit and explicit attitudes with regard to vaping and as such our hypothesis was not supported; we cannot reject the null hypothesis.\nWhat do you conclude from the scatterplots? The scatterplots with age suggest that age is highly skewed with only a few participants older than 25. For now, let’s say we’ll therefore calculate Spearman’s rho, rather than Pearson’s r. That is ok for now, but if you were analysing these data for a research project, you’d want to have a closer look at the age variable (think histogram, qq-plot, and think about either collecting more data from older participants or transforming the variable (more about that next year).\nWhat do you conclude from the results of the correlation analysis? No significant correlation with age was found."
  },
  {
    "objectID": "PSYC122/Part1.html",
    "href": "PSYC122/Part1.html",
    "title": "Intro",
    "section": "",
    "text": "This is a collection of tuition material written for Psychology undergraduates at Lancaster University. At the moment the content represents the “lab materials” for the PSYC122 module in first year. As was the case for PSYC121, they feature tuition of programming with R, building on the skills you developed last term.\n\n\nSome parts should be completed before you attend the lab session (watching lectures, reading chapters, pre-lab activities). All the links to the different materials and activities are also in the ‘to-do list’ for the relevant week on Moodle."
  },
  {
    "objectID": "PSYC122/Part1.html#analysis-labs-and-pre-lab-work",
    "href": "PSYC122/Part1.html#analysis-labs-and-pre-lab-work",
    "title": "Intro",
    "section": "",
    "text": "Some parts should be completed before you attend the lab session (watching lectures, reading chapters, pre-lab activities). All the links to the different materials and activities are also in the ‘to-do list’ for the relevant week on Moodle."
  },
  {
    "objectID": "PSYC122/Week13.html",
    "href": "PSYC122/Week13.html",
    "title": "Week 13: The linear model",
    "section": "",
    "text": "Written by Margriet Groen (partly adapted from materials developed by the PsyTeachR team a the University of Glasgow)\n\nThis week we will focus on the linear model and simple linear regression.\n\n\nThe lecture material for this week is presented in two parts:\n\nThe linear model (~25 min)\nHow to build a linear model in R (~30 min) You can find the example script in this week’s zip-folder (see under Pre-lab activity 3).\n\n\n\n\nThe reading that accompanies the lectures this week is from chapter 10 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden.\n\n\n\nAfter having watched the lectures and read the textbook chapter you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nHave a look at this visualisation of the regression line by Ryan Safner.\nIn this shiny app, you see a randomly-generated set of data points (within specific parameters, to keep the graph scaled properly). You can choose a slope and intercept for the regression line by using the sliders. The graph also displays the residuals as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger residuals. The shiny app also calculates the sum of squared errors (SSE) and the standard error of the regression (SER), which calculates the average size of the error (the red numbers). These numbers reflect how well the regression line fits the data, but you don’t need to worry about those for now.\nIn the app he uses the equation Y = aX + b in which b is the intercept and a is the slope.\nThis is slightly different from the equation you saw during the lecture. There we talked about Y = b0 + b1*X + e. Same equation, just different letters. So b0 in the lecture is equivalent to b in the app and b1 in the lecture is equivalent to a in the app.\nPre-lab activity questions:\n\nChange the slider for the intercept. How does it change the regression line?\nChange the slider for the slope. How does it change the regression line?\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line?\n\n\n\n\nIn this week’s online tutorials, you will practise visualing data.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nVisualisation basics Practise the basics of how to create a graph, how to add variables and how to make different types of graphs.\nScatterplots This tutorial revisits scatterplots. Along the way, you will learn to build multi-layer plots and to use new coordinate systems.\n\n\n\n\n\n\nDownload the 122_week13_forStudents.zip file and upload it into the new folder in RStudio Server you created.\n\n\n\n\n\nIn this lab, you’ll gain understanding of and practice with:\n\nconducting simple regression in R\ninterpreting simple regression in R\nreporting the results in APA format\nwhen and why to apply simple regression to answer questions in psychological science\n\n\n\n\n\nWhat is the regression equation as discussed during the lecture and what does each letter represent?\n\n\n\nWhat are residuals?\n\n\n\nDiscuss the answers to the pre-lab activity questions. What did you find?\n\nChange the slider for the intercept. How does it change the regression line? The value for y at x = 0 changes.\nChange the slider for the slope. How does it change the regression line? The steepness of the line changes.\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line? The distance between the fitted values (the line) and the observed values (the dots) increases. Therefore, the red dashed lines become longer suggesting that the residuals increase. The model therefore fits the data less well.\n\n\n\n\n\nIn this lab, we’ll be working with real data and using regression to explore the question of whether there is a relationship between statistics anxiety and engagement in course activities. You’ll need the data files psess.csv and stars2.csv and the R-script 122_wk13_labAct2_template.R that you downloaded when completing Pre-lab activity 3.\n\n\nThe hypothesis is that students who are more anxious about statistics are less likely to engage in course-related activities. This avoidance behaviour could ultimately be responsible for lower performance for these students (although we won’t be examining the assessment scores in this activity).\nWe are going to analyse data from the STARS Statistics Anxiety Survey, which was administered to students in the third-year statistics course in Psychology at the University of Glasgow. All the responses have been anonymised by associating the responses for each student with an arbitrary ID number (integer).\nThe STARS survey (Cruise, Cash, & Bolton, 1985) is a 51-item questionnaire, with each response on a 1 to 5 scale, with higher numbers indicating greater anxiety.\nCruise, R. J., Cash, R. W., & Bolton, D. L. (1985). Development and validation of an instrument to measure statistical anxiety. Proceedings of the American Statistical Association, Section on Statistical Education, Las Vegas, NV.\nExample items from the STARS survey\n\nAs a measure of engagement in the course, we will use data from Moodle usage analytics. Over the course of the term, there were eight optional weekly on-line sessions that students could attend for extra support. The variable n_weeks in the psess.csv file tells you how many (out of eight) a given student attended.\nOur hypothesis was that greater anxiety would be reflected in lower engagement. Answer the following question.\nQUESTION 1: If our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks?\nBefore we do anything else, let’s load the libraries that we need for this analysis.\n\nTASK: Load broom, car and tidyverse. HINT: Use library().\n\n\nTASK: Read in both files, have a look at the layout of the data and familiarise yourself with it. HINTYou can use the read_csv() and the head() function.\n\nQUESTION 2 In the stars table, what do the numbers in the first row across the three columns refer to?\nNow that we’ve read in both data files, the next step is to calculate the mean anxiety scores for each participant. At the moment we have scores on all questions separately for each participant in the stars table. Instead we need one mean anxiety score for each participant.\n\nTASK: Write the code to calculate mean anxiety scores. Remember that participant is identified by the ID variable. Store the resulting table in a variable named stars_mean. HINT: Use group_by() and summarise(). Also, remember to use na.rm = TRUE when calculating the mean scores to deal with participants who have missing data (NAs).\n\nQUESTION 3 What is the mean anxiety score for participant 3?\nOk, before we get ahead of ourselves, in order to perform the regression analysis we need to combine the data from stars (the mean anxiety scores) with the data from engage (n_weeks).\n\nTASK: Join the two tables, call the resulting table joined. HINT: Use the inner_join() function (making use of the variable that is common across both tables) to join.\n\nWe now need descriptive statistics for both variables.\n\nTASK: Calculate the mean and standard deviations for the anxiety scores and the engagement data.\n\nQUESTION 4 What are the means and standard deviation for anxiety and engagement with the statistics module?\nAs always, it is a good idea to visualise your data.Now that we have all the variables in one place, make a scatterplot of anxiety as a function of engagement.\n\nTASK: Write the code to create the scatterplot. HINT: For this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels with the labs() function.\n\nQUESTION 5 What does the scatterplot suggest about the relationship between anxiety and engagement?\nWith all the variables in place, we’re ready now to start building the regression model.\n\nTASK: Use the lm() function to run the regression model when you model engagement (the outcome variable) as a function of anxiety (the predictor variable) and use the summary() function to look at the output. HINT: lm(outcome ~ predictor, data = my_data).\n\nQUESTION 6 What is the estimate of the y-intercept for the model, rounded to three decimal places?\nQUESTION 7 To three decimal places, if the General Linear Model for this model is Y=beta0 + beta1X + e, then beta1 is …\nQUESTION 8 To three decimal places, for each unit increase in anxiety, engagement decreases by …\nQUESTION 9 To two decimal places, what is the overall F-ratio of the model?\nQUESTION 10 Is the overall model significant?\nQUESTION 11 What proportion of the variance does the model explain?\nNow that we’ve fitted a model, let’s check whether the model meets the assumptions of linearity, normality and homoscedasticity.\n\nTASK: Write the code to check the assumptions. HINT: crPlots() to check linearity, qqPlot() to check normality of the residuals, and residualPlot() to check homoscedasticity of the residuals.\n\nQUESTION 12 Does the relationship appear to be linear?\nQUESTION 13 Do the residuals show normality?\nQUESTION 14 Do the residuals show homoscedasticity?\nFinally, it’s time to write up the results following APA guidelines.\nQUESTION 15 What would the results section look like if you wrote them up, following APA guidelines? HINT: The Purdue writing lab website is helpful for guidance on punctuating statistics.\n\n\n\n\n\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nWhat is the regression equation as discussed during the lecture and what does each letter represent? Y = b0 + b1 * X + e Y = outcome variable b0 = intercept b1 = slope X = predictor variable e = error\nWhat are residuals? Residuals reflect the discrepancy between the observed values and the fitted values and give an indication of how well the model ‘fits’ the data.\nDiscuss the answers to the pre-lab activity questions. What did you find?\n\n\n\nChange the slider for the intercept. How does it change the regression line? The value for y at x = 0 changes.\n\n\nChange the slider for the slope. How does it change the regression line? The steepness of the line changes.\n\n\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line? The distance between the fitted values (the line) and the observed values (the dots) increases. Therefore, the red dashed lines become longer suggesting that the residuals increase. The model therefore fits the data less well.\n\n\n\n\n\nYou can download the R-script that contains the code to complete lab activity 2 here: 122_wk13_labAct2.R.\n\nIf our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks? A negative correlation\nIn the stars table, what do the numbers in the first row across the three columns refer to? ID = 3, Question = Q01 and Score = 1 shows us that participant 3 reported a score of 1 on question 1.\nWhat is the mean anxiety score for participant 3? 1.058824\nWhat are the means and standard deviation for anxiety and engagement with the statistics module? Anxiety M = 2.08, SD = 0.56; Engagement M = 4.54, SD = 2.42.\nWhat does the scatterplot suggest about the relationship between anxiety and engagement? That there might indeed be a relatively strong negative correlation between the two; students with more anxiety, engage less.\nWhat is the estimate of the y-intercept for the model, rounded to three decimal places? 9.057. Explanation: In the summary table, this is the estimate of the intercept.\nTo three decimal places, if the General Linear Model for this model is Y=beta0 + beta1X + e, then beta1 is … -2.173. Explanation: In the summary table, this is the estimate of mean_anxiety, i.e., the slope.\nTo three decimal places, for each unit increase in anxiety, engagement decreases by … 2.173. Explanation: In the summary table, this is also the estimate of mean_anxiety, the slope is how much it decreases so you just remove the - sign.\nTo two decimal places, what is the overall F-ratio of the model? 11.99. Explanation: In the summary table, the F-ratio is noted as the F-statistic.\nIs the overall model significant? Yes. Explanation: The overall model p-value is .001428 which is less than .05, therefore significant.\nWhat proportion of the variance does the model explain? 25.52%. Explanation: The variance explained is determined by R-squared, you simply multiple it by 100 to get the percent.\nDoes the relationship appear to be linear? Yes, the pink link roughly falls across the dashed blue line and looks mostly linear.\nDo the residuals show normality? Yes, in the qq-plot the open circles mostly assemble around the solid blue line, and fall mostly within the range of the dashed blue lines.\nDo the residuals show homoscedasticity? Yes, the residual plot shows that the spread of the residuals is roughly similar for different fitted values.\nWhat would the results section look like if you wrote them up, following APA guidelines? A simple linear regression was performed with engagement (M = 4.54, SD = 0.56) as the outcome variable and statistics anxiety (M = 2.08, SD = 0.56) as the predictor variable. The results of the regression indicated that the model significantly predicted course engagement (F(1, 35) = 11.99, p &lt; .001, R^2 = 0.25), accounting for 25% of the variance. Anxiety was a significant negative predictor (beta = -2.17, p &lt; 0.001): as anxiety increased, course engagement decreased."
  },
  {
    "objectID": "PSYC122/Week13.html#lectures",
    "href": "PSYC122/Week13.html#lectures",
    "title": "Week 13: The linear model",
    "section": "",
    "text": "The lecture material for this week is presented in two parts:\n\nThe linear model (~25 min)\nHow to build a linear model in R (~30 min) You can find the example script in this week’s zip-folder (see under Pre-lab activity 3)."
  },
  {
    "objectID": "PSYC122/Week13.html#reading",
    "href": "PSYC122/Week13.html#reading",
    "title": "Week 13: The linear model",
    "section": "",
    "text": "The reading that accompanies the lectures this week is from chapter 10 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden."
  },
  {
    "objectID": "PSYC122/Week13.html#pre-lab-activities",
    "href": "PSYC122/Week13.html#pre-lab-activities",
    "title": "Week 13: The linear model",
    "section": "",
    "text": "After having watched the lectures and read the textbook chapter you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nHave a look at this visualisation of the regression line by Ryan Safner.\nIn this shiny app, you see a randomly-generated set of data points (within specific parameters, to keep the graph scaled properly). You can choose a slope and intercept for the regression line by using the sliders. The graph also displays the residuals as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger residuals. The shiny app also calculates the sum of squared errors (SSE) and the standard error of the regression (SER), which calculates the average size of the error (the red numbers). These numbers reflect how well the regression line fits the data, but you don’t need to worry about those for now.\nIn the app he uses the equation Y = aX + b in which b is the intercept and a is the slope.\nThis is slightly different from the equation you saw during the lecture. There we talked about Y = b0 + b1*X + e. Same equation, just different letters. So b0 in the lecture is equivalent to b in the app and b1 in the lecture is equivalent to a in the app.\nPre-lab activity questions:\n\nChange the slider for the intercept. How does it change the regression line?\nChange the slider for the slope. How does it change the regression line?\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line?\n\n\n\n\nIn this week’s online tutorials, you will practise visualing data.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nVisualisation basics Practise the basics of how to create a graph, how to add variables and how to make different types of graphs.\nScatterplots This tutorial revisits scatterplots. Along the way, you will learn to build multi-layer plots and to use new coordinate systems.\n\n\n\n\n\n\nDownload the 122_week13_forStudents.zip file and upload it into the new folder in RStudio Server you created."
  },
  {
    "objectID": "PSYC122/Week13.html#lab-activities",
    "href": "PSYC122/Week13.html#lab-activities",
    "title": "Week 13: The linear model",
    "section": "",
    "text": "In this lab, you’ll gain understanding of and practice with:\n\nconducting simple regression in R\ninterpreting simple regression in R\nreporting the results in APA format\nwhen and why to apply simple regression to answer questions in psychological science\n\n\n\n\n\nWhat is the regression equation as discussed during the lecture and what does each letter represent?\n\n\n\nWhat are residuals?\n\n\n\nDiscuss the answers to the pre-lab activity questions. What did you find?\n\nChange the slider for the intercept. How does it change the regression line? The value for y at x = 0 changes.\nChange the slider for the slope. How does it change the regression line? The steepness of the line changes.\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line? The distance between the fitted values (the line) and the observed values (the dots) increases. Therefore, the red dashed lines become longer suggesting that the residuals increase. The model therefore fits the data less well.\n\n\n\n\n\nIn this lab, we’ll be working with real data and using regression to explore the question of whether there is a relationship between statistics anxiety and engagement in course activities. You’ll need the data files psess.csv and stars2.csv and the R-script 122_wk13_labAct2_template.R that you downloaded when completing Pre-lab activity 3.\n\n\nThe hypothesis is that students who are more anxious about statistics are less likely to engage in course-related activities. This avoidance behaviour could ultimately be responsible for lower performance for these students (although we won’t be examining the assessment scores in this activity).\nWe are going to analyse data from the STARS Statistics Anxiety Survey, which was administered to students in the third-year statistics course in Psychology at the University of Glasgow. All the responses have been anonymised by associating the responses for each student with an arbitrary ID number (integer).\nThe STARS survey (Cruise, Cash, & Bolton, 1985) is a 51-item questionnaire, with each response on a 1 to 5 scale, with higher numbers indicating greater anxiety.\nCruise, R. J., Cash, R. W., & Bolton, D. L. (1985). Development and validation of an instrument to measure statistical anxiety. Proceedings of the American Statistical Association, Section on Statistical Education, Las Vegas, NV.\nExample items from the STARS survey\n\nAs a measure of engagement in the course, we will use data from Moodle usage analytics. Over the course of the term, there were eight optional weekly on-line sessions that students could attend for extra support. The variable n_weeks in the psess.csv file tells you how many (out of eight) a given student attended.\nOur hypothesis was that greater anxiety would be reflected in lower engagement. Answer the following question.\nQUESTION 1: If our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks?\nBefore we do anything else, let’s load the libraries that we need for this analysis.\n\nTASK: Load broom, car and tidyverse. HINT: Use library().\n\n\nTASK: Read in both files, have a look at the layout of the data and familiarise yourself with it. HINTYou can use the read_csv() and the head() function.\n\nQUESTION 2 In the stars table, what do the numbers in the first row across the three columns refer to?\nNow that we’ve read in both data files, the next step is to calculate the mean anxiety scores for each participant. At the moment we have scores on all questions separately for each participant in the stars table. Instead we need one mean anxiety score for each participant.\n\nTASK: Write the code to calculate mean anxiety scores. Remember that participant is identified by the ID variable. Store the resulting table in a variable named stars_mean. HINT: Use group_by() and summarise(). Also, remember to use na.rm = TRUE when calculating the mean scores to deal with participants who have missing data (NAs).\n\nQUESTION 3 What is the mean anxiety score for participant 3?\nOk, before we get ahead of ourselves, in order to perform the regression analysis we need to combine the data from stars (the mean anxiety scores) with the data from engage (n_weeks).\n\nTASK: Join the two tables, call the resulting table joined. HINT: Use the inner_join() function (making use of the variable that is common across both tables) to join.\n\nWe now need descriptive statistics for both variables.\n\nTASK: Calculate the mean and standard deviations for the anxiety scores and the engagement data.\n\nQUESTION 4 What are the means and standard deviation for anxiety and engagement with the statistics module?\nAs always, it is a good idea to visualise your data.Now that we have all the variables in one place, make a scatterplot of anxiety as a function of engagement.\n\nTASK: Write the code to create the scatterplot. HINT: For this you’ll need the ggplot() function together with geom_point() and geom_smooth(). Make sure to give your axes some sensible labels with the labs() function.\n\nQUESTION 5 What does the scatterplot suggest about the relationship between anxiety and engagement?\nWith all the variables in place, we’re ready now to start building the regression model.\n\nTASK: Use the lm() function to run the regression model when you model engagement (the outcome variable) as a function of anxiety (the predictor variable) and use the summary() function to look at the output. HINT: lm(outcome ~ predictor, data = my_data).\n\nQUESTION 6 What is the estimate of the y-intercept for the model, rounded to three decimal places?\nQUESTION 7 To three decimal places, if the General Linear Model for this model is Y=beta0 + beta1X + e, then beta1 is …\nQUESTION 8 To three decimal places, for each unit increase in anxiety, engagement decreases by …\nQUESTION 9 To two decimal places, what is the overall F-ratio of the model?\nQUESTION 10 Is the overall model significant?\nQUESTION 11 What proportion of the variance does the model explain?\nNow that we’ve fitted a model, let’s check whether the model meets the assumptions of linearity, normality and homoscedasticity.\n\nTASK: Write the code to check the assumptions. HINT: crPlots() to check linearity, qqPlot() to check normality of the residuals, and residualPlot() to check homoscedasticity of the residuals.\n\nQUESTION 12 Does the relationship appear to be linear?\nQUESTION 13 Do the residuals show normality?\nQUESTION 14 Do the residuals show homoscedasticity?\nFinally, it’s time to write up the results following APA guidelines.\nQUESTION 15 What would the results section look like if you wrote them up, following APA guidelines? HINT: The Purdue writing lab website is helpful for guidance on punctuating statistics."
  },
  {
    "objectID": "PSYC122/Week13.html#answers",
    "href": "PSYC122/Week13.html#answers",
    "title": "Week 13: The linear model",
    "section": "",
    "text": "When you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nWhat is the regression equation as discussed during the lecture and what does each letter represent? Y = b0 + b1 * X + e Y = outcome variable b0 = intercept b1 = slope X = predictor variable e = error\nWhat are residuals? Residuals reflect the discrepancy between the observed values and the fitted values and give an indication of how well the model ‘fits’ the data.\nDiscuss the answers to the pre-lab activity questions. What did you find?\n\n\n\nChange the slider for the intercept. How does it change the regression line? The value for y at x = 0 changes.\n\n\nChange the slider for the slope. How does it change the regression line? The steepness of the line changes.\n\n\nWhat happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line? The distance between the fitted values (the line) and the observed values (the dots) increases. Therefore, the red dashed lines become longer suggesting that the residuals increase. The model therefore fits the data less well.\n\n\n\n\n\nYou can download the R-script that contains the code to complete lab activity 2 here: 122_wk13_labAct2.R.\n\nIf our hypothesis is correct, what type of correlation (if any) should we observe between students’ mean anxiety levels and the variable n_weeks? A negative correlation\nIn the stars table, what do the numbers in the first row across the three columns refer to? ID = 3, Question = Q01 and Score = 1 shows us that participant 3 reported a score of 1 on question 1.\nWhat is the mean anxiety score for participant 3? 1.058824\nWhat are the means and standard deviation for anxiety and engagement with the statistics module? Anxiety M = 2.08, SD = 0.56; Engagement M = 4.54, SD = 2.42.\nWhat does the scatterplot suggest about the relationship between anxiety and engagement? That there might indeed be a relatively strong negative correlation between the two; students with more anxiety, engage less.\nWhat is the estimate of the y-intercept for the model, rounded to three decimal places? 9.057. Explanation: In the summary table, this is the estimate of the intercept.\nTo three decimal places, if the General Linear Model for this model is Y=beta0 + beta1X + e, then beta1 is … -2.173. Explanation: In the summary table, this is the estimate of mean_anxiety, i.e., the slope.\nTo three decimal places, for each unit increase in anxiety, engagement decreases by … 2.173. Explanation: In the summary table, this is also the estimate of mean_anxiety, the slope is how much it decreases so you just remove the - sign.\nTo two decimal places, what is the overall F-ratio of the model? 11.99. Explanation: In the summary table, the F-ratio is noted as the F-statistic.\nIs the overall model significant? Yes. Explanation: The overall model p-value is .001428 which is less than .05, therefore significant.\nWhat proportion of the variance does the model explain? 25.52%. Explanation: The variance explained is determined by R-squared, you simply multiple it by 100 to get the percent.\nDoes the relationship appear to be linear? Yes, the pink link roughly falls across the dashed blue line and looks mostly linear.\nDo the residuals show normality? Yes, in the qq-plot the open circles mostly assemble around the solid blue line, and fall mostly within the range of the dashed blue lines.\nDo the residuals show homoscedasticity? Yes, the residual plot shows that the spread of the residuals is roughly similar for different fitted values.\nWhat would the results section look like if you wrote them up, following APA guidelines? A simple linear regression was performed with engagement (M = 4.54, SD = 0.56) as the outcome variable and statistics anxiety (M = 2.08, SD = 0.56) as the predictor variable. The results of the regression indicated that the model significantly predicted course engagement (F(1, 35) = 11.99, p &lt; .001, R^2 = 0.25), accounting for 25% of the variance. Anxiety was a significant negative predictor (beta = -2.17, p &lt; 0.001): as anxiety increased, course engagement decreased."
  },
  {
    "objectID": "PSYC122/references.html",
    "href": "PSYC122/references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "PSYC122/Week11.html",
    "href": "PSYC122/Week11.html",
    "title": "Week 11: Correlation",
    "section": "",
    "text": "Written by Margriet Groen (partly adapted from materials developed by the PsyTeachR team a the University of Glasgow)\n\nToday we will take a look at correlation as a measure of association between two numerical variables. We will create scatterplots to visualise correlations, we will run a correlation analysis and we will practise interpreting and reporting the results.\n\n\nThe lecture material for this week is presented in two parts:\n\nTheory Watch this part before you complete the reading and the pre-lab activities.\nHow to Watch this part either after the ‘Theory’ part of after you’ve completed the pre-lab activities 1 to 3. Definitely watch it before you come to your lab session.\n\n\n\n\nThe reading that accompanies the lectures this week and next week is from chapter 9 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden.\nRougly, this week we’ll cover the material in sections 9.1 to 9.4, as well as sections 9.8 to 9.11 and section 9.15. Next week, we’ll cover the material in sections 9.5 to 9.7, and sections 9.12 to 9.13. Even if a section is not mentioned here, all of chapter 9 is relevant.\n\n\n\nAfter having watched the lectures on correlation and read the textbook sections you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nHave a look at this visualisation of correlations by Kristoffer Magnusson.\nAfter having read the relevant sections of Howell (2017) Chapter 9, use this visualisation page to visually replicate the scatterplots in Figures 9.1 and 9.2 - use a sample of 100. After that, visually replicate the scatterplots in Figure 9.3.\nEach time you change the correlation, pay attention to the shared variance (the overlap between the two variables) and see how this changes with the changing level of relationship between the two variables. The greater the shared variance, the stronger the relationship.\nAlso, try setting the correlation to r = .5 and then moving a single dot to see how one data point, a potential outlier, can change the stated correlation value between two variables.\n\n\n\nNow that you are well versed in interpreting scatterplots (scattergrams) have a go at this online app on guessing the correlation.\nThis is a very basic app that allows you to see how good you are at recognising different correlation strengths from the scatterplots. We would recommend you click the “Track Performance” tab so you can keep an overview of your overall bias to underestimate or overestimate a correlation.\nIs this all just a bit of fun? Well, yes, because stats is actually fun, and no, because it serves a purpose of helping you determine if the correlations you see in your own data are real, and to help you see if correlations in published research match with what you are being told. As you will have seen from the above examples, one data point can lead to a misleading relationship and even what might be considered a medium to strong relationship may actually have only limited relevance in the real world. One only needs to mention Anscombe’s Quartet to be reminded of the importance of visualising your data, which leads us to the final pre-lab activity for this week.\n\n\n\nAnscombe (1973) showed that four sets of bivariate data (X, Y) that have the exact same means, medians, and relationships can look very different when plotted. You can read more about this here.\nAll in this is a clear example of why you should visualise your data and not to rely on just the numbers.\n\n\n\n\n\nYou might want to re-visit some of the materials that John and Tom provided in PSYC121:\n\nBasics of working with RStudio\nVideo on how to upload a zip file and import data (3.5 mins)\nVideo on basic operations in RStudio (8 mins)\nVideo on using scripts and using the console (3 mins)\n\n\n\n\nClick here for the instructions from Week 6 of PSYC121 if you are unsure.\n\n\n\nDownload the 122_week11_forStudents.zip file and upload it into the new folder in RStudio Server you created at the previous step. If you need them, here are the instructions from Week 2 of PSYC121.\n\n\n\n\n\nIn this lab, you’ll gain understanding of and practice with:\n\nconstructing and interpreting scatterplots\nrunning correlation analysis and interpret the results\nreporting the results in APA format\nconstructing a correlation matrix in APA format\nwhen and why to apply correlation analysis to answers questions in psychological science\n\n\n\n\n\nBelow are scatterplots that show the relationship between ‘how much you know about correlation and how attractive you appear to members of the opposite (&/or same) sex’. Choose the type of correlation (strength and direction) displayed in each graph using one of the following:\n\nPerfect positive correlation\nPerfect negative correlation\nStrong positive correlation\nStrong negative correlation\nModerate positive correlation\nModerate negative correlation\nNull correlation\n\nFigure A  Figure B  Figure C  Figure D \n\n\n\nSuppose it was observed that there is a correlation of r = -.81 between a driver’s age and the cost of car insurance. This correlation would mean that, in general, older people pay more for car insurance.\nTRUE or FALSE? Explain why.\nNote: explain your chosen answer based on the statistic given, not on why you think the correlation may or may note make ‘logical’ sense).\n\n\n\nSuppose that there is a correlation of r = .87 between the length of time a person is in prison and the amount of aggression the person displays on a psychological inventory administered at release. This means that spending a longer amount of time in prison causes people to become more aggressive.\nTRUE or FALSE? Explain why.\n\n\n\nA significant correlation was found between having great hair and performance in correlation labs. The correlation coefficient was .7. How much variance in correlation lab performance can the ‘greatness’ of your hair explain?\n\n51%\n70%\n49%\n30%\nWho cares I’ve got great hair.\n\nWhat was the reason for your answer?\nWhat is this ‘new coefficient’ called?\n\n\n\n\nGoing back to the data discussed in Chapter 11 of Miller & Haden, you’ll remember it contains data from 25 8-year-old children on:\n\na standardised test of reading ability (Abil)\nintelligence (IQ)\nthe number of minutes per week spent reading in the home (Home)\nand the number of minutes per week spent watching TV (TV)\n\nIn the video on ‘How to conduct correlation analysis using R’ we looked at the correlation between reading ability and intelligence. Now, let’s look at the correlation between number of minutes per week spent reading in the home and watching TV.\nThe folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’ contains the datafile (“MillerHadenData.csv”) as well as the R-script from the ‘How to …’ video (122_wk11_howtoExample.R) that you can use here and adapt.\n\nLoad the ‘broom’ and the ‘tidyverse’ libraries by running the first two lines of code.\nRead in the data. You should now see an object with 25 observations and 5 variables in the ‘Environment’. Click on it to view it.\nConstruct a scatterplot of the relationship between ‘Home’ and ‘TV’.\nWhat can you tell from the scatterplot about the direction of the relationship?\nConduct the correlation analysis.\nWhat is the correlation coefficient (Pearson’s r)?\nWhat is the p value?\nIs the correlation significant at the p &lt; .05 level?\nWhat are the degrees of freedom you need to report?\nHow much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’? (Hint: you can use the Console in RStudio as a calculator.)\nWrite a few sentences in which you report this result, following APA guidelines.\n\n\n\n\nResearchers were interested in the relationship between hazardous alcohol use and impulsivity (making unplanned, rapid decisions without thinking or ‘acting on a whim’). To investigate the relationship, 20 participants completed both the alcohol use disorder identification test (AUDIT; Saunders, Aasland, Babor, de la Fuente, & Grant, 1993) and the Barratt’s Impulsiveness Scale (BIS-11) (Patton, Stanford, & Barratt, 1995). The datafile (“alcoholUse_Impulsivity.csv”) is in the folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’. Again, the R-script from the ‘How to …’ video (122_wk11_howtoExample.R) is useful here.\n\nLoad the ‘broom’ and the ‘tidyverse’ libraries by running the first two lines of code.\nRead in the data. You should now see an object containing the data in the ‘Environment’. How many variables does it have?\nConstruct a scatterplot of the relationship between ‘Hazardous Alcohol Use’ and ‘Impulsivity’.\nWhat can you tell from the scatterplot about the direction of the relationship?\nConduct the correlation analysis.\nWhat is the correlation coefficient (Pearson’s r)?\nWhat is the p value?\nIs the correlation significant at the p &lt; .05 level?\nWhat are the degrees of freedom you need to report?\nHow much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’? (Hint: you can use the Console in RStudio as a calculator.)\nConstruct a correlation matrix to display the correlation coefficient in a table.\nGive three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\n\nJob completed — Well done!\n\n\n\n\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nScatterplots\n\n\n\nstrong positive correlation\n\n\nnull correlation\n\n\nmoderate positive correlation\n\n\nperfect negative correlation\n\n\n\nFALSE Explanation: The correlation coefficient is negative and therefore infers a negative correlation. As such, older people pay less for car insurance: as age increases, car insurance costs decrease.\nFALSE Explanation: This is a bit of trick question as it has the sneaky ‘cause’ word in. The correlation coefficient is a positive number, suggesting a positive relationship between length of time in prison and aggression. However, causation cannot be inferred from correlation and therefore we cannot know whether time spent in prison CAUSES aggression, and rather we suggest a relationship between the two that as length of time in prison increases, aggression increases.\nc 49% The ‘coefficient of determination’ or ‘R-squared’ tells us the proportion or variance in one variable that can be predicted if we know the other variable. We can determine this by squaring the r. Therefore, .72 = .49, R2 = .49.\n\n\n\n\nYou can download the R-script that contains the code to complete lab activities 2 and 3 here: 122_wk11_labActivities2_3.R.\n\nSee R script\nSee R script\nSee R script\nWhat can you tell from the scatterplot about the direction of the relationship? There is a negative association between ‘Home’ and ‘TV’. This means that the longer a child spends watching TV, the shorter they will read at home.\nConduct the correlation analysis. See R script\nWhat is the correlation coefficient (Pearson’s r)? r = -.65\nWhat is the p value? p &lt; .001\nIs the correlation significant at the p &lt; .05 level? Yes, because the p-value is smaller than .005\nWhat are the degrees of freedom you need to report? 23\nHow much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’? 42%\nWrite a few sentences in which you report this result, following APA guidelines. Something along the lines of: A Pearson’s correlation coefficient was used to assess the relationship between time spent reading at home and time spent watching TV at home. There was a significant negative correlation, r(23) = -.65, p &lt; .001. As time spent watching TV at home increased, time spent reading at home decreased.\n\n\n\n\n\nSee R script\nHow many variables does it have? 3\nSee R script\nWhat can you tell from the scatterplot about the direction of the relationship? There is a positive association between ‘hazardous alcohol use’ and ‘impulsivity’. This means that as a participant’s score on ‘hazardous alcohol use’ goes up, their score on ‘impulsivity’ also goes up.\nSee R script\nWhat is the correlation coefficient (Pearson’s r)? r = .54\nWhat is the p value? p = .014\nIs the correlation significant at the p &lt; .05 level? Yes\nWhat are the degrees of freedom you need to report? 18\nHow much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’? (Hint: you can use the Console in RStudio as a calculator.) 29%\nConstruct a correlation matrix to display the correlation coefficient in a table.\n\n 12. Give three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\nJust really looking for reasoning here.\nExamples:\n\nBeing more impulsive may make people consume more alcohol.\nConsuming more alcohol may make people more impulsive.\nAn outgoing personality might influence both your level of impulsivity and you are more likely to be socialising in the pub and consuming alcohol. So the same ‘third factor’ may influence both our variables of interest."
  },
  {
    "objectID": "PSYC122/Week11.html#lectures",
    "href": "PSYC122/Week11.html#lectures",
    "title": "Week 11: Correlation",
    "section": "",
    "text": "The lecture material for this week is presented in two parts:\n\nTheory Watch this part before you complete the reading and the pre-lab activities.\nHow to Watch this part either after the ‘Theory’ part of after you’ve completed the pre-lab activities 1 to 3. Definitely watch it before you come to your lab session."
  },
  {
    "objectID": "PSYC122/Week11.html#reading",
    "href": "PSYC122/Week11.html#reading",
    "title": "Week 11: Correlation",
    "section": "",
    "text": "The reading that accompanies the lectures this week and next week is from chapter 9 of the core text by Howell (2017). Please note that I mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Miller and Haden.\nRougly, this week we’ll cover the material in sections 9.1 to 9.4, as well as sections 9.8 to 9.11 and section 9.15. Next week, we’ll cover the material in sections 9.5 to 9.7, and sections 9.12 to 9.13. Even if a section is not mentioned here, all of chapter 9 is relevant."
  },
  {
    "objectID": "PSYC122/Week11.html#pre-lab-activities",
    "href": "PSYC122/Week11.html#pre-lab-activities",
    "title": "Week 11: Correlation",
    "section": "",
    "text": "After having watched the lectures on correlation and read the textbook sections you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nHave a look at this visualisation of correlations by Kristoffer Magnusson.\nAfter having read the relevant sections of Howell (2017) Chapter 9, use this visualisation page to visually replicate the scatterplots in Figures 9.1 and 9.2 - use a sample of 100. After that, visually replicate the scatterplots in Figure 9.3.\nEach time you change the correlation, pay attention to the shared variance (the overlap between the two variables) and see how this changes with the changing level of relationship between the two variables. The greater the shared variance, the stronger the relationship.\nAlso, try setting the correlation to r = .5 and then moving a single dot to see how one data point, a potential outlier, can change the stated correlation value between two variables.\n\n\n\nNow that you are well versed in interpreting scatterplots (scattergrams) have a go at this online app on guessing the correlation.\nThis is a very basic app that allows you to see how good you are at recognising different correlation strengths from the scatterplots. We would recommend you click the “Track Performance” tab so you can keep an overview of your overall bias to underestimate or overestimate a correlation.\nIs this all just a bit of fun? Well, yes, because stats is actually fun, and no, because it serves a purpose of helping you determine if the correlations you see in your own data are real, and to help you see if correlations in published research match with what you are being told. As you will have seen from the above examples, one data point can lead to a misleading relationship and even what might be considered a medium to strong relationship may actually have only limited relevance in the real world. One only needs to mention Anscombe’s Quartet to be reminded of the importance of visualising your data, which leads us to the final pre-lab activity for this week.\n\n\n\nAnscombe (1973) showed that four sets of bivariate data (X, Y) that have the exact same means, medians, and relationships can look very different when plotted. You can read more about this here.\nAll in this is a clear example of why you should visualise your data and not to rely on just the numbers.\n\n\n\n\n\nYou might want to re-visit some of the materials that John and Tom provided in PSYC121:\n\nBasics of working with RStudio\nVideo on how to upload a zip file and import data (3.5 mins)\nVideo on basic operations in RStudio (8 mins)\nVideo on using scripts and using the console (3 mins)\n\n\n\n\nClick here for the instructions from Week 6 of PSYC121 if you are unsure.\n\n\n\nDownload the 122_week11_forStudents.zip file and upload it into the new folder in RStudio Server you created at the previous step. If you need them, here are the instructions from Week 2 of PSYC121."
  },
  {
    "objectID": "PSYC122/Week11.html#lab-activities",
    "href": "PSYC122/Week11.html#lab-activities",
    "title": "Week 11: Correlation",
    "section": "",
    "text": "In this lab, you’ll gain understanding of and practice with:\n\nconstructing and interpreting scatterplots\nrunning correlation analysis and interpret the results\nreporting the results in APA format\nconstructing a correlation matrix in APA format\nwhen and why to apply correlation analysis to answers questions in psychological science\n\n\n\n\n\nBelow are scatterplots that show the relationship between ‘how much you know about correlation and how attractive you appear to members of the opposite (&/or same) sex’. Choose the type of correlation (strength and direction) displayed in each graph using one of the following:\n\nPerfect positive correlation\nPerfect negative correlation\nStrong positive correlation\nStrong negative correlation\nModerate positive correlation\nModerate negative correlation\nNull correlation\n\nFigure A  Figure B  Figure C  Figure D \n\n\n\nSuppose it was observed that there is a correlation of r = -.81 between a driver’s age and the cost of car insurance. This correlation would mean that, in general, older people pay more for car insurance.\nTRUE or FALSE? Explain why.\nNote: explain your chosen answer based on the statistic given, not on why you think the correlation may or may note make ‘logical’ sense).\n\n\n\nSuppose that there is a correlation of r = .87 between the length of time a person is in prison and the amount of aggression the person displays on a psychological inventory administered at release. This means that spending a longer amount of time in prison causes people to become more aggressive.\nTRUE or FALSE? Explain why.\n\n\n\nA significant correlation was found between having great hair and performance in correlation labs. The correlation coefficient was .7. How much variance in correlation lab performance can the ‘greatness’ of your hair explain?\n\n51%\n70%\n49%\n30%\nWho cares I’ve got great hair.\n\nWhat was the reason for your answer?\nWhat is this ‘new coefficient’ called?\n\n\n\n\nGoing back to the data discussed in Chapter 11 of Miller & Haden, you’ll remember it contains data from 25 8-year-old children on:\n\na standardised test of reading ability (Abil)\nintelligence (IQ)\nthe number of minutes per week spent reading in the home (Home)\nand the number of minutes per week spent watching TV (TV)\n\nIn the video on ‘How to conduct correlation analysis using R’ we looked at the correlation between reading ability and intelligence. Now, let’s look at the correlation between number of minutes per week spent reading in the home and watching TV.\nThe folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’ contains the datafile (“MillerHadenData.csv”) as well as the R-script from the ‘How to …’ video (122_wk11_howtoExample.R) that you can use here and adapt.\n\nLoad the ‘broom’ and the ‘tidyverse’ libraries by running the first two lines of code.\nRead in the data. You should now see an object with 25 observations and 5 variables in the ‘Environment’. Click on it to view it.\nConstruct a scatterplot of the relationship between ‘Home’ and ‘TV’.\nWhat can you tell from the scatterplot about the direction of the relationship?\nConduct the correlation analysis.\nWhat is the correlation coefficient (Pearson’s r)?\nWhat is the p value?\nIs the correlation significant at the p &lt; .05 level?\nWhat are the degrees of freedom you need to report?\nHow much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’? (Hint: you can use the Console in RStudio as a calculator.)\nWrite a few sentences in which you report this result, following APA guidelines.\n\n\n\n\nResearchers were interested in the relationship between hazardous alcohol use and impulsivity (making unplanned, rapid decisions without thinking or ‘acting on a whim’). To investigate the relationship, 20 participants completed both the alcohol use disorder identification test (AUDIT; Saunders, Aasland, Babor, de la Fuente, & Grant, 1993) and the Barratt’s Impulsiveness Scale (BIS-11) (Patton, Stanford, & Barratt, 1995). The datafile (“alcoholUse_Impulsivity.csv”) is in the folder you were asked to download under ‘Pre-lab activity 4: Getting ready for the lab class’. Again, the R-script from the ‘How to …’ video (122_wk11_howtoExample.R) is useful here.\n\nLoad the ‘broom’ and the ‘tidyverse’ libraries by running the first two lines of code.\nRead in the data. You should now see an object containing the data in the ‘Environment’. How many variables does it have?\nConstruct a scatterplot of the relationship between ‘Hazardous Alcohol Use’ and ‘Impulsivity’.\nWhat can you tell from the scatterplot about the direction of the relationship?\nConduct the correlation analysis.\nWhat is the correlation coefficient (Pearson’s r)?\nWhat is the p value?\nIs the correlation significant at the p &lt; .05 level?\nWhat are the degrees of freedom you need to report?\nHow much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’? (Hint: you can use the Console in RStudio as a calculator.)\nConstruct a correlation matrix to display the correlation coefficient in a table.\nGive three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\n\nJob completed — Well done!"
  },
  {
    "objectID": "PSYC122/Week11.html#answers",
    "href": "PSYC122/Week11.html#answers",
    "title": "Week 11: Correlation",
    "section": "",
    "text": "When you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nScatterplots\n\n\n\nstrong positive correlation\n\n\nnull correlation\n\n\nmoderate positive correlation\n\n\nperfect negative correlation\n\n\n\nFALSE Explanation: The correlation coefficient is negative and therefore infers a negative correlation. As such, older people pay less for car insurance: as age increases, car insurance costs decrease.\nFALSE Explanation: This is a bit of trick question as it has the sneaky ‘cause’ word in. The correlation coefficient is a positive number, suggesting a positive relationship between length of time in prison and aggression. However, causation cannot be inferred from correlation and therefore we cannot know whether time spent in prison CAUSES aggression, and rather we suggest a relationship between the two that as length of time in prison increases, aggression increases.\nc 49% The ‘coefficient of determination’ or ‘R-squared’ tells us the proportion or variance in one variable that can be predicted if we know the other variable. We can determine this by squaring the r. Therefore, .72 = .49, R2 = .49.\n\n\n\n\nYou can download the R-script that contains the code to complete lab activities 2 and 3 here: 122_wk11_labActivities2_3.R.\n\nSee R script\nSee R script\nSee R script\nWhat can you tell from the scatterplot about the direction of the relationship? There is a negative association between ‘Home’ and ‘TV’. This means that the longer a child spends watching TV, the shorter they will read at home.\nConduct the correlation analysis. See R script\nWhat is the correlation coefficient (Pearson’s r)? r = -.65\nWhat is the p value? p &lt; .001\nIs the correlation significant at the p &lt; .05 level? Yes, because the p-value is smaller than .005\nWhat are the degrees of freedom you need to report? 23\nHow much variance in ‘time spent reading’ can be accounted for by ‘time spent watching TV’? 42%\nWrite a few sentences in which you report this result, following APA guidelines. Something along the lines of: A Pearson’s correlation coefficient was used to assess the relationship between time spent reading at home and time spent watching TV at home. There was a significant negative correlation, r(23) = -.65, p &lt; .001. As time spent watching TV at home increased, time spent reading at home decreased.\n\n\n\n\n\nSee R script\nHow many variables does it have? 3\nSee R script\nWhat can you tell from the scatterplot about the direction of the relationship? There is a positive association between ‘hazardous alcohol use’ and ‘impulsivity’. This means that as a participant’s score on ‘hazardous alcohol use’ goes up, their score on ‘impulsivity’ also goes up.\nSee R script\nWhat is the correlation coefficient (Pearson’s r)? r = .54\nWhat is the p value? p = .014\nIs the correlation significant at the p &lt; .05 level? Yes\nWhat are the degrees of freedom you need to report? 18\nHow much variance in ‘impulsivity’ can be accounted for by ‘hazardous alcohol use’? (Hint: you can use the Console in RStudio as a calculator.) 29%\nConstruct a correlation matrix to display the correlation coefficient in a table.\n\n 12. Give three logically possible directions of causality, indicating for each direction whether it is a plausible explanation in light of the variables involved (and why). No, this is not a trick question —-I know that correlation does not infer causation, but think critically! New studies/ideas are constructed by thinking what the previous study doesn’t tell us about what could be happening with the variables of interest.\nJust really looking for reasoning here.\nExamples:\n\nBeing more impulsive may make people consume more alcohol.\nConsuming more alcohol may make people more impulsive.\nAn outgoing personality might influence both your level of impulsivity and you are more likely to be socialising in the pub and consuming alcohol. So the same ‘third factor’ may influence both our variables of interest."
  },
  {
    "objectID": "PSYC122/Part2.html",
    "href": "PSYC122/Part2.html",
    "title": "Preface",
    "section": "",
    "text": "Welcome to our overview of the materials and activities you will work with in PSYC122 Part 2.\nWe will complete a series of four classes which will run in weeks 16-19.\nThe classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\n\n\n\n\n\n\nTip\n\n\n\nWe will reference lectures in these other modules where you may benefit from connecting ideas about research integrity and open science to your critical thinking on what we are doing.\n\n\n\n\nIn weeks 16-20, we are going to focus on working in research in context (see Figure Figure 1).\n\n\n\n\n\n\n\n\nG\n\n  \n\nreading\n\n reading   \n\nknowledge\n\n knowledge   \n\nreading–knowledge\n\n   \n\nconventions\n\n conventions   \n\nknowledge–conventions\n\n   \n\nconcepts\n\n concepts   \n\nknowledge–concepts\n\n   \n\npractices\n\n practices   \n\nknowledge–practices\n\n  \n\n\nFigure 1: This is a simple graphviz graph.\n\n\n\n\nYou have been introduced to R. We know that some of you are new to R so we will practice the skills you are learning. We will consolidate, revise, and extend these skills.\nWe will build your understanding of the linear model.\nThe big change is this focus on the context. The reason is that not talking about the context risks limiting how you approach, do or think about data analysis.\nIn traditional methods teaching, the schedule of classes will progress through a series of tests, one test a week, from simpler to more complex tests (e.g., from t-test to multiple regression at the undergraduate level). Textbooks often mirror this structure, presenting one test per chapter.\nIn this approach, the presentation is often brief about the context: the question the researchers are investigating; the methods they use to collect data, including the measurements; and the assumptions they make about how your reasoning can get you from the things you measure to the things you are trying to understand. In this approach, also, example data may be presented in a limited, partial, way.\nThe reasons for this are understandable: methods are complex, technical, subjects for learning, and teachers and students do not also have time, perhaps, to think about statistics and about theoretical or measurement assumptions.\nThis is risky because it presents a misleading view of the challenge in learning methods: the misleading view is that the challenge is just the (difficult enough) challenge of learning about statistical methods, or dealing with numbers. It is risky, also, because it implies that if you learn the method, and can match the textbook example – the variables, the state of the data – when it is your turn to do an analysis, all will be well.\nA more productive approach – this is the approach we will take – is to expose, and talk about some of the real challenges that anybody who handles data, or quantitative evidence, in professional life. These challenges include:\n\nThinking about the mapping from our concerns to the research questions, to the things we measure, to analysis we do, and then the conclusions we make.\nSelecting or constructing valid measures that can be assumed to measure the things they are supposed to measure.\nTaking samples of observations, and making conclusions about the population.\nMaking estimates and linking these estimates to an account that is explicit about causes."
  },
  {
    "objectID": "PSYC122/Part2.html#a-change-in-approach",
    "href": "PSYC122/Part2.html#a-change-in-approach",
    "title": "Preface",
    "section": "",
    "text": "In weeks 16-20, we are going to focus on working in research in context (see Figure Figure 1).\n\n\n\n\n\n\n\n\nG\n\n  \n\nreading\n\n reading   \n\nknowledge\n\n knowledge   \n\nreading–knowledge\n\n   \n\nconventions\n\n conventions   \n\nknowledge–conventions\n\n   \n\nconcepts\n\n concepts   \n\nknowledge–concepts\n\n   \n\npractices\n\n practices   \n\nknowledge–practices\n\n  \n\n\nFigure 1: This is a simple graphviz graph.\n\n\n\n\nYou have been introduced to R. We know that some of you are new to R so we will practice the skills you are learning. We will consolidate, revise, and extend these skills.\nWe will build your understanding of the linear model.\nThe big change is this focus on the context. The reason is that not talking about the context risks limiting how you approach, do or think about data analysis.\nIn traditional methods teaching, the schedule of classes will progress through a series of tests, one test a week, from simpler to more complex tests (e.g., from t-test to multiple regression at the undergraduate level). Textbooks often mirror this structure, presenting one test per chapter.\nIn this approach, the presentation is often brief about the context: the question the researchers are investigating; the methods they use to collect data, including the measurements; and the assumptions they make about how your reasoning can get you from the things you measure to the things you are trying to understand. In this approach, also, example data may be presented in a limited, partial, way.\nThe reasons for this are understandable: methods are complex, technical, subjects for learning, and teachers and students do not also have time, perhaps, to think about statistics and about theoretical or measurement assumptions.\nThis is risky because it presents a misleading view of the challenge in learning methods: the misleading view is that the challenge is just the (difficult enough) challenge of learning about statistical methods, or dealing with numbers. It is risky, also, because it implies that if you learn the method, and can match the textbook example – the variables, the state of the data – when it is your turn to do an analysis, all will be well.\nA more productive approach – this is the approach we will take – is to expose, and talk about some of the real challenges that anybody who handles data, or quantitative evidence, in professional life. These challenges include:\n\nThinking about the mapping from our concerns to the research questions, to the things we measure, to analysis we do, and then the conclusions we make.\nSelecting or constructing valid measures that can be assumed to measure the things they are supposed to measure.\nTaking samples of observations, and making conclusions about the population.\nMaking estimates and linking these estimates to an account that is explicit about causes."
  },
  {
    "objectID": "PSYC122/Week14.html",
    "href": "PSYC122/Week14.html",
    "title": "Week 14: Chi-square",
    "section": "",
    "text": "Written by Margriet Groen (partly adapted from materials developed by the PsyTeachR team a the University of Glasgow)\n\nThis week we will focus on Chi-square as a measure of association between categorical variables.\n\n\nThe lecture material for this week is presented in two parts:\n\nAssociations between categorical variables (~23 min)\nHow to do Chi-square in R (~19 min) You can find the example script in this week’s zip-folder (see under Pre-lab activity 3).\n\n\n\n\nThe reading that accompanies the lectures this week is Chapter 19: Chi-square of the core text by Howell (2017). Please note that I might mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Greene & D’Oliveira (2006).\n\n\n\nAfter having watched the lectures and read the textbook chapter you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nIs there a relationship between the number of people who smoke and the number of people who drink? Please note that the question is the number of people (frequency) and not how much people drink/smoke. Alos not that these are fictitious data. You’ll need a table of critical values for Chi-square. This can be found in the ‘Week 14 – resources’ folder on Moodle.\n Pre-lab activity questions:\n\nComplete the Pearson’s Chi-square test by hand using the data above and fill in the blanks:\n\n\\(\\chi 2\\) ( , N = ) = , p\n\nCan you reject the null hypothesis?\n\n\n\n\nIn this week’s online tutorials, you will practise creating bar chars, a device for visualising the distribution of categorical variables.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nBar Charts In this tutorial you will learn how to make and enhance bar charts with the ggplot2 package.\n\n\n\n\n\n\nDownload the 122_week14_forStudents.zip file and upload it into the new folder in RStudio Server you created.\n\n\n\n\n\nIn this lab, you’ll gain understanding of and practice with:\n\nconducting Pearson’s Chi-square in R\ninterpreting Pearson’s Chi-square in R\nreporting the results in APA format\nwhen and why to apply Pearson’s Chi-square to answer questions in psychological science\n\n\n\n\n\nHow does Pearson’s chi-square differ from Pearson’s correlation?\n\n\n\nChi-square test of independence would be appropriate when testing the following questions:\n\n\nWhat is the relationship between gender and soft drink preference? True or False?\n\nb.How do males and females compare in terms of wanting to be a psychologist when they leave school? True or False?\n\n\n\n\nWrite the chi-square formula below.\n\n\n\nWhat were your answers to the pre-lab activity 1 questions? Please compare them with other students at your table.\n+a. Complete the Pearson’s Chi-square test by hand using the data above and fill in the blanks:\n\\(\\chi 2\\) ( , N = ) = , p\n+b. Can you reject the null hypothesis?\n\n\n\nWhy is it recommended to opt for multiple 2 x 2 chi-squares instead of chi-squares larger than 2 x 2?\n\n\n\nHow could you ‘modify’ the contingency table below for chi-square analysis to aid subsequent interpretation of the data/results?\n\n\n\n\n\nFor this lab, we’re going to use data from Rogers, T. & Milkman, K. L. (2016). Reminders through association. Psychological Science, 27, 973-986. You can read the full paper online but the short version is that the authors looked at how people remember to follow through with the intention of doing something.\nAlthough there are lots of potential reasons (e.g., some people may lack the self-control resources), Rogers and Milkman (2016) propose that some people fail to follow through simply because they forget about their good intentions. If this is the case, the authors argue, then having visual reminders to follow through on their intentions may help people remember to keep them. For example, a person may choose to put a sticker for their gym on their car window, so that every time they get in the car they remember to go to the gym.\nIn Study 1 by Rogers and colleagues, participants took part in an unrelated experiment but at the start of the task they were asked to return a small stack of paper clips to the reception of the building at the end of the study and if they did so the researchers would donate $1 to a charity. They were then asked if they intended to do this. Those in the reminder-through-association (RTA) condition read “Thank you! To remind you to pick up a paper clip, an elephant statuette will be sitting on the counter as you collect your payment.” This message was followed by a picture of the elephant statuette. Those in the control condition simply read “Thank you!”.\nWhat we want to do is to run a chi-square analysis to determine whether those in the RTA condition were more likely to remember to return the paper-clips than those in the control condition. Open the 122_wk14_labAct2.R script in RStudio and work your way through it. All instructions, hints and questions are contained in the script.\n\n\n\n\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nHow does Pearson’s Chi-square differ from Pearson’s correlation? Pearson’s Chi-square assesses whether there is a relationship between categorical (or nominal) variables. Pearson’s correlation assesses whether there is a relationship between continuous (or interval/ratio) variables.\nChi-square test of independence would be appropriate when testing the following questions:\n\n\n\nWhat is the relationship between gender and soft drink preference? True\n\n\nHow do males and females compare in terms of wanting to be a psychologist when they leave school? True\n\n\n\nWrite the chi-square formula below. \nWhat were your answers to the pre-lab activity 1 questions? Please compare them with other students at your table.\n\n\n\nComplete the Pearson’s chi-square test by hand using the data above and fill in the blanks:\n\n\n\nFirst determine expected frequencies: Smoke/drink: (70 x 65) / 110 = 41.36 Smoke/don’t drink (70 x 45) / 110 = 28.63 Don’t smoke/drink (40 x 65) / 110 = 23.63 Don’t smoke/don’t drink (40 x 45) / 110 = 16.36\nThen calculate chi-square: Smoke/drink: (50-41.36)2 / 41.36 = 1.80 Smoke/don’t drink: (20-28.63)2 / 28.63 = 2.60 Don’t smoke/drink: (15-23.63)2 / 23.63 = 3.15 Don’t smoke/don’t drink: (25-16.36)2 / 16.36 = 4.56 1.80+2.60+3.15+4.56 = 12.11\n\\(\\chi 2\\) (1, N = 110) = 12.11, p &lt; .001\n\n\nCan you reject the null hypothesis? Yes\n\n\n\nWhy is it recommended to opt for multiple 2 x 2 chi-squares instead of chi-squares larger than 2 x 2? It is easier to interpret and such a design requires a smaller sample size.\nHow could you ‘modify’ the contingency table below for chi-square analysis to aid subsequent interpretation of the data/results?\n\n\nBy combining ‘interested’ and ‘somewhat interested’ or by partitioning (doing multiple 2 x 2 chi-squares, while using Bonferroni correction to account for running multiple tests\n\n\n\nPlease see the 122_wk14_labAct2_withAnswers.R script for the relevant code and answers to questions."
  },
  {
    "objectID": "PSYC122/Week14.html#lectures",
    "href": "PSYC122/Week14.html#lectures",
    "title": "Week 14: Chi-square",
    "section": "",
    "text": "The lecture material for this week is presented in two parts:\n\nAssociations between categorical variables (~23 min)\nHow to do Chi-square in R (~19 min) You can find the example script in this week’s zip-folder (see under Pre-lab activity 3)."
  },
  {
    "objectID": "PSYC122/Week14.html#reading",
    "href": "PSYC122/Week14.html#reading",
    "title": "Week 14: Chi-square",
    "section": "",
    "text": "The reading that accompanies the lectures this week is Chapter 19: Chi-square of the core text by Howell (2017). Please note that I might mention an alternative textbook in the lectures. The content is highly similar, but this year, we’ve decided to use Howell (2017) as the core text for PSYC121 and PSYC122. So no need to look at the book by Greene & D’Oliveira (2006)."
  },
  {
    "objectID": "PSYC122/Week14.html#pre-lab-activities",
    "href": "PSYC122/Week14.html#pre-lab-activities",
    "title": "Week 14: Chi-square",
    "section": "",
    "text": "After having watched the lectures and read the textbook chapter you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n\nIs there a relationship between the number of people who smoke and the number of people who drink? Please note that the question is the number of people (frequency) and not how much people drink/smoke. Alos not that these are fictitious data. You’ll need a table of critical values for Chi-square. This can be found in the ‘Week 14 – resources’ folder on Moodle.\n Pre-lab activity questions:\n\nComplete the Pearson’s Chi-square test by hand using the data above and fill in the blanks:\n\n\\(\\chi 2\\) ( , N = ) = , p\n\nCan you reject the null hypothesis?\n\n\n\n\nIn this week’s online tutorials, you will practise creating bar chars, a device for visualising the distribution of categorical variables.\nIf you’re ready to begin, go to the tutorial linked to below. There is no need to install or download anything. Each tutorial has everything you need to write and run R code, right in the tutorial.\n\nBar Charts In this tutorial you will learn how to make and enhance bar charts with the ggplot2 package.\n\n\n\n\n\n\nDownload the 122_week14_forStudents.zip file and upload it into the new folder in RStudio Server you created."
  },
  {
    "objectID": "PSYC122/Week14.html#lab-activities",
    "href": "PSYC122/Week14.html#lab-activities",
    "title": "Week 14: Chi-square",
    "section": "",
    "text": "In this lab, you’ll gain understanding of and practice with:\n\nconducting Pearson’s Chi-square in R\ninterpreting Pearson’s Chi-square in R\nreporting the results in APA format\nwhen and why to apply Pearson’s Chi-square to answer questions in psychological science\n\n\n\n\n\nHow does Pearson’s chi-square differ from Pearson’s correlation?\n\n\n\nChi-square test of independence would be appropriate when testing the following questions:\n\n\nWhat is the relationship between gender and soft drink preference? True or False?\n\nb.How do males and females compare in terms of wanting to be a psychologist when they leave school? True or False?\n\n\n\n\nWrite the chi-square formula below.\n\n\n\nWhat were your answers to the pre-lab activity 1 questions? Please compare them with other students at your table.\n+a. Complete the Pearson’s Chi-square test by hand using the data above and fill in the blanks:\n\\(\\chi 2\\) ( , N = ) = , p\n+b. Can you reject the null hypothesis?\n\n\n\nWhy is it recommended to opt for multiple 2 x 2 chi-squares instead of chi-squares larger than 2 x 2?\n\n\n\nHow could you ‘modify’ the contingency table below for chi-square analysis to aid subsequent interpretation of the data/results?\n\n\n\n\n\nFor this lab, we’re going to use data from Rogers, T. & Milkman, K. L. (2016). Reminders through association. Psychological Science, 27, 973-986. You can read the full paper online but the short version is that the authors looked at how people remember to follow through with the intention of doing something.\nAlthough there are lots of potential reasons (e.g., some people may lack the self-control resources), Rogers and Milkman (2016) propose that some people fail to follow through simply because they forget about their good intentions. If this is the case, the authors argue, then having visual reminders to follow through on their intentions may help people remember to keep them. For example, a person may choose to put a sticker for their gym on their car window, so that every time they get in the car they remember to go to the gym.\nIn Study 1 by Rogers and colleagues, participants took part in an unrelated experiment but at the start of the task they were asked to return a small stack of paper clips to the reception of the building at the end of the study and if they did so the researchers would donate $1 to a charity. They were then asked if they intended to do this. Those in the reminder-through-association (RTA) condition read “Thank you! To remind you to pick up a paper clip, an elephant statuette will be sitting on the counter as you collect your payment.” This message was followed by a picture of the elephant statuette. Those in the control condition simply read “Thank you!”.\nWhat we want to do is to run a chi-square analysis to determine whether those in the RTA condition were more likely to remember to return the paper-clips than those in the control condition. Open the 122_wk14_labAct2.R script in RStudio and work your way through it. All instructions, hints and questions are contained in the script."
  },
  {
    "objectID": "PSYC122/Week14.html#answers",
    "href": "PSYC122/Week14.html#answers",
    "title": "Week 14: Chi-square",
    "section": "",
    "text": "When you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. Remember, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. Actively engaging with the material is the way to learn these analysis skills, not by looking at someone else’s completed code…\n\n\n\n\nHow does Pearson’s Chi-square differ from Pearson’s correlation? Pearson’s Chi-square assesses whether there is a relationship between categorical (or nominal) variables. Pearson’s correlation assesses whether there is a relationship between continuous (or interval/ratio) variables.\nChi-square test of independence would be appropriate when testing the following questions:\n\n\n\nWhat is the relationship between gender and soft drink preference? True\n\n\nHow do males and females compare in terms of wanting to be a psychologist when they leave school? True\n\n\n\nWrite the chi-square formula below. \nWhat were your answers to the pre-lab activity 1 questions? Please compare them with other students at your table.\n\n\n\nComplete the Pearson’s chi-square test by hand using the data above and fill in the blanks:\n\n\n\nFirst determine expected frequencies: Smoke/drink: (70 x 65) / 110 = 41.36 Smoke/don’t drink (70 x 45) / 110 = 28.63 Don’t smoke/drink (40 x 65) / 110 = 23.63 Don’t smoke/don’t drink (40 x 45) / 110 = 16.36\nThen calculate chi-square: Smoke/drink: (50-41.36)2 / 41.36 = 1.80 Smoke/don’t drink: (20-28.63)2 / 28.63 = 2.60 Don’t smoke/drink: (15-23.63)2 / 23.63 = 3.15 Don’t smoke/don’t drink: (25-16.36)2 / 16.36 = 4.56 1.80+2.60+3.15+4.56 = 12.11\n\\(\\chi 2\\) (1, N = 110) = 12.11, p &lt; .001\n\n\nCan you reject the null hypothesis? Yes\n\n\n\nWhy is it recommended to opt for multiple 2 x 2 chi-squares instead of chi-squares larger than 2 x 2? It is easier to interpret and such a design requires a smaller sample size.\nHow could you ‘modify’ the contingency table below for chi-square analysis to aid subsequent interpretation of the data/results?\n\n\nBy combining ‘interested’ and ‘somewhat interested’ or by partitioning (doing multiple 2 x 2 chi-squares, while using Bonferroni correction to account for running multiple tests\n\n\n\nPlease see the 122_wk14_labAct2_withAnswers.R script for the relevant code and answers to questions."
  },
  {
    "objectID": "PSYC122/Week17.html",
    "href": "PSYC122/Week17.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials you will work with in PSYC122 Week 17.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 17 class, we will aim to answer the research question:\n\nWhat person attributes predict success in understanding?"
  },
  {
    "objectID": "PSYC122/Week17.html#sec-week-17-intro",
    "href": "PSYC122/Week17.html#sec-week-17-intro",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials you will work with in PSYC122 Week 17.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the week 17 class, we will aim to answer the research question:\n\nWhat person attributes predict success in understanding?"
  },
  {
    "objectID": "PSYC122/Week17.html#our-learning-goals",
    "href": "PSYC122/Week17.html#our-learning-goals",
    "title": "Statistics for Psychologists",
    "section": "Our learning goals",
    "text": "Our learning goals\nIn Week 17, we aim to further develop skills in analyzing and in visualizing psychological data.\n\n\n\n\n\n\nTip\n\n\n\n\nIn psychological science, research questions like our question can be examined using linear models.\n\n\n\nWhen we do these analyses, we will need to think about how we report the results:\n\nwe usually need to report information about the kind of model we specify;\nwe will need to report the nature of the association estimated in our model;\nand we usually need to decide, is the association significant?\ndoes the association reflect a positive or negative relationship between outcome and predictor?\nis the association we see in our sample data relatively strong or weak?\n\n\n\n\n\n\n\nTip\n\n\n\n\nWe will aim to build skills in producing professional-looking plots for our audiences.\n\n\n\nAt every stage, as we work, we will visualize the data to:\n\nUnderstand the shape of the relationships we may observe or predict."
  },
  {
    "objectID": "PSYC122/Week17.html#sec-w17-resources-intro",
    "href": "PSYC122/Week17.html#sec-w17-resources-intro",
    "title": "Statistics for Psychologists",
    "section": "Your resources",
    "text": "Your resources\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found under the Week 17 resources section title, on Moodle:\nLink to Moodle"
  },
  {
    "objectID": "PSYC122/Week17.html#lectures-video-recordings",
    "href": "PSYC122/Week17.html#lectures-video-recordings",
    "title": "Statistics for Psychologists",
    "section": "Lectures: video recordings",
    "text": "Lectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 19 minutes\nPart 2 of 4; about 11 minutes\nPart 3 of 4; about 9 minutes\nPart 4 of 4; about 15 minutes\nThe lectures have two main areas of focus.\n1. Understanding the scientific process\nI outline the steps through which a psychological scientist may progress, in logic and practice, from research questions to hypotheses to analyses.\nI rehearse some of the key points I have discussed, previously, in order to build a framework in which you can understand how we go from:\n\na set of concerns – here: What makes it easy or difficult to understand written health information?\nthrough choices about what data to collect, and how\nto specific questions, and then predictions\nthat we can test using linear models.\n\nWe are learning data analysis methods. But the key point is that we use these methods in the context of a research project with concerns, aims, methodological assumptions, and choices. This is generally true so the aim is to present a concrete example of how research works.\nAs part of the discussion, I raise questions you might want to consider. These questions – and questions you can originate for yourselves – are also part of the context for our data analysis, because they help to inform how you interpret or evaluate the results. These questions are examples of the critical evaluation that you will need to develop through your studies.\n2. The linear model\nWe look at how the linear model can be used to address research questions in the context of the Clearly understood health comprehension project. But I aim to outline some general ideas about why we use the linear model technique, and how it works. I build on work you have done with Margriet Groen in earlier PSYC122 classes, so that we can strengthen understanding, and extend skills.\n\n\n\n\n\n\nTip\n\n\n\nTo work with the recordings:\n\nWatch the video parts right through.\nUse the printable versions of the slides (provided on Moodle) to make notes.\nTry out the coding exercises in the how-to and the workbook (see Section 5.1) to see for yourself how you can construct visualizations and do analyses.\n\n\n\nIn the lecture, I talk about how we use the linear model to estimate the association between (1.) an outcome like mean accuracy of understanding and (2.) a predictor like vocabulary knowledge: estimating the association as the expected change in the average outcome given variation in the predictor.\nIn the plot on the right of Figure 1, we show the distribution curve of mean (comprehension) accuracy scores observed at each value of vocabulary. You can see that the middle – the average – of each distribution, marked by a line, increases as we go from left (low scores) to right (high scores).\n\n\n\n\n\nFigure 1: Plots showing the association between the outcome mean accuracy of understanding a predictor, vocabulary knowledge. Both plots show the same data. The plot on the right is modified to show how accuracy of understanding varies between individuals in the sample with the sample vocabulary test scores\n\n\n\n\nIn the lecture, I talk about how the information we get from a linear model allows us to predict the way in which outcome values may vary (increase or decrease), given different values in the predictor variable.\nWe could form a prediction line anywhere but the linear model helps us to estimate the prediction (“best fit”) line that minimizes the differences between observed and predicted outcomes: the residuals, as shown in Figure 2.\n\n\n\n\n\nFigure 2: Plot showing the prediction of mean accuracy of understanding, given information about participant vocabulary knowledge, with lines drawn to show the difference between observed outcomes (shown in orange-red) and predicted outcomes (shown as black circles on the blue line) for each vocabulary test score value in our sample\n\n\n\n\nThe lectures end with a discussion of the critical information you must identify and extract when you view the summary of a linear model results.\nI then show you how to report the results. I give you examples of the conventional language you can use to report your results.\n\n\n\n\n\n\nTip\n\n\n\nIn reporting linear model results, we need to explain what they tell us about the association between outcome and predictor variables.\n\n\nWe can use visualization to help us to interpret the model estimates. In the how-to guide and in the lab activity workbook (see Section 5.1), we look at how you can draw prediction plots, given model estimates.\n\nLinks to other classes\nWe do not provide further reading for this class but you will find it helpful to revise some of the key ideas you have been learning about, in PSYC122 and in other modules.\n\nThe lectures in PSYC123 on: (week 2) reliability and validity; (week 3) experimental design, especially between-subjects studies and individual differences; and (week 9) precise hypotheses.\nThe lecture in PSYC122 (week 13) introducing the linear model.\n\n\n\nPre-lab activity 1\nIn weeks 16-19, we will be working together on a research project to investigate how people vary in their response to health advice.\nCompleting the project involves collecting responses from PSYC122 students.\nTo enter your responses, we invite you to complete a short survey.\nComplete the survey by clicking on the link here\n\n\n\n\n\n\nTip\n\n\n\nIn our week 19 class activity, we will analyze the data we collect here.\n\n\n\nSurvey information\nThe survey asks you to:\n\ncomplete some questions about who you are;\nand then answer some questions about what you know about some English words, about what you know about health matters, and about how you approach reading.\n\nThe survey then asks you to:\n\nread five short extracts from patient information leaflets about different kinds of health issue;\nrespond to some multiple choice questions about each extract;\nand rate how well you think you understand the advice.\n\nThe survey should take about 20 minutes to complete. Some people will take less time, and some people might take a little more time.\nTaking part in the survey is completely voluntary. You can stop at any time without completing the survey if you do not want to finish it. If you do not want to do the survey, you can do an alternative activity (see below).\nAll responses will be recorded completely anonymously.\n\n\n\nPre-lab activity 1 alternative\nIf you do not want to complete the survey, we invite you to read the pre-registered research plan for the PSYC122 health advice research project.\nRead the project pre-registration"
  },
  {
    "objectID": "PSYC122/Week17.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "href": "PSYC122/Week17.html#pre-lab-activity-2-getting-ready-for-the-lab-class",
    "title": "Statistics for Psychologists",
    "section": "Pre-lab activity 2: Getting ready for the lab class",
    "text": "Pre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w17_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes the data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand the code files:\n\n2022-23-PSYC122-w17-how-to.R\n2022-23-PSYC122-w17-workbook.R\n\nYou will use 2022-23-PSYC122-w17-workbook.R in the lab activity practical class.\nAlternatively, you can instead download the resources you need from the week 17 section of the Moodle page for the PSYC122 module:\nLink to Moodle\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\n\nOur aim is to make sure you can work with code, and write notes in the .R files.\n\n\n\nIn the workbook.R file you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\nIn both the .R files:\n\n2022-23-PSYC122-w17-how-to.R\n2022-23-PSYC122-w17-workbook.R\n\nwe will take things step-by-step.\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\nThe how-to guide comprises an .R file 2022-23-PSYC122-w17-how-to.R with code and advice. The code in the .R file was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 6) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity."
  },
  {
    "objectID": "PSYC122/Week17.html#sec-w17-activity",
    "href": "PSYC122/Week17.html#sec-w17-activity",
    "title": "Statistics for Psychologists",
    "section": "Lab activity",
    "text": "Lab activity\nIn the lab activity .R file 2022-23-PSYC122-w17-workbook.R, you will work with data from a study about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv\n\nThe data are similar in format to the response data we are collecting as part of the PSYC122 project.\n\nTasks\nIn the activity, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library()\nRead in the data file – using read_csv()\nInspect the data – using head() and summary()\nVisualize the distribution of values – using geom_histogram()\nEdit the appearance of the histogram plots, using binwidth, theme_bw(), labs(), xlim() and geom_vline()\nVisualize the potential association between the values of two variables by producing a scatterplot using geom_point()\nEdit the appearance of the scatterplots by using the geom_point() arguments alpha, size, colour, and shape, and by changing the axis limits using xlim() and ylim().\nRevise how you test the associations between pairs of variables through correlation analyses using cor.test()\nExamine the predictive relation between outcome and predictor variables using lm()\nVisualize the model predictions – using geom_abline() and information from the model results\n\nThe activity 2022-23-PSYC122-w17-workbook.R file takes you through the tasks, one by one.\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure about what you need to do, look in 2022-23-PSYC122-w17-how-to.R.\n\n\nIn the how-to, you will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code. You will need to change the names of the dataset or the variables to complete the tasks in the activity.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC122/Week16.html",
    "href": "PSYC122/Week16.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials and guidance you will work with in PSYC122 Week 16.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\nWe encounter written health information all the time: in warnings signs, on medication labels, in clinics when we go to see the doctor, and online when we research things we are worried about. It is not always easy to understand this information. The problem is that it is unclear how health information should be communicated. As psychologists, we can help to improve health communication.\n\n\n\n\n\n\nImportant\n\n\n\nIn these classes, we will complete a research project to answer the research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\n\nWe will present our PSYC122 lessons in the context of this research project because we think that this context will help you to make sense of the data, and to see why we ask you to practice the skills we are teaching.\nWe will be revisiting some of the ideas and techniques you have seen introduced in previous classes. This is to give you the opportunity to revise and consolidate your learning. We will extend your development with some new ideas, to strengthen your skills.\nUltimately, we aim to contribute new findings from the data we will collect together. These new findings will, we hope, help to make the provision of health advice a bit more useful in future.\n\n\nIn Week 16, we will ask you to do three things.\n\n\n\n\n\n\nNote\n\n\n\nFirst, we will ask you to do a pre-lab activity that involves completing a survey.\n\n\n\nCompleting the survey will help you to make sense of the numbers you will be working with in the activities.\nCompleting the pre-lab activity will help to teach you about the challenges of measurement, a key aspect of the scientific thinking skills we will help you to develop.\n\n\n\n\n\n\n\nNote\n\n\n\nSecond, we will ask you to do a set of practical tasks in the lab activity that are designed to consolidate your learning on data visualization.\n\n\n\nWe will be:\n\n\nUsing histograms to examine the distributions of variables;\nLearning to edit the histograms to present them professionally.\n\n\n\n\n\n\nFigure 1: How-to guide example of a histogram showing observed mean accuracy of understanding of health information\n\n\n\n\n\nWe will be:\n\n\nUsing scatterplots to examine the association of variables;\nLearning to edit the plots to present them professionally.\n\n\n\n\n\n\nFigure 2: How-to guide example of a scatterplot representing the potential association between values of health literacy (HLVA) and mean self-rated accuracy of understanding of health information.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThird, we will ask you to think critically about predictions about potential associations between measures of participant attributes and measures of understanding of health information.\n\n\n\nWe will use correlations to test predictions about associations and thus answer research questions."
  },
  {
    "objectID": "PSYC122/Week16.html#sec-week-16-intro",
    "href": "PSYC122/Week16.html#sec-week-16-intro",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to your overview of the materials and guidance you will work with in PSYC122 Week 16.\nWe will complete four classes in weeks 16-19. These classes are designed to help students to revise and put into practice some of the key ideas and skills you have been developing in the first year research methods modules PSYC121, PSYC123 and PSYC124.\nWe will do this in the context of a live research project with potential real world impacts: the Clearly understood project.\n\nOur focus will be on what makes it easy or difficult for people to understand written health information.\n\nWe encounter written health information all the time: in warnings signs, on medication labels, in clinics when we go to see the doctor, and online when we research things we are worried about. It is not always easy to understand this information. The problem is that it is unclear how health information should be communicated. As psychologists, we can help to improve health communication.\n\n\n\n\n\n\nImportant\n\n\n\nIn these classes, we will complete a research project to answer the research questions:\n\nWhat person attributes predict success in understanding?\nCan people accurately evaluate whether they correctly understand written health information?\n\n\n\nWe will present our PSYC122 lessons in the context of this research project because we think that this context will help you to make sense of the data, and to see why we ask you to practice the skills we are teaching.\nWe will be revisiting some of the ideas and techniques you have seen introduced in previous classes. This is to give you the opportunity to revise and consolidate your learning. We will extend your development with some new ideas, to strengthen your skills.\nUltimately, we aim to contribute new findings from the data we will collect together. These new findings will, we hope, help to make the provision of health advice a bit more useful in future.\n\n\nIn Week 16, we will ask you to do three things.\n\n\n\n\n\n\nNote\n\n\n\nFirst, we will ask you to do a pre-lab activity that involves completing a survey.\n\n\n\nCompleting the survey will help you to make sense of the numbers you will be working with in the activities.\nCompleting the pre-lab activity will help to teach you about the challenges of measurement, a key aspect of the scientific thinking skills we will help you to develop.\n\n\n\n\n\n\n\nNote\n\n\n\nSecond, we will ask you to do a set of practical tasks in the lab activity that are designed to consolidate your learning on data visualization.\n\n\n\nWe will be:\n\n\nUsing histograms to examine the distributions of variables;\nLearning to edit the histograms to present them professionally.\n\n\n\n\n\n\nFigure 1: How-to guide example of a histogram showing observed mean accuracy of understanding of health information\n\n\n\n\n\nWe will be:\n\n\nUsing scatterplots to examine the association of variables;\nLearning to edit the plots to present them professionally.\n\n\n\n\n\n\nFigure 2: How-to guide example of a scatterplot representing the potential association between values of health literacy (HLVA) and mean self-rated accuracy of understanding of health information.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThird, we will ask you to think critically about predictions about potential associations between measures of participant attributes and measures of understanding of health information.\n\n\n\nWe will use correlations to test predictions about associations and thus answer research questions."
  },
  {
    "objectID": "PSYC122/Week16.html#sec-resources-intro",
    "href": "PSYC122/Week16.html#sec-resources-intro",
    "title": "Statistics for Psychologists",
    "section": "Your resources",
    "text": "Your resources\nYou will see – below – links to the lectures, information about the data we will analyze, and an explanation of the activities.\nAll the links to the lectures, and everything you need for your practical work class can also be found in the Week 16 files folder on Moodle, here:\nLink to Moodle\n\nLectures: video recordings\nThe lecture material for this week is presented in four short parts.\nClick on a link and your browser should open a tab showing the Panopto video for the lecture part. (You will need to be on campus or logged in to the university VPN to get access to the videos.)\nPart 1 of 4; about 15 minutes\nPart 2 of 4; about 11 minutes\nPart 3 of 4; about 22 minutes\nPart 4 of 4; about 11 minutes\nThe lecture is designed to give you an overview of:\n\nWhat we are doing in weeks 16-20, and how and why you will develop your critical thinking skills;\nA summary of the health communication project, and the ideas we assume to develop our hypotheses;\nHow we visualize and think about distributions and associations;\nAnd how we use R to estimate and test correlations.\n\n\n\n\n\n\n\nTip\n\n\n\nTo work with the recordings:\n\nWatch the video parts right through.\nUse the printable versions of the slides (provided on Moodle) to make notes.\nTry out the coding exercises in the how-to and the workbook (see Section 2.5.1) to see for yourself how you can construct visualizations and do analyses.\n\n\n\n\n\nLinks to other classes\nWe do not provide further reading for this class but you will find it helpful to revise some of the key ideas you have been learning about PSYC122 and in other modules.\n\nThe lectures in PSYC123 on: (week 1) the scientific method; (week 2) reliability and validity; (week 3) experimental design, especially between-subjects studies; (week 6) hypothesis testing; and (week 9) precise hypotheses.\nThe lecture in PSYC122 on (week 11) correlation.\n\n\n\nPre-lab activity 1\nIn weeks 16-19, we will be working together on a research project to investigate how people vary in their response to health advice.\nCompleting the project involves collecting responses from PSYC122 students.\nTo enter your responses, we invite you to complete a short survey.\nComplete the survey by clicking on the link here\n\n\n\n\n\n\nTip\n\n\n\nIn our week 19 class activity, we will analyze the data we collect here.\n\n\n\nSurvey information\nThe survey asks you to:\n\ncomplete some questions about who you are;\nand then answer some questions about what you know about some English words, about what you know about health matters, and about how you approach reading.\n\nThe survey then asks you to:\n\nread five short extracts from patient information leaflets about different kinds of health issue;\nrespond to some multiple choice questions about each extract;\nand rate how well you think you understand the advice.\n\nThe survey should take about 20 minutes to complete. Some people will take less time, and some people might take a little more time.\nTaking part in the survey is completely voluntary. You can stop at any time without completing the survey if you do not want to finish it. If you do not want to do the survey, you can do an alternative activity (see below).\nAll responses will be recorded completely anonymously.\n\n\n\nPre-lab activity 1 alternative\nIf you do not want to complete the survey, we invite you to read the pre-registered research plan for the PSYC122 health advice research project.\nRead the project pre-registration\n\n\nPre-lab activity 2: Getting ready for the lab class\n\nGet your files ready\nDownload the 122-22-w16_for-students.zip files you need and upload them to your RStudio Server.\nThe folder includes data files:\n\nstudy-one-general-participants.csv\nstudy-two-general-participants.csv\n\nand the code files:\n\n2022-23-PSYC122-w16-how-to.R\n2022-23-PSYC122-w16-workbook.R\n\nAlternatively, you can instead download the resources you need from the module Moodle page for PSYC122:\nLink to Moodle\n\n\nWhat is in the how-to and workbook.R files?\n\n\n\n\n\n\nImportant\n\n\n\nYou have been getting used to working with .R script files.\n\nNow our aim is to make sure you can work with code, and write notes in the files.\nIn the workbook you use for the lab activity, we identify tasks and questions, and leave you spaces where you can write code or answers.\n\n\n\nIn both the .R files:\n\n2022-23-PSYC122-w16-how-to.R\n2022-23-PSYC122-w16-workbook.R\n\nwe will take things step-by-step.\nWe split .R scripts into parts, tasks and questions:\n\ndifferent parts for different phases of the analysis process;\ndifferent tasks for different steps in each phase;\ndifferent questions to examine different ideas or coding steps.\n\n\n\n\n\n\n\nTip\n\n\n\n\nMake sure you start at the top of the .R file and work your way, in order, through each task.\nComplete each task before you move on to the next task.\n\n\n\n\n\n\nReview the how-to guide\nThe how-to guide comprises an .R file 2022-23-PSYC122-w16-how-to.R with code and advice. The code in the .R file was written to work with the data file:\n\nstudy-one-general-participants.csv.\n\n\n\n\n\n\n\nTip\n\n\n\nWe show you how to do everything you need to do in the lab activity (Section 2.7) in the how-to guide.\n\nStart by looking at the how-to guide to understand what steps you need to follow in the lab activity.\n\n\n\n\n\nLab activity\nIn the lab activity .R file 2022-23-PSYC122-w16-workbook.R, you will work with data from a study about how people respond to guidance about a variety of health topics (general topics):\n\nstudy-two-general-participants.csv\n\nThe data are similar in format to the response data we are collecting as part of the PSYC122 project.\n\nTasks\nIn the activity workbook, we are going to work through the following tasks.\n\nEmpty the R environment – using rm(list=ls())\nLoad relevant libraries – using library()\nRead in the data file – using read_csv()\nInspect the data – using head() and summary()\nChange the type classification of a variable in the data – using as.factor()\nDraw histograms to examine the distributions of variables – using ggplot() and geom_histogram()\nEdit the appearance of one variable histogram plot step-by-step\nDraw scatterplots to examine the associations between some variables – using ggplot() and geom_point()\nDraw scatterplots to examine different variables\nEdit the appearance of a plot step-by-step\nExamine associations between variables using correlation.\n\nThe activity 2022-23-PSYC122-w16-workbook.R file takes you through the tasks, one by one.\n\n\n\n\n\n\nTip\n\n\n\nIf you are unsure about what you need to do, look at the advice in 2022-23-PSYC122-w16-how-to.R on what steps you have to follow, and examples on how to write the code.\n\n\nYou will see that you can match a task in the activity to the same task in the how-to. The how-to shows you what function you need and how you should write the function code.\n\nDon’t forget: You will need to change the names of the dataset or the variables to complete the tasks in the activity.\n\nThis process of adapting demonstration code is a process critical to data literacy and to effective problem solving in working with data in psychological science.\n\n\nWhat is in the data files\nEach of the data files we will work with has a similar structure, as you can see in this extract.\n\n\n\n\n\nparticipant_ID\nmean.acc\nmean.self\nstudy\nAGE\nSHIPLEY\nHLVA\nFACTOR3\nQRITOTAL\nGENDER\nEDUCATION\nETHNICITY\n\n\n\n\nstudytwo.1\n0.4107143\n6.071429\nstudytwo\n26\n27\n6\n50\n9\nFemale\nHigher\nAsian\n\n\nstudytwo.10\n0.6071429\n8.500000\nstudytwo\n38\n24\n9\n58\n15\nFemale\nSecondary\nWhite\n\n\nstudytwo.100\n0.8750000\n8.928571\nstudytwo\n66\n40\n13\n60\n20\nFemale\nHigher\nWhite\n\n\nstudytwo.101\n0.9642857\n8.500000\nstudytwo\n21\n31\n11\n59\n14\nFemale\nHigher\nWhite\n\n\n\n\n\n\n\nYou can use the scroll bar at the bottom of the data window to view different columns.\nYou can see the columns:\n\nparticipant_ID participant code\nmean.acc average accuracy of response to questions testing understanding of health guidance\nmean.self average self-rated accuracy of understanding of health guidance\nstudy varianble coding for what study the data were collected in\nAGE age in years\nHLVA health literacy test score\nSHIPLEY vocabulary knowledge test score\nFACTOR3 reading strategy survey score\nGENDER gender code\nEDUCATION education level code\nETHNICITY ethnicity (Office National Statistics categories) code\n\n\n\n\n\n\n\nTip\n\n\n\nIt is always a good idea to view the dataset – click on the name of the dataset in the R-Studio Environment window, and check out the columns, scroll through the rows – to get a sense of what you are working with.\n\n\n\n\n\nAnswers\n\n\n\n\n\n\nTip\n\n\n\nYou can now download the answers version of the activity workbook .R here.\n\n\nThe answers version presents my answers for questions, and some extra information where that is helpful."
  },
  {
    "objectID": "PSYC402/Week8.html",
    "href": "PSYC402/Week8.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC402/Week9.html",
    "href": "PSYC402/Week9.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC402/Week1.html",
    "href": "PSYC402/Week1.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC402/Week2.html",
    "href": "PSYC402/Week2.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC402/Week3.html",
    "href": "PSYC402/Week3.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC402/Week7.html",
    "href": "PSYC402/Week7.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC402/Week6.html",
    "href": "PSYC402/Week6.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC402/Week4.html",
    "href": "PSYC402/Week4.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week8.html",
    "href": "PSYC214/Week8.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week9.html",
    "href": "PSYC214/Week9.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week1.html",
    "href": "PSYC214/Week1.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week2.html",
    "href": "PSYC214/Week2.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week3.html",
    "href": "PSYC214/Week3.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week7.html",
    "href": "PSYC214/Week7.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week6.html",
    "href": "PSYC214/Week6.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "PSYC214/Week4.html",
    "href": "PSYC214/Week4.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Hi"
  }
]