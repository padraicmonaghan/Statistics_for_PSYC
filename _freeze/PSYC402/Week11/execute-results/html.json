{
  "hash": "83f9ca88fc869e0ba7215fe26304b0a4",
  "result": {
    "markdown": "---\ntitle: 1. Recap of the linear model and practising data-wrangling in R\nsubtitle: Written by Margriet Groen (partly adapted from materials developed by the PsyTeachR team a the University of Glasgow)\norder: 2\n---\n\n\n\n\nBefore we start covering new material, we want to spent some time on recapping the basic concepts of the linear model (correlation, simple regression, multiple regression). You all come from different educational backgrounds and therefore have vastly different knowledge of, and experience with statistics. Therefore, please follow your own judgement as to whether you feel you want to/need to revisit material outlining the theoretical background to and the practical implementation in R for these topics. Below we provide some guidance as to materials that are relevant. **Just to be clear: We don’t expect you to watch and/or read and/or do everything, please have a look at what you feel you need and spend some time with those materials.**\n\n## Lectures\n**The linear model was discussed in weeks 6 to 9 of PSYC401, so that is a good place to start.**\n\nAlternatively, if you don't feel confident about the material, these recorded lectures might help.\n\n* [**The linear model: theory (~30 min)**](https://estream.lancaster.ac.uk/View.aspx?id=60909~5l~EskRm8Serj) \nAn introduction to the linear model and linear regression. I follow material as discussed in Chapter 4 of Bodo Winter's book *Statistics for Linguists: An Introduction using R* (see below under 'Reading').\n\n* [**How to build a linear model in R (~30 min)**](https://estream.lancaster.ac.uk/View.aspx?id=60910~5d~uUTzHPgBLN)\nIn this video I demonstrate how to build a linear model in R by talking you through a simple linear regression script (you can download it here [stats_linearModel_howTo.R](images/stats_linearModel_howTo.R)). If you are unclear on what different parts of the lm() function do, or how to read the output, this video might help clarify that.\n\n* [**Multiple regression: theory (~35 min)**](https://estream.lancaster.ac.uk/View.aspx?id=60911~5e~WIhZrOrXiD)\nAn introduction to multiple regression. I follow material as discussed in Chapter 5 of Bodo Winter's book *Statistics for Linguists: An Introduction using R* (see below under 'Reading').\n\n* [**Centering and standardising (~5 min)**](https://estream.lancaster.ac.uk/View.aspx?id=60912~5f~xjxBbNDjGF)\nBrief explanation of what centering and standardising are.\n\n## Reading\n\n### Miller & Haden (2013)\n[**Link**](https://drive.google.com/file/d/0B1fyuTuvj3YoaFdUR3FZaXNuNXc/view)\n\n**Chapter 10** gives you a brief overview of what correlation and regression are. **Chapter 11** introduces correlation in more detail. **Chapters 12 and 14** provide accessible overviews of simple and multiple regression, respectively. All these chapters are really short but provide a good basis to understanding. We consider this the minimum level of understanding you should acquire.\n\n### Winter (2020)\n[**Link**](https://eu.alma.exlibrisgroup.com/leganto/public/44LAN_INST/citation/83408786230001221?auth=SAML)\n\n**Chapter 4** provides and excellent conceptual introduction to the linear model and also explains how this is implemented in R (highly recommended).\n\n**Chapter 5** takes a slightly different approach to the one taken in Miller & Haden (2013) to introducing correlation. If you already understand the basic theory behind correlation, this will be an interesting read. Chapter 5 also clearly explains what centering and standardizing are and why you need to bother with these linear transformations.\n\n**Chapter 6** provides an excellent overview of multiple regression and also explains how this is implemented in R. \n\n## Pre-lab activities\nAfter having watched the lectures and read the textbook chapters you’ll be in a good position to try these activities. Completing them before you attend your lab session will help you to consolidate your learning and help move through the lab activities more smoothly.\n\n### Pre-lab activity 1: Visualising the regression line\nHave a look at [**this visualisation of the regression line**](https://ryansafner.shinyapps.io/ols_estimation_by_min_sse/) by Ryan Safner.\n\nIn this shiny app,  you see a randomly-generated set of data points (within specific parameters, to keep the graph scaled properly). You can choose a slope and intercept for the regression line by using the sliders. The graph also displays the residuals as dashed red lines. Moving the slope or the intercept too much causes the generated line to create much larger residuals. The shiny app also calculates the sum of squared errors (SSE) and the standard error of the regression (SER), which calculates the average size of the error (the red numbers). These numbers reflect how well the regression line fits the data, but you don’t need to worry about those for now.\n\nIn the app he uses the equation Y = aX + b in which b is the intercept and a is the slope.\n\nThis is slightly different from the equation you saw during the lecture. There we talked about Y = b0 + b1*X + e. Same equation, just different letters. So b0 in the lecture is equivalent to b in the app and b1 in the lecture is equivalent to a in the app.\n\nPre-lab activity questions:\n\n1. Change the slider for the intercept. How does it change the regression line?\n2. Change the slider for the slope. How does it change the regression line?\n3. What happens to the residuals (the red dashed lines) when you change the slope and the intercept of the regression line?\n\n### Pre-lab activity 2: Data-wrangling in R\nIn PSYC401, you've already learned how to read in data, how to select variables and how to compute summary statistics, so re-visiting the PSYC401 materials is a good place to start.\n\nRStudio also provides some useful interactive tutorials that take you through the basics:\n\n* [**The Basics**](https://rstudio.cloud/learn/primers/1) Start here to learn how to inspect, visualize, subset and transform your data, as well as how to run code.\n\n* [**Work with Data**](https://rstudio.cloud/learn/primers/2) Learn how to extract values form a table, subset tables, calculate summary statistics, and derive new variables.\n\n* [**Visualize Data**](https://rstudio.cloud/learn/primers/3) Learn how to use ggplot2 to make any type of plot with your data. The tutorials on [**Exploratory Data Analysis**](https://rstudio.cloud/learn/primers/3.1) and [**Scatterplots**](https://rstudio.cloud/learn/primers/3.5) are particularly relevant.\n\nPlease note that there are often different ways to do the same or similar things in R. This means you might encounter slightly different functions or styles of coding in different materials. This is not something to worry about. Just make sure you're clear on what a bit of code achieves and choose the function/style that you feel most comfortable with.\n\n### Pre-lab activity 3: Getting ready for the lab class\n#### Remind yourself of how to access and work with the RStudio Server.\n\n* Revisit [**PSYC401**](https://modules.lancaster.ac.uk/course/view.php?id=37245) to remind yourself of how to access the RStudio Server.\n\n* I highly recommend using R Projects to structure your workflow. You could create an R project for each week of the module. Have a look at section [**8 Workflow: projects**](https://r4ds.had.co.nz/workflow-projects.html) of **R for Data Science** by Hadley Wickam and Gareth Grolemund for an introduction.\n\n#### Get your files ready\nDownload the [402_week11_forStudents.zip](images/402_week11_forStudents.zip) file and upload it into a new folder in RStudio Server. \n\n## Lab activities\nIn this lab, you’ll gain understanding of and practice with:\n\n* when and why to apply simple and multiple regression to answer questions in psychological science\n* conducting multiple regression in R\n* interpreting the R output of simple and multiple linear regression\n* reporting results for simple and multiple linear regression following APA guidelines\n\n### Lab activity 1: Interpreting and reporting results\n\nHave a look at the R output below.\n\n**R Output 1**\n\n![](images/w11_labAct1_output.png)\n\n1. What is the outcome or dependent variable?\n2. What is the predictor or independent variable?\n3. Is the overall model significant?\n4. How much variance does the model account for?\n\nThinking about assumptions, what do you conlcude from the plots and output below?\n\n5. Does the relationship appear linear?\n\n6. Do the residuals show normality and homoscedasticity?\n\n**Scatterplot**\n\n![](images/w11_labAct1_scatterplot.png)\n\n**QQ-plot**\n\n![](images/w11_labAct1_qqplot.png)\n\n**R Output 2**\n\n![](images/w11_labAct1_output2.png)\n\n### Lab activity 2: Conducting simple and multiple regression\n\n#### Background\nToday, to help get a practical understanding of regression, you will be working with real data and using regression to explore the question of whether there is a relationship between voice acoustics and ratings of perceived trustworthiness.\n\n##### The Voice\nThe prominent theory of voice production is the source-filter theory (Fant, 1960) which suggests that vocalisation is a two-step process: air is pushed through the larynx (vocal chords) creating a vibration, i.e. the source, and this is then shaped and moulded into words and utterances as it passes through the neck, mouth and nose, and depending on the shape of those structures at any given time you produce different sounds, i.e. the filter. One common measure of the source is pitch (otherwise called Fundamental Frequency or F0 (F-zero)) (Titze, 1994), which is a measure of the vibration of the vocal chords, in Hertz (Hz); males have on average a lower pitch than females for example. Likewise, one measure of the filter is called formant dispersion (measured again in Hz), and is effectively a measure of the length of someone’s vocal tract (or neck). Height and neck length are suggested to be negatively correlated with formant dispersion, so tall people tend to have smaller formant dispersion. So all in, the sound of your voice is thought to give some indication of what you look like.\n\nMore recently, work has focussed on what the sound of your voice suggests about your personality. McAleer, Todorov and Belin (2014) suggested that vocal acoustics give a perception of your trustworthiness and dominance to others, regardless of whether or not it is accurate. One extension of this is that trust may be driven by malleable aspects of your voice (e.g. your pitch) but not so much by static aspects of your voice (e.g. your formant dispersion). Pitch is considered malleable because you can control the air being pushed through your vocal chords (though you have no conscious control of your vocal chords), whereas dispersion may be controlled by the structure of your throat which is much more rigid due to muscle, bone, and other things that keep your head attached. This idea of certain traits being driven by malleable features and others by static features was previously suggested by Oosterhof and Todorov (2008) and has been tested with some validation by Rezlescu, Penton, Walsh, Tsujimura, Scott and Banissy (2015).\n\nSo, the research question today is: Can vocal acoustics, namely pitch and formant dispersion, predict perceived trustworthiness from a person’s voice? We will only look at male voices today, but you have the data for female voices as well should you wish to practice (note that in the field, tendency is to analyse male and female voices separately as they are effectively sexually dimorphic). As such, we hypothesise that a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. This is what we will analyse.\n\nTo complete this lab activity, you can use the R-script (402_wk11_labAct2.R) that you downloaded as part of the 'pre-lab activities' as a template. Work through the activity below, adding relevant bits of code to your script as you go along.  \n\n#### Step 1: Background and set up\n\nBefore you do anything else, when starting a new analysis, it is a good idea to empty the R environment. This prevents objects and variables from previous analyses interfering with the current one. \n\n> **TASK**: Use the code snippet below to clear the environment. **TIP**: If you hover your mouse over the box that includes the code snippet, a 'copy to clipboard' icon will appear in the top right corner of the box. Click that to copy the code. Now you can easily paste it into your script.\n![](images/copy.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls())                            \n```\n:::\n\n\nBefore we can get started we need to tell R which libraries to use. For this analysis we'll need broom, car, pwr and tidyverse.\n\n> **TASK**: Load the relevant libraries. **HINT**: Use the `library()` function.\n\nIn this lab, we are setting out to test whether a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. We'll be working with two data files:\n\n* voice_acoustics.csv - shows the VoiceID, the sex of the voice, and the pitch and dispersion values\n* voice_ratings.csv - shows the VoiceID and the ratings of each voice by 28 participants on a scale of 1 to 9 where 9 was extremely trustworthy and 1 was extremely untrustworthy.\n\n> **TASK**: Read in both files (using the `read_csv()` function), have a look at the layout of the data and familiarise yourself with it. The ratings data is rather messy and in a different layout to the acoustics but can you tell what is what?\n\n**QUESTION 1**\nHow are the acoustics data and the ratings data organised (wide or long)? Are both data files 'tidy'? If you need more info on what that means, have a look [**here**](https://r4ds.had.co.nz/tidy-data.html).\n\n#### Step 2: Restructuring the ratings data\n\nWe are going to need to do some data-wrangling before we do any analysis! Specifically, we need the change the ratings data to the long format.\n\nHere we'll use the `pivot_longer()` function (see [**here**](https://tidyr.tidyverse.org/reference/pivot_longer.html) or type `?pivot_longer` in the Console for more info) to restructure the ratings data from wide to long and store the resulting table as 'ratings_tidy'.\n\n> **TASK**: Use the code snippet below to restructure the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nratings_tidy <- pivot_longer(\n  data = ratings,    # the data you want to restructure\n  cols = P1:P28,     # columns you want to restructure\n  names_to = \"participant\", # variable name that captures whatever is across the columns\n  # (in this case P1 to P28 for the 28 different participants)\n  values_to = \"rating\") # variable name that captures whatever is in the cells\n  # (in this case numbers for ratings)\n```\n:::\n\n\n#### Step 3: Calculate mean trustworthiness rating for each voice \n\nNow that we have the ratings data into a tidy format, the next step is to calculate the mean rating for each voice. Remember that each voice is identified by the 'VoiceID' variable.\n\n> **TASK**: Calculate the mean rating for each voice and store the resulting table in a variable named 'ratings_mean'. **HINT**: Use `group_by()` and `summarise()`. Are you using the tidy data? Also, remember that if there are any missing values (NAs) then na.rm = TRUE would help.\n\n#### Step 4: Join the data together\n\nOk, before we get ahead of ourselves, in order to perform the regression analysis we need to combine the data from 'ratings_mean' (the mean ratings) with 'acoustics' (the pitch and dispersion ratings). Also, as we said, we only want to analyse male voices today.\n\n> **TASK**: Join the two tables and keep only the data for the male voices, call the resulting table 'joined'. **HINT**: Use the `inner_join()` function (making use of the variable that is common across both tables) to join. See [**here**](https://r4ds.had.co.nz/relational-data.html?q=inner#understanding-joins) or type `?inner_join` in the Console for more info. Use the `filter()` function to only keep male voices. Remember that the Boolean operator for exactly equal is `==`.\n\n#### Step 5: Spreading the data\nOk so we are starting to get an understanding of our data and we want to start thinking about the regression. However, the regression would be easier to work with if Pitch and Dispersion were in separate columns. This can be achieved using the `pivot_wider()` function (see [**here**](https://tidyr.tidyverse.org/reference/pivot_wider.html) or type `?pivot_wider` in the Console for more info). This is basically the inverse of `pivot_longer()`. It increases the number of columns and decreases the number of rows.\n\n> **TASK**: Use the code snippet below to spread the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njoined_wide <- joined %>%\n  pivot_wider(\n    names_from = measures, # name of the categorical column to spread\n    values_from = value) # name of the data to spread\n```\n:::\n\n\n**QUESTION 2**\nWhy do we not need to specify within the `pivot_wider()` function which data to use?\n\n#### Step 6: Visualising the data\nAs always, it is a good idea to visualise your data.\n\n>**TASK**: Now that we have all the variables in one place, make two scatterplots, one of mean trustworthiness rating with dispersion and one for mean trustworthiness rating and pitch. **HINT**: For this you'll need the `ggplot()` function together with `geom_point()` and `geom_smooth()`. Make sure to give your axes some sensible labels.\n\n**QUESTION 3**\nAccording to the scatterplots, how would you decribe the relationships between trustworthiness\nand dispersion and trustworthiness and pitch in terms of direction and strength? Which one of the two \nseems stronger?\n\n#### Step 7: Conducting and interpreting simple regression\nWith all the variables in place, we're ready now to start building two simple linear regression models:\n\n1. Predicting trustworthiness mean ratings from Pitch\n\n2. Predicting trustworthiness mean ratings from Dispersion\n\n> **TASK**: Use the `lm()` function to run the following two regression models and use the `summary()` function to look at the output of each model. Store the first model in a table called 'mod_pitch' and store the second model in 'mod_disp'. **HINT**: `lm(dv ~ iv, data = my_data)`\n\n**QUESTION 4**\nWhat do you conclude from the output of these models? Which model is significant? Which predictors\nare significant? How much variance does each model describe?\n\n#### Step 8: Conducting and interpreting multiple regression\nNow let's look at both predictors in the same model. Before we do this, it is sensible to center and standardise the predictors.\n\nLook at the code below. Can you follow how the predictors are first centered (_c) and then standardised (_z)?\n\nHere I do this by hand because I think it makes it clearer, even though there are functions that do this in one step (`scale()`). \n\n\n::: {.cell}\n\n```{.r .cell-code}\njoined_wide <- mutate(joined_wide,\n                      Dispersion_c = Dispersion - mean(Dispersion),\n                      Dispersion_z = Dispersion_c / sd(Dispersion_c),\n                      Pitch_c = Pitch - mean(Pitch),\n                      Pitch_z = Pitch_c / sd(Pitch_c))\n```\n:::\n\n\n> **TASK**: Now use the centered and standardised data for the multiple regression. Use the `lm()` function to run a model for predicting trustworthiness mean ratings from Pitch and Dispersion, and store the model in 'mod_pitchdisp_z'. Use the 'summary()' function to look at the output. **HINT**: `lm(dv ~ iv1 + iv2, data = my_data)`\n\n**QUESTION 5**\nWhat do you conclude from the output of this model? Is the overall model significant? Which predictors\nare significant? How much variance does the model describe? Which model would you say is best for predicting ratings of trustworthiness, the Pitch only, the Dispersion only or the Pitch+Dispersion model?\n\n#### Step 9: Checking assumptions\n\nNow that we've established which model best fits the data, let's check whether it meets the assumptions of linearity, normality and homoscedasticity.\n\n> **TASK**: Check the assumptions of linearity, normality and homoscedasticity.\n**HINT**: `crPlots()` to check linearity\n      `qqPlot()` and `shapiro.test()` to check normality of the residuals\n      `residualPlot()` and `nvcTest()` to check homoscedasticity of the residuals\n\n**QUESTION 6**\nWhat do you conclude from the graphs and output? Should we also check for collinearity?\n\n#### Step 10: Writing up the results\n\n> **TASK**: Write up the results following APA guidelines. **HINT**: [**The Purdue writing lab website**](https://owl.purdue.edu/owl/research_and_citation/apa6_style/apa_formatting_and_style_guide/statistics_in_apa.html) is helpful for guidance on punctuating statistics. The [**APA Style 7th Edition Numbers and Statistics Guide**](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf) is also useful.\n\n## Answers\n\nWhen you have completed all of the lab content, you may want to check your answers with our completed version of the script for this week. **Remember**, looking at this script (studying/revising it) does not replace the process of working through the lab activities, trying them out for yourself, getting stuck, asking questions, finding solutions, adding your own comments, etc. **Actively engaging** with the material is the way to learn these analysis skills, not by looking at someone else's completed code...\n\n<!--The answers to the questions and the script containing the code will be available after lab session has taken place.-->\n\n### Lab activity 1: Interpreting and reporting results\n1. What is the outcome or dependent variable? **Word reading**\n2. What is the predictor or independent variable? **Non-word reading**\n3. Is the overall model significant? **Yes, *F*(1,50) = 69.03, *p* < .001**\n4. How much variance does the model account for? **58%**\n5. Does the relationship appear linear? **Yes. The dots and the pink line assemble quite closely on the dashed line.**\n6. Do the residuals show normality and homoscedasticity? **The qq-plot suggest that the residuals are normally distributed as the dots fall close to the solid blue line and within the range of the dashed blue lines. The Shapiro-Wilk test of normality confirms this (it is not significant). Similarly, the output of the non-constant variance score tests is not significant suggesting that the residuals are homoscedastic.**\n\n### Lab activity 2: Conducting simple and multiple regression\nYou can download the R-script that includes the relevant code and answers to the questions here: [402_wk11_labAct2_withAnswers.R](images/402_wk11_labAct2_withAnswers.R).",
    "supporting": [
      "Week11_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}