{
  "hash": "26c6068b5df8bc77b8eb88e5162cf138",
  "result": {
    "markdown": "---\ntitle: 7. Introduction to linear mixed-effects models\nsubtitle: Written by Rob Davies\norder: 8\n---\n\n::: {.cell}\n\n:::\n\n\n## Motivations: repeated measures designs and crossed random effects {#sec-intro-motivations}\n\nIn [Week 16](Week16.qmd), we looked at a multilevel structured dataset in which there were observations about children's grades, and it became evident that those children can be grouped by or under classes. As we discussed, this kind of data structure will come from studies with a very common design in which, for example, the researcher records observations about a sample of children who are members of a sample of classes. In working with these kind of data, it is common to say that the observations of children's grades are *nested* within classes in a hierarchy.\n\nMany Psychologists conduct studies where observations are properly understood to be structured in groups of some form but where, nevertheless, it is inappropriate to think of the observations as being nested [@baayen2008]. We are talking, here, about **repeated-measures designs** where the experimenter presents a sample of multiple stimuli for response to each participant in a sample of multiple participants. This is another *very* common experimental design in psychological science.\n\nStudies with repeated-measures designs will produce data with a structure that, also, requires the use of mixed-effects models but, as we shall see, the way we think about the structure will be a bit more complicated. We could say that observations of the responses made by participants to each stimulus can be grouped by participant: each person will tend to respond in similar ways to different stimuli. Or, we could say that observations of responses can be grouped by stimulus because each stimulus will tend to evoke similar kinds of responses in different people. Or, we could say that both forms of grouping should be taken into account at the same time.\n\nWe shall take the third position and this chapter will concern why, and how we will adapt our thinking and practice.\n\n## The key idea to get us started {#sec-intro-key}\n\n::: callout-important\nLinear mixed-effects models and multilevel models are basically the same.\n:::\n\nThis week, we again look at data with multilevel structure. But we are looking at data where participants were asked to respond to a set of stimuli (here, words) so that our observations consist of recordings made of the response made by each child to each stimulus. We use the same procedure we did for multilevel data but with one significant change which we shall identify and explain.\n\n## Targets {#sec-intro-targets}\n\nOur learning objectives again include the development of both concepts and skills.\n\n1.  **skills** -- practice how to tidy experimental data for mixed-effects analysis.\n2.  **concepts** -- begin to develop an understanding of crossed random effects of participants and stimuli.\n3.  **skills and concepts** -- practice fitting linear mixed-effects models incorporating random effects of participants and stimuli.\n\n## Study guide {#sec-intro-mixed-guide}\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n\n**1. Video recordings of lectures**\n\n1.1. I have recorded a lecture in three parts. The lectures should be accessible by anyone who has the link.\n\n-   [Part 1](https://dtu-panopto.lancs.ac.uk/Panopto/Pages/Viewer.aspx?id=2fa60b12-2eca-4412-8672-acd60104da1d) -- about 8 minutes\n-   [Part 2](https://dtu-panopto.lancs.ac.uk/Panopto/Pages/Viewer.aspx?id=7f31cef0-1e39-4712-b2a1-acd601195c78) -- about 21 minutes\n-   [Part 3](https://dtu-panopto.lancs.ac.uk/Panopto/Pages/Viewer.aspx?id=58ab0c0c-37be-491f-a9e5-acd6011c38dd) -- about 15 minutes\n\n1.2. I suggest you watch the recordings then read the rest of this chapter. The lectures provide a summary of the main points.\n\n**2. Chapter: 02-mixed**\n\n2.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to analyse multilevel structured data with crossed random effects. \n\n2.2. The practical elements include data tidying, visualization and analysis steps. \n\n2.3. You can read the chapter, run the code, and do the exercises.\n\n-   Read in the example **CP reading study** datasets.\n-   Identify how the data are structured by both participant and stimulus differences.\n-   Use visualizations to explore the impact of the structure.\n-   Run analyses using linear mixed-effects models involving multiple random effects.\n-   Review the recommended readings (@sec-intro-mixed-recommended-reading).\n\n**3. Practical workbook materials**\n\n3.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning.\n\n## The data we will work with: CP reading study {#sec-intro-mixed-data}\n\nIn this chapter, we will be working with the **CP reading study** dataset. CP tested 62 children (aged 116-151 months) on reading aloud in English. In the experimental reading task, she presented 160 words as stimuli. The same 160 words were presented to all children. The words were presented one at a time on a computer screen. Each time a word was shown, each child had to read the word out loud and their response was recorded. Thus, the CP reading study dataset comprised observations about the responses made by 62 children to 160 words.\n\nIn addition to the reading task, CP administered tests of reading skill [TOWRE sight word and phonemic tests, @torgesen1999towre], reading experience [CART, @stainthorp1997children], the Spoonerisms sub-test of the Phonological Awareness test Battery [PhAB, @Frederickson1997a], and an orthographic choice test measure of orthographic knowledge. She also recorded the gender and the handedness of the children.\n\nWe are going to use the CP study data to examine the answers to a research question similar to the question CP investigated:\n\n::: callout-note\n-   Research question: What word properties influence responses to words in a test of reading aloud?\n:::\n\nWe can look at the answers to this question while also taking into account the impacts of random differences -- between sampled participants or between sampled words -- using mixed-effects models.\n\nUltimately, the CP dataset were incorporated in an analysis of the impact of age on reading skills over the life-span, reported by @davies2017. You can find more details on the data and the methods in that paper. (Data and analysis code are shared through the journal article webpage \\[paywalled\\] [here](https://psycnet.apa.org/doiLanding?doi=10.1037%2Fxlm0000366), and a preprint version of the article can be accessed [here](https://eprints.lancs.ac.uk/id/eprint/86453/1/individual_differences_in_reading_subm_accepted.pdf).)\n\nThe CP study resembles many studies in psychological science. The critical features of the study are that:\n\n-   We have an outcome measure -- the reading response -- observed multiple times.\n\n1.  We have *multiple responses recorded for each participant*: they make one response to each stimulus (here, each stimulus word), for the multiple stimuli that they see in the experimental reading task.\n2.  And we have *multiple responses recorded for each stimulus*: one response is made to each stimulus by each participant, for all the participants who completed the task, in a sample of multiple participants.\n\nThe presence of these features is the reason why we need to use mixed-effects models in our analysis. These features are common across a range of study designs so the lessons we learn will apply frequently in psychological research. This is the reason why it is important we teach and learn how to use mixed-effects models.\n\n### Locate and download the data file {#sec-intro-mixed-data-download}\n\nYou can download the [data-02-mixed.zip](files/data-02-mixed.zip) files folder to get the data you need for the practical work we will be doing for this chapter.\n\nWe will be working with multiple data files located in a .zip folder called `data-02-mixed`. In this folder, we have got four files that we will need to import or read in to R:\n\n-   `CP study word naming rt 180211.dat`\n-   `CP study word naming acc 180211.dat`\n-   `words.items.5 120714 150916.csv`\n-   `all.subjects 110614-050316-290518.csv`\n\nThe `words.items` file holds information about the 160 stimulus words presented in the experimental reading (word naming) task. The `all.subjects` file holds information about the 62 participants who volunteered to take part in the experiment. These `.csv` files are *comma separated values* files.\n\nThe `.dat` files are *tab delimited* files holding behavioural data: the latency or reaction time `rt` (in milliseconds) and the accuracy `acc` of response made by each participant to each stimulus.\n\nIn the following, I will describe a series of steps through which we get the data ready for analysis. However, as we shall see, you can avoid these steps by using the pre-tidied dataset:\n\n-   `long.all.noNAs.csv`\n\nThe data files are collected together with the .R scripts:\n\n-   `02-mixed-workbook.R` the workbook you will need to do the practical exercises.\n-   `02-mixed-workbook-answers.R` with answers to questions and code for exercises.\n\nBefore we do anything else, we need to talk about the *messiness* of real Psychological data and how we deal with it.\n\n## The challenges of working with real (untidy) experimental data {#sec-intro-mixed-data-untidy}\n\nOrdinarily, textbooks and guides to data analysis give you the data ready for analysis but this situation will never be true for your professional practice (at least, not at first). Instead of pretending that data arrive ready for analysis, we are going to look at the process of **data tidying**, step-by-step. This will help you to get ready for the same process when you have to develop and use it in your own research.\n\nWe are going to spend a bit of time looking at the data tidying process. This process involves identifying and resolving a series of challenges, in order. Looking at the tidying process will give you a concrete sense of the structure in the data. You should also take this opportunity to reflect on the nature of the process itself -- what we have to do and why, in what order and why -- so that you can develop a sense of the process you might need to build when the time comes for you to prepare your own data for analysis.\n\nThe time that we spend looking at data tidying is an investment in learning that will save you time later, in your professional work. If, however, you want to skip it, go to section @sec-intro-mixed-crossed-random.\n\n### The data we need to use for analysis are not all in the same file {#sec-intro-data-distributed}\n\nIn analyzing psychological data, the first step is usually to collect the data together. In psychological research, the data may exist, at first, in separate files. For the CP study, we have *separate files* for each of the pieces of information we need to use in our analyses:\n\n1.  **Participant attributes**: information about participants' age, gender, identifier code, and abilities on various measures.\n2.  **Stimulus attributes**: information about stimulus items, e.g., the word, its item number, its value on each variable in a set of psycholinguistic properties (like word length, frequency).\n3.  **Behaviour**: behavioural observations e.g. reaction time or accuracy of responses made by each participant to each stimulus word.\n\nOften, we need all these kinds of information for our analyses but different pieces of information are produced in separate ways and come to us in separate files. For example, we may collect experimental response data using software like PsychoPy, E-Prime, Qualtrics or DMDX. We may collect information about participant characteristics using standardized measures, or by asking participants to complete a set of questions on their age, gender, and so on.\n\n### The data we need to use are untidy {#sec-intro-mixed-data-untidy-2}\n\nOften, the files we get are **untidy**: not in a useful or *tidy* format. For example, if you open the file `CP_study_word_naming_rt_180211.dat` (a `.dat` or tab delimited file) in Excel, you will see a spreadsheet that looks like @fig-CP-study-RTs.\n\n![CP study RTs .dat file](CP_study_word_naming_rt_180211-excel-shot.png){#fig-CP-study-RTs}\n\nTypical of the output from data collection software, we can see a data table with:\n\n1.  in the top row, column header labels `item_name, AislingoC, AllanaD ...`;\n2.  in the first (leftmost) column, row labels `item_name, act, ask, both ...`;\n3.  for each row, we see values equal to the reaction time (RT) observed for the response made to each stimulus (listed in the row labels);\n4.  for each column, we see values equal to the RTs observed for each person (listed in the column labels);\n5.  and at each intersection of row and column (for each cell), we see the RT observed for a response made by a participant to a stimulus.\n\nData laid out like this are sometimes said to be in *wide* format. You can see that the data are *wide* because at least one variable -- here, reading reaction time -- is held not in one column but spread out over several columns, side-by-side. Thus, the dataset is wide with fewer rows and many columns.\n\nWe want the data in what is called the *tidy* format.\n\n#### How tidy data are tidy {#sec-intro-mixed-data-tidy}\n\nThere are three inter-related rules which make data *tidy* [@grolemund].\n\n::: callout-important\nIn tidy data:\n\n1.  Each variable must have its own column.\n2.  Each observation must have its own row.\n3.  Each value must have its own cell.\n:::\n\nYou can read more about tidy data [here](http://r4ds.had.co.nz/tidy-data.html).\n\nFor our purposes, the reason we want the data in *tidy* format is that it is required for the functions we are going to use for mixed-effects modelling. However, in general, *tidy* format is maximally flexible, and convenient, for use with different R functions.\n\n## Tidy the data {#sec-intro-mixed-tidy-data}\n\nTo answer our research question, we will need to combine the behavioural data with information about the participants (age, gender ...) and about the words (word, frequency ...) We will need to ensure that the dataset we construct will be in *tidy* format. We will need to *select* variables (columns) to get just those required for our later analyses. And we will need to *filter* cases (rows), excluding errors or outliers.\n\nWe shall need to do this work in a series of processing steps:\n\n1.  Import the data or read the data into R, see @sec-intro-mixed-data-import\n2.  Restructure the data, see @sec-intro-mixed-data-restructure\n3.  Select or transform variables, see @sec-intro-mixed-data-transform\n4.  Filter observations, see @sec-intro-mixed-data-filter\n\nWe will use `{tidyverse}` library functions from the beginning, starting with the import stage.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n(Every step can also be done in alternative processing steps with the same result using *base R* code.)\n\n### Read in the data files by using the read_csv() and read_tsv() functions {#sec-intro-mixed-data-import}\n\nI am going to assume you have downloaded the data files, that they are all in the same folder, and that you know where they are on your computer or server. We need to use different versions of the `read_` function to read all four files into R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbehaviour.rt <- read_tsv(\"data/CP study word naming rt 180211.dat\", na = \"-999\")\nbehaviour.acc <- read_tsv(\"data/CP study word naming acc 180211.dat\", na = \"-999\")\nsubjects <- read_csv(\"data/all.subjects 110614-050316-290518.csv\", na = \"-999\")\nwords <- read_csv(\"data/words.items.5 120714 150916.csv\", na = \"-999\")\n```\n:::\n\n\nThese different versions respect the different ways in which the `.dat` and `.csv` file formats work. We need `read_tsv()` when data files consist of tab separated values. We need `read_csv()` when data files consist of comma separated values.\n\nYou can read more about the `{tidyverse}` `{readr}` library of helpful functions [here](https://readr.tidyverse.org/)\n\nIt is *very* common to get experimental data in all sorts of different formats. Learning to use **tidyverse** functions will make it easier to cope with this when you do research.\n\n::: callout-tip\nWe use the `read_` function to read in the data\n\n- entering (here, at least) arguments -- inside the brackets after the function name -- to tell R what file we need and how missing values (`NAs`) are coded.\n:::\n\nIt will help your understanding to examine an example. Take a look at what this line of code includes, element by element.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbehaviour.rt <- read_tsv(\"data/CP study word naming rt 180211.dat\", na = \"-999\")\n```\n:::\n\n\n1.  We write `behaviour.rt <- read_tsv(...)` to create an object in the R environment, which we call `behaviour.rt`: the object with this name is the dataset we read into R using `read_tsv(...)`.\n2.  When we write the function `read_tsv(...)` we include two arguments inside it.\n3.  `read_tsv(\"CP study word naming rt 180211.dat\", ...` first, the name of the file, given in quotes `\"\"` and then a comma.\n4.  `read_tsv(..., na = \"-999\")` second, we tell R that there are some missing values `na` which are coded with the value `\"-999\"`.\n\n### A quick lesson about missing value codes {#sec-intro-missing}\n\nIn the datasets -- typically, the spreadsheets -- we create in our research, we will have values missing for different reasons. Take another look at the data spreadsheet you saw earlier, @fig-CP-study-RTs.\n\n::: callout-tip\nIn R, a missing value is said to be \"not available\": `NA`.\n:::\n\nYou should be able to see that the spreadsheet holds information, as explained, about the RTs of the responses made by each child to each stimulus word. Each of the cells in the spreadsheet (i.e. the box where a column intersects with a row) includes a number value. Most of the values are positive numbers like `751.3`: the reaction time of a response, recorded in milliseconds. The values have to be positive because they represent the length of time between the moment the stimulus word is presented on the test computer screen and the moment the child's spoken word response has begun to be registered by the computer microphone and sound recording software.\n\nSome of the cells hold the value `-999`, however. Obviously, we cannot have negative RT. The value represents the fact that we have no data. Take a look at @fig-CP-study-RTs: we have a `-999` where we should have a RT for the response made by participant `AllanaD` to the word `broad`. This `-999` is there because, for some reason, we did not record an RT or a response for that combination of participant and stimulus.\n\nWe can choose any value we like, as researchers, to code for missing data like this. Some researchers choose not to code for the absence of a response recording or leave the cell in a spreadsheet blank or empty where data are missing. This is **bad practice** though it is common.\n\nThere are a number of reasons why it is bad practice to just leave a cell empty when it is empty because no observation is to be recorded.\n\n1.  Data may be missing for different reasons: maybe a child did not make any response to a stimulus (often called a \"null response\"); or maybe a child made a response but there was a microphone or other technical fault; or maybe a child made a response but it was an error and (here) the corresponding performance measure (RT) cannot be counted.\n2.  If you do not code for missingness in the data then the software you use will do it for you, but you may not know how it does so, or where.\n3.  If you have missing data, you ought to be able to identify where the data are missing.\n\nI use `-999` to code for missing values because you should never see a value like that in real reading RT data. You can use whatever value you like but you should make sure you *do* code for missing data somehow.\n\n### Reshape the data from wide to long using the pivot_longer() function {#sec-intro-mixed-data-restructure}\n\nWe are going to need to restructure these data from a wide format to a longer format. We need to restructure both behavioural datasets, accuracy and RT. We do this using the `pivot_longer()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt.long <- behaviour.rt %>%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n\nacc.long <- behaviour.acc %>%\n              pivot_longer(2:62, names_to = \"subjectID\", values_to = \"accuracy\")\n```\n:::\n\n\nResearchers used to have to do this sort of thing by hand, using copying and pasting, in Excel or SPSS. Doing the process by hand takes many hours or days. And you *always* make errors.\n\n::: callout-tip\nDoing dataset construction programmatically, using R functions, is generally faster and more reliable than doing it by hand.\n:::\n\nHere, we use a function you may have seen before: `pivot_longer()`. It will help your understanding to examine the code carefully.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt.long <- behaviour.rt %>%\n             pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")\n```\n:::\n\n\nThe name of the function comes from the fact that we are starting with data in wide format e.g. `behaviour.rt` where we have what should be a single variable of observations (RTs) arranged in a wide series of multiple columns, side-by-side (one column for each participant). But we want to take those wide data and *lengthen* the dataset, increasing the number of rows and decreasing the number of columns.\n\nLet's look at this line of code bit by bit. It includes a powerful function that accomplishes a lot of tasks, so it is worth explaining this function in some detail.\n\n1.  `rt.long <- behaviour.rt %>%`\n\n-   At the start, I tell R that I am going to create a new longer dataset (more rows, fewer columns) that I shall call `rt.long`.\n-   I will create this longer dataset from `<-` the original wide dataset `behaviour.rt`.\n-   and I will create the new longer dataset by taking the original wide dataset and piping it `%>%` to the pivot function coded on the next line:\n\n2.  `pivot_longer(2:62, names_to = \"subjectID\", values_to = \"RT\")`\n\n-   On this next line, I tell R how to do the pivoting by using three arguments.\n\na.  `pivot_longer(2:62...)`\n\n-   First, I tell R that I want to re-arrange all the columns that can be found in the dataset from the second column to the sixty-second column.\n-   In a spreadsheet, we have a number of columns.\n-   Columns can be identified by their position in the spreadsheet.\n-   The position of a column in a spreadsheet can be identified by number, from the leftmost column (column number 1) to the rightmost column (here, column number 62) in our dataset.\n-   So this argument tells R exactly which columns I want to pivot.\n\nb.  `pivot_longer(..., names_to = \"subjectID\", ...)`\n\n-   Second, I tell R that I want it to take the column labels and put them into a new column, called `subjectID`.\n-   In the wide dataset `behaviour.rt`, each column holds a list of numbers (RTs) but begins with a word in the topmost cell, the name code for a participant, in the column label position.\n-   We want to keep the information about which participant produces which response when we pivot the wide data to a longer structure.\n-   We do this by asking R to take the column labels (the participant names) and listing them in a new column, called `subjectID` which now holds the names as participant ID codes.\n\nc.  `pivot_longer(...values_to = \"RT\")`\n\n-   Third, we tell R that all the RT values should be put in a single column.\n-   We can understand that this new column `RT` will hold RT observations in a vertical stack, one cell for each response by a person to a word, with rows ordered by `subjectID`.\n\nThere are 61 columns of data listed by participant though 62 children were tested because we lost one child's data through an administrative error. As a result, in the wide data sets there are 62 columns, with the first column holding `item_name` data.\n\nYou can find more information about pivoting data [here](https://tidyr.tidyverse.org/articles/pivot.html)\n\nAnd you can find more information specifically about the `pivot_longer()` operation [here](https://tidyr.tidyverse.org/articles/pivot.html)\n\n### Why we restructure the data {#sec-intro-mixed-data-restructure-why}\n\nAs I noted, one problem with the wide format is that the data are structured so that the column names are not names of variables. In our example wide format dataset, `behaviour.rt`, the columns are headed by a participant identity code or name but a participant code is not the name of a variable, it is a value of the variable I call `subjectID`.\n\nIn the design of the CP reading study, we want to take into account the impact of differences between participants on response RT (so, we need to identify which participant makes which response). But we do not see the responses made by a participant as a predictor variable.\n\nA second problem is that, in a wide format file like `behaviour.rt`, information about the responses made to each stimulus word is all on the same row (that seems good) but in different columns. Each person responded to all the words. But the response made to a word e.g. `act` made by one participant is in a different column (e.g., 594.8ms, for `AislingoC`) from the response made to the same word by a different participant (e.g., 586ms, for `AlexB`). This means that information about the responses made to each stimulus word are spread out as values across multiple columns.\n\nYou can see this for yourself if you inspect the source rt data using `head()` to view the top four rows of the dataset.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> item_name </th>\n   <th style=\"text-align:right;\"> AislingoC </th>\n   <th style=\"text-align:right;\"> AlexB </th>\n   <th style=\"text-align:right;\"> AllanaD </th>\n   <th style=\"text-align:right;\"> AmyR </th>\n   <th style=\"text-align:right;\"> AndyD </th>\n   <th style=\"text-align:right;\"> AnnaF </th>\n   <th style=\"text-align:right;\"> AoifeH </th>\n   <th style=\"text-align:right;\"> ChloeBergin </th>\n   <th style=\"text-align:right;\"> ChloeF </th>\n   <th style=\"text-align:right;\"> ChloeS </th>\n   <th style=\"text-align:right;\"> CianR </th>\n   <th style=\"text-align:right;\"> ConorF </th>\n   <th style=\"text-align:right;\"> DavidL </th>\n   <th style=\"text-align:right;\"> DillonF </th>\n   <th style=\"text-align:right;\"> DJHerlihy </th>\n   <th style=\"text-align:right;\"> EamonD </th>\n   <th style=\"text-align:right;\"> EimearK </th>\n   <th style=\"text-align:right;\"> EllenH </th>\n   <th style=\"text-align:right;\"> EoinL </th>\n   <th style=\"text-align:right;\"> GrainneH </th>\n   <th style=\"text-align:right;\"> JackBr </th>\n   <th style=\"text-align:right;\"> JackK </th>\n   <th style=\"text-align:right;\"> JackS </th>\n   <th style=\"text-align:right;\"> JamesoC </th>\n   <th style=\"text-align:right;\"> JenniferoS </th>\n   <th style=\"text-align:right;\"> KateF </th>\n   <th style=\"text-align:right;\"> KayleighMc </th>\n   <th style=\"text-align:right;\"> KenW </th>\n   <th style=\"text-align:right;\"> KevinL </th>\n   <th style=\"text-align:right;\"> KieranF </th>\n   <th style=\"text-align:right;\"> KillianB </th>\n   <th style=\"text-align:right;\"> KirstyC </th>\n   <th style=\"text-align:right;\"> LeeJ </th>\n   <th style=\"text-align:right;\"> MarkC </th>\n   <th style=\"text-align:right;\"> MatthewC </th>\n   <th style=\"text-align:right;\"> MeganoB </th>\n   <th style=\"text-align:right;\"> MichaelaoD </th>\n   <th style=\"text-align:right;\"> NataliaR </th>\n   <th style=\"text-align:right;\"> NiallG </th>\n   <th style=\"text-align:right;\"> NiallGavin </th>\n   <th style=\"text-align:right;\"> NiallW </th>\n   <th style=\"text-align:right;\"> OisinN </th>\n   <th style=\"text-align:right;\"> OlaA </th>\n   <th style=\"text-align:right;\"> OwenD </th>\n   <th style=\"text-align:right;\"> PalomaM </th>\n   <th style=\"text-align:right;\"> PauricT </th>\n   <th style=\"text-align:right;\"> PerryD </th>\n   <th style=\"text-align:right;\"> RachelD </th>\n   <th style=\"text-align:right;\"> RebeccaGr </th>\n   <th style=\"text-align:right;\"> RebeccaM </th>\n   <th style=\"text-align:right;\"> RebeccaR </th>\n   <th style=\"text-align:right;\"> RoisinF </th>\n   <th style=\"text-align:right;\"> RonanT </th>\n   <th style=\"text-align:right;\"> SarahP </th>\n   <th style=\"text-align:right;\"> ShaunaBr </th>\n   <th style=\"text-align:right;\"> SiobhanR </th>\n   <th style=\"text-align:right;\"> TaraB </th>\n   <th style=\"text-align:right;\"> TeeTeeOj </th>\n   <th style=\"text-align:right;\"> ThomasK </th>\n   <th style=\"text-align:right;\"> TristianT </th>\n   <th style=\"text-align:right;\"> Zainab </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> act </td>\n   <td style=\"text-align:right;\"> 594.8 </td>\n   <td style=\"text-align:right;\"> 586.0 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 693.0 </td>\n   <td style=\"text-align:right;\"> 597 </td>\n   <td style=\"text-align:right;\"> 627.0 </td>\n   <td style=\"text-align:right;\"> 649.0 </td>\n   <td style=\"text-align:right;\"> 1081.0 </td>\n   <td style=\"text-align:right;\"> 642.0 </td>\n   <td style=\"text-align:right;\"> 622.7 </td>\n   <td style=\"text-align:right;\"> 701.0 </td>\n   <td style=\"text-align:right;\"> 686.0 </td>\n   <td style=\"text-align:right;\"> 951.0 </td>\n   <td style=\"text-align:right;\"> 661.0 </td>\n   <td style=\"text-align:right;\"> 692.0 </td>\n   <td style=\"text-align:right;\"> 670.0 </td>\n   <td style=\"text-align:right;\"> 502.4 </td>\n   <td style=\"text-align:right;\"> 578.4 </td>\n   <td style=\"text-align:right;\"> 651.8 </td>\n   <td style=\"text-align:right;\"> 441.6 </td>\n   <td style=\"text-align:right;\"> 895.2 </td>\n   <td style=\"text-align:right;\"> 529.0 </td>\n   <td style=\"text-align:right;\"> 639.0 </td>\n   <td style=\"text-align:right;\"> 809 </td>\n   <td style=\"text-align:right;\"> 676.9 </td>\n   <td style=\"text-align:right;\"> 568.8 </td>\n   <td style=\"text-align:right;\"> 586.1 </td>\n   <td style=\"text-align:right;\"> 591.3 </td>\n   <td style=\"text-align:right;\"> 587 </td>\n   <td style=\"text-align:right;\"> 586 </td>\n   <td style=\"text-align:right;\"> 723.9 </td>\n   <td style=\"text-align:right;\"> 1428.0 </td>\n   <td style=\"text-align:right;\"> 557.0 </td>\n   <td style=\"text-align:right;\"> 639.4 </td>\n   <td style=\"text-align:right;\"> 676 </td>\n   <td style=\"text-align:right;\"> 714.8 </td>\n   <td style=\"text-align:right;\"> 623.3 </td>\n   <td style=\"text-align:right;\"> 615.0 </td>\n   <td style=\"text-align:right;\"> 796.0 </td>\n   <td style=\"text-align:right;\"> 568.4 </td>\n   <td style=\"text-align:right;\"> 800.9 </td>\n   <td style=\"text-align:right;\"> 595 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 574.9 </td>\n   <td style=\"text-align:right;\"> 628 </td>\n   <td style=\"text-align:right;\"> 797.0 </td>\n   <td style=\"text-align:right;\"> 652 </td>\n   <td style=\"text-align:right;\"> 757.0 </td>\n   <td style=\"text-align:right;\"> 631.0 </td>\n   <td style=\"text-align:right;\"> 520.5 </td>\n   <td style=\"text-align:right;\"> 640.0 </td>\n   <td style=\"text-align:right;\"> 733.0 </td>\n   <td style=\"text-align:right;\"> 566 </td>\n   <td style=\"text-align:right;\"> 758.0 </td>\n   <td style=\"text-align:right;\"> 670.0 </td>\n   <td style=\"text-align:right;\"> 532.4 </td>\n   <td style=\"text-align:right;\"> 615.7 </td>\n   <td style=\"text-align:right;\"> 540.0 </td>\n   <td style=\"text-align:right;\"> 1390.0 </td>\n   <td style=\"text-align:right;\"> 747 </td>\n   <td style=\"text-align:right;\"> 651.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ask </td>\n   <td style=\"text-align:right;\"> 481.5 </td>\n   <td style=\"text-align:right;\"> 864.0 </td>\n   <td style=\"text-align:right;\"> 1163.0 </td>\n   <td style=\"text-align:right;\"> 694.4 </td>\n   <td style=\"text-align:right;\"> 616 </td>\n   <td style=\"text-align:right;\"> 631.0 </td>\n   <td style=\"text-align:right;\"> 538.0 </td>\n   <td style=\"text-align:right;\"> 799.3 </td>\n   <td style=\"text-align:right;\"> 603.0 </td>\n   <td style=\"text-align:right;\"> 526.0 </td>\n   <td style=\"text-align:right;\"> 591.5 </td>\n   <td style=\"text-align:right;\"> 699.6 </td>\n   <td style=\"text-align:right;\"> 827.2 </td>\n   <td style=\"text-align:right;\"> 635.0 </td>\n   <td style=\"text-align:right;\"> 654.0 </td>\n   <td style=\"text-align:right;\"> 508.0 </td>\n   <td style=\"text-align:right;\"> 564.0 </td>\n   <td style=\"text-align:right;\"> 822.0 </td>\n   <td style=\"text-align:right;\"> 479.7 </td>\n   <td style=\"text-align:right;\"> 600.0 </td>\n   <td style=\"text-align:right;\"> 617.6 </td>\n   <td style=\"text-align:right;\"> 555.9 </td>\n   <td style=\"text-align:right;\"> 654.6 </td>\n   <td style=\"text-align:right;\"> 765 </td>\n   <td style=\"text-align:right;\"> 856.0 </td>\n   <td style=\"text-align:right;\"> 576.7 </td>\n   <td style=\"text-align:right;\"> 690.3 </td>\n   <td style=\"text-align:right;\"> 501.9 </td>\n   <td style=\"text-align:right;\"> 634 </td>\n   <td style=\"text-align:right;\"> 626 </td>\n   <td style=\"text-align:right;\"> 523.3 </td>\n   <td style=\"text-align:right;\"> 640.7 </td>\n   <td style=\"text-align:right;\"> 516.0 </td>\n   <td style=\"text-align:right;\"> 625.7 </td>\n   <td style=\"text-align:right;\"> 561 </td>\n   <td style=\"text-align:right;\"> 698.2 </td>\n   <td style=\"text-align:right;\"> 685.0 </td>\n   <td style=\"text-align:right;\"> 606.0 </td>\n   <td style=\"text-align:right;\"> 793.0 </td>\n   <td style=\"text-align:right;\"> 551.9 </td>\n   <td style=\"text-align:right;\"> 668.7 </td>\n   <td style=\"text-align:right;\"> 722 </td>\n   <td style=\"text-align:right;\"> 868.0 </td>\n   <td style=\"text-align:right;\"> 633.0 </td>\n   <td style=\"text-align:right;\"> 578 </td>\n   <td style=\"text-align:right;\"> 660.2 </td>\n   <td style=\"text-align:right;\"> 851 </td>\n   <td style=\"text-align:right;\"> 640.3 </td>\n   <td style=\"text-align:right;\"> 630.0 </td>\n   <td style=\"text-align:right;\"> 535.0 </td>\n   <td style=\"text-align:right;\"> 568.0 </td>\n   <td style=\"text-align:right;\"> 579.7 </td>\n   <td style=\"text-align:right;\"> 562 </td>\n   <td style=\"text-align:right;\"> 747.0 </td>\n   <td style=\"text-align:right;\"> 602.4 </td>\n   <td style=\"text-align:right;\"> 558.4 </td>\n   <td style=\"text-align:right;\"> 691.0 </td>\n   <td style=\"text-align:right;\"> 580.8 </td>\n   <td style=\"text-align:right;\"> 590.9 </td>\n   <td style=\"text-align:right;\"> 795 </td>\n   <td style=\"text-align:right;\"> 740.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> both </td>\n   <td style=\"text-align:right;\"> 457.5 </td>\n   <td style=\"text-align:right;\"> 670.0 </td>\n   <td style=\"text-align:right;\"> 1114.3 </td>\n   <td style=\"text-align:right;\"> 980.0 </td>\n   <td style=\"text-align:right;\"> 1019 </td>\n   <td style=\"text-align:right;\"> 796.1 </td>\n   <td style=\"text-align:right;\"> 545.2 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 581.0 </td>\n   <td style=\"text-align:right;\"> 568.4 </td>\n   <td style=\"text-align:right;\"> 665.0 </td>\n   <td style=\"text-align:right;\"> 751.0 </td>\n   <td style=\"text-align:right;\"> 917.0 </td>\n   <td style=\"text-align:right;\"> 808.1 </td>\n   <td style=\"text-align:right;\"> 737.6 </td>\n   <td style=\"text-align:right;\"> 597.0 </td>\n   <td style=\"text-align:right;\"> 475.0 </td>\n   <td style=\"text-align:right;\"> 608.0 </td>\n   <td style=\"text-align:right;\"> 699.2 </td>\n   <td style=\"text-align:right;\"> 600.6 </td>\n   <td style=\"text-align:right;\"> 527.3 </td>\n   <td style=\"text-align:right;\"> 601.0 </td>\n   <td style=\"text-align:right;\"> 982.1 </td>\n   <td style=\"text-align:right;\"> 917 </td>\n   <td style=\"text-align:right;\"> 854.4 </td>\n   <td style=\"text-align:right;\"> 571.6 </td>\n   <td style=\"text-align:right;\"> 825.3 </td>\n   <td style=\"text-align:right;\"> 584.0 </td>\n   <td style=\"text-align:right;\"> 720 </td>\n   <td style=\"text-align:right;\"> 571 </td>\n   <td style=\"text-align:right;\"> 624.0 </td>\n   <td style=\"text-align:right;\"> 853.4 </td>\n   <td style=\"text-align:right;\"> NA </td>\n   <td style=\"text-align:right;\"> 703.0 </td>\n   <td style=\"text-align:right;\"> 781 </td>\n   <td style=\"text-align:right;\"> 1065.8 </td>\n   <td style=\"text-align:right;\"> 591.5 </td>\n   <td style=\"text-align:right;\"> 559.2 </td>\n   <td style=\"text-align:right;\"> 837.6 </td>\n   <td style=\"text-align:right;\"> 612.0 </td>\n   <td style=\"text-align:right;\"> 743.4 </td>\n   <td style=\"text-align:right;\"> 743 </td>\n   <td style=\"text-align:right;\"> 919.0 </td>\n   <td style=\"text-align:right;\"> 883.0 </td>\n   <td style=\"text-align:right;\"> 616 </td>\n   <td style=\"text-align:right;\"> 734.6 </td>\n   <td style=\"text-align:right;\"> 658 </td>\n   <td style=\"text-align:right;\"> 574.4 </td>\n   <td style=\"text-align:right;\"> 634.9 </td>\n   <td style=\"text-align:right;\"> 559.7 </td>\n   <td style=\"text-align:right;\"> 689.0 </td>\n   <td style=\"text-align:right;\"> 997.8 </td>\n   <td style=\"text-align:right;\"> 648 </td>\n   <td style=\"text-align:right;\"> 753.4 </td>\n   <td style=\"text-align:right;\"> 548.0 </td>\n   <td style=\"text-align:right;\"> 530.5 </td>\n   <td style=\"text-align:right;\"> 625.4 </td>\n   <td style=\"text-align:right;\"> 552.0 </td>\n   <td style=\"text-align:right;\"> 985.0 </td>\n   <td style=\"text-align:right;\"> 640 </td>\n   <td style=\"text-align:right;\"> 764.0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> box </td>\n   <td style=\"text-align:right;\"> 546.0 </td>\n   <td style=\"text-align:right;\"> 748.6 </td>\n   <td style=\"text-align:right;\"> 975.0 </td>\n   <td style=\"text-align:right;\"> 678.0 </td>\n   <td style=\"text-align:right;\"> 589 </td>\n   <td style=\"text-align:right;\"> 604.0 </td>\n   <td style=\"text-align:right;\"> 574.0 </td>\n   <td style=\"text-align:right;\"> 658.0 </td>\n   <td style=\"text-align:right;\"> 688.7 </td>\n   <td style=\"text-align:right;\"> 492.0 </td>\n   <td style=\"text-align:right;\"> 641.0 </td>\n   <td style=\"text-align:right;\"> 699.0 </td>\n   <td style=\"text-align:right;\"> 824.0 </td>\n   <td style=\"text-align:right;\"> 731.6 </td>\n   <td style=\"text-align:right;\"> 634.0 </td>\n   <td style=\"text-align:right;\"> 557.5 </td>\n   <td style=\"text-align:right;\"> 520.1 </td>\n   <td style=\"text-align:right;\"> 581.0 </td>\n   <td style=\"text-align:right;\"> 1191.0 </td>\n   <td style=\"text-align:right;\"> 612.0 </td>\n   <td style=\"text-align:right;\"> 540.0 </td>\n   <td style=\"text-align:right;\"> 540.0 </td>\n   <td style=\"text-align:right;\"> 810.0 </td>\n   <td style=\"text-align:right;\"> 885 </td>\n   <td style=\"text-align:right;\"> 573.8 </td>\n   <td style=\"text-align:right;\"> 614.9 </td>\n   <td style=\"text-align:right;\"> 722.0 </td>\n   <td style=\"text-align:right;\"> 594.6 </td>\n   <td style=\"text-align:right;\"> 632 </td>\n   <td style=\"text-align:right;\"> 549 </td>\n   <td style=\"text-align:right;\"> 564.0 </td>\n   <td style=\"text-align:right;\"> 632.6 </td>\n   <td style=\"text-align:right;\"> 668.6 </td>\n   <td style=\"text-align:right;\"> 627.0 </td>\n   <td style=\"text-align:right;\"> 577 </td>\n   <td style=\"text-align:right;\"> 785.9 </td>\n   <td style=\"text-align:right;\"> 649.0 </td>\n   <td style=\"text-align:right;\"> 616.0 </td>\n   <td style=\"text-align:right;\"> 928.0 </td>\n   <td style=\"text-align:right;\"> 608.7 </td>\n   <td style=\"text-align:right;\"> 690.0 </td>\n   <td style=\"text-align:right;\"> 857 </td>\n   <td style=\"text-align:right;\"> 886.6 </td>\n   <td style=\"text-align:right;\"> 544.0 </td>\n   <td style=\"text-align:right;\"> 863 </td>\n   <td style=\"text-align:right;\"> 726.0 </td>\n   <td style=\"text-align:right;\"> 812 </td>\n   <td style=\"text-align:right;\"> 640.5 </td>\n   <td style=\"text-align:right;\"> 613.8 </td>\n   <td style=\"text-align:right;\"> 395.1 </td>\n   <td style=\"text-align:right;\"> 656.7 </td>\n   <td style=\"text-align:right;\"> 604.7 </td>\n   <td style=\"text-align:right;\"> 530 </td>\n   <td style=\"text-align:right;\"> 667.4 </td>\n   <td style=\"text-align:right;\"> 547.7 </td>\n   <td style=\"text-align:right;\"> 570.0 </td>\n   <td style=\"text-align:right;\"> 585.6 </td>\n   <td style=\"text-align:right;\"> 532.0 </td>\n   <td style=\"text-align:right;\"> 634.0 </td>\n   <td style=\"text-align:right;\"> 764 </td>\n   <td style=\"text-align:right;\"> 912.0 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThis structure is a problem for visualization and for analysis because the functions we will use require us to specify *single* columns for an outcome variable like reaction time.\n\nWe are looking at the process of tidying data because untidiness is very common. Learning how to deal with it will save you a lot of time and grief later.\n\nYou should check for yourself how `subjectID` and `RT` or `accuracy` scores get transposed from the old wide structure to the new long structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# RT data\nhead(behaviour.rt)\nhead(rt.long)\n\n# accuracy data\nhead(behaviour.acc)\nhead(acc.long)\n```\n:::\n\n\nIf you compare the `rt.long` or `acc.long` data with the data in the original wide format then you can see how -- in going from wide -- we have re-arranged the data to a longer and narrower set of columns:\n\n-   one column listing each word;\n-   one column for `subjectID`;\n-   and one column for `RT` or `accuracy`.\n\nWhat a check will show you is that we have multiple rows for responses to each item so that the item is repeated multiple times in different rows.\n\nThese data are *now tidy*.\n\n-   Each column has information about one variable\n-   And each row has information about one observation, here, the response made by a participant to a word\n\nThis is a big part of data tidying now done. However, these data are *incomplete*. Next we shall combine behavioural observations with data about stimulus words and about participants.\n\n### The tidyverse evolves {#sec-intro-mixed-tidyverse-evolves}\n\nOver the years, different ways of reshaping data have evolved. This reflects how important and common the task is. An older way to do the same operation uses the function `gather()`.\n\nYou can read more about `gather()` [here](http://r4ds.had.co.nz/tidy-data.html#spreading-and-gathering)\n\nIn `{tidyverse}` the functions designed to enable you to restructure data have evolved through a series of different forms. This change is one of the real benefits of using open software like R. In my experience, the newer functions can be useful for *really* untidy data. I expect things will continue to evolve and improve over time.\n\n### Merging data from different datasets using \\_join() functions {#sec-intro-mixed-data-join}\n\nTo answer our research question, we next need to combine the **RT** with the **accuracy** data, and then the combined behavioural data with **participant** information and **stimulus** information. This is because, as we have seen, information about behavioural responses, about participant attributes or stimulus word properties, are located in separate files.\n\nMany researchers have completed this kind of operation by hand. This involves copying and pasting bits of data in a spreadsheet. It can take hours or days. I know because I have done it, and I have seen others do it.\n\n::: callout-warning\nPlease do not try to combine datasets through manual operations e.g. in Excel. I guarantee that:\n\n1.  You will make mistakes.\n2.  You will not know when or where those mistakes are in your data.\n\nThere are better ways to spend your time.\n:::\n\nWe can combine the datasets, in the way that we need, using the `{tidyverse}` `full_join()` function. This gets the job done quickly, and accurately.\n\nFirst, we join RT and accuracy data together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong <- rt.long %>% \n          full_join(acc.long)\n```\n:::\n\n\nThen, we join subject and item information to the behavioural data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong.subjects <- long %>% \n                   full_join(subjects, by = \"subjectID\")\n\nlong.all <- long.subjects %>%\n              full_join(words, by = \"item_name\")\n```\n:::\n\n\nNotice, we can let R figure out how to join the pieces of data together. If we were doing this by hand then we would need to check *very carefully* the correspondences between observations in different datasets.\n\nHere, in a series of steps, we take one dataset and join it (merge it) with the second dataset. Let's look at an example element by element to better understand how this is accomplished.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong <- rt.long %>% \n           full_join(acc.long)\n```\n:::\n\n\nThe code work as follows.\n\n1.  `long <- rt.long %>%`\n\n-   We create a new dataset we call `long`.\n-   We do this by taking one original dataset `rt.long` and `%>%` piping it to the operation defined in the second step.\n\n2.  `full_join(acc.long)`\n\n-   In this second step, we use the function `full_join()` to add observations from a second original dataset `acc.long` to those already from `rt.long`\n\nThe addition of observations from one database joining to those from another happens through a matching process.\n\n-   R looks at the datasets being merged.\n-   It identifies if the two datasets have columns in common. Here, the datasets have `subjectID` and `item_name` in common).\n-   R can use these common columns to identify rows of data. Here, each row of data will be identified by both `subjectID` and `item_name` i.e. as data about the response made by a participant to a word.\n-   R will then do a series of identity checks, comparing one dataset with the other and, row by row, looking for matching values in the columns that are common to both datasets.\n-   If there is a match then R joins the corresponding rows of data together.\n-   If there isn't a match then it creates `NAs` where there are missing entries in one row for one dataset which cannot be matched to a row from the joining dataset.\n\n::: callout-tip\nNote that in one example, the example of code I discuss here, I did not specify identifying columns in common, allowing the function to do the work. In the other code chunks I did: `long.all <- long.subjects %>% full_join(words, by = \"item_name\")` using the `by = ...` argument.\n\n-   Sometimes, you can vary in how you employ a `_join()` function.\n-   It may help to specify the identifying column if you want to make explicit (to yourselves and others) how the process is to be completed.\n:::\n\n#### Relational data {#sec-intro-mixed-data-relational}\n\nIn the `{tidyverse}` family of `dplyr` functions, when you work with multiple datasets (tables of data), we call the datasets [relational data](http://r4ds.had.co.nz/relational-data.html#relational-data).\n\nThere are three families of functions (like verbs) designed to work with relational data:\n\n-   Mutating joins, which add new variables to one data frame from matching observations in another.\n-   Filtering joins, which filter observations from one data frame based on whether or not they match an observation in the other table.\n-   Set operations, which treat observations as if they were set elements.\n\nWe can connect datasets -- relate them -- according to shared variables like `subjectID, item_name` (for our data). In `{tidyverse}`, the variables that connect pairs of tables are called keys where, and this is what counts, *key(-s) are variable(-s) that uniquely identify an observation*.\n\nFor the experimental reading data, we have observations about each response made by a participant (one of 61 subjects) to an item (one of 160 words). For these data, we can match up a pair of RT and accuracy observations for each (unique) `subjectID-item_name` combination.\nIf you reflect, we could not combine the RT and accuracy data correctly if we did not have both identifying variables in both datasets, because both column variables are required to uniquely identify each observation.\n\nFurther, we could not combine the RT and accuracy data correctly if there were mismatches in values of the identifying variable.\nSometimes, I have done this operation and it has gone wrong because a subjectID has been spelled one way in one dataset e.g. `hugh` and another way in the other dataset e.g. `HughH`. This leads me to share some advice.\n\n::: callout-tip\n- Be careful about spelling identifiers.\n- Always check your work after merger operations. \n\nYou can check your work by calculating dataset lengths to ensure the number of rows in the new dataset matches your expectations, given the study design and data collection procedure.\n:::\n\n#### \\_join functions {#sec-intro-mixed-data-join-functions}\n\nWe used the `full_join()` function.\n\nThere are three kinds of joins.\n\n-   A left join keeps all observations in x.\n-   A right join keeps all observations in y.\n-   A full join keeps all observations in x and y.\n\nI used `full_join()` because I wanted to retain all observations from both datasets, whether there was a match (as assumed) or not, in the identifying variables, between observations in each dataset.\n\n#### Exercise {#sec-intro-mixed-data-exx-join}\n\n-   Break the join: You can examine how the `full_join()` works by experimenting with stopping it from working.\n\nAs I discuss, you need to have matches in values on key (common) variables. If the `subjectID` is different on different datasets, you will lose data that would otherwise be merged to form the merged or composite dataset.\n\nCheck what happens if you deliberately misspell one of the `subjectID` values in one of the original source wide behavioural data files.\n\nTo be safe, you might want to do this exercise with copies of the source files kept in a folder you create for this purpose. If it goes wrong, you can always re-access the source files and read them in again.\n\nYou can check what happens before and after you break the match by counting the number of rows in the dataset that results from the merger. We can count the number of rows in a dataset with:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(long.all$RT)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9762\n```\n:::\n:::\n\n\nThis bit of code takes the length of the vector (i.e. variable column `RT` in dataset `long.all`), thus counting the number of rows in the dataset.\n\n### Select or transform the variables {#sec-intro-mixed-data-transform}\n\nOK, now we have all the data about everything all in one big, long and wide, dataset. But we do not actually require all of the dataset for the analyses we are going to do.\n\nWe next need to do two things. First, we need to get rid of variables we will not use: we do that by using `select()`. Then, we need to remove errors and outlying short RT observations: we do that by using `filter()` in Section @sec-intro-mixed-data-filter.\n\nWe are going to select just the variables we need using the `select()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong.all.select <- long.all %>% \n                        select(item_name, subjectID, RT, accuracy, \n                               Lg.UK.CDcount, brookesIMG, AoA_Kup_lem, \n                               Ortho_N, regularity, Length, BG_Mean, \n                               Voice,\tNasal,\tFricative,\tLiquid_SV,\n                               Bilabials,\tLabiodentals,\tAlveolars,\n                               Palatals,\tVelars,\tGlottals, \n                               age.months, TOWREW_skill, TOWRENW_skill, \n                               spoonerisms, CART_score)\n```\n:::\n\n\n::: callout-tip\nNotice that these variables do not have *reader-friendly* names: but **naming things is important**.\n\n-   Check out the ever-useful Jenny Bryan's [advice](https://speakerdeck.com/jennybc/how-to-name-files).\n:::\n\nThe names we have in the CP study data were fine for internal use within my research group but we should be careful to ensure that variables have names that make sense to others and to our future selves. We can adjust variable names using the `rename()` function but I will leave that as an exercise for you to do.\n\n#### Exercise {#sec-intro-mixed-data-exx-transform}\n\n-   Select different variables: You could analyze the CP study data for a research report.\n\nWhat if you wanted to analyze a different set of variables, could you select different variables?\n\n### Filter observations {#sec-intro-mixed-data-filter}\n\nWe now have a tidy dataset `long.all.select` with 26 columns and 9762 rows.\n\nThe dataset includes missing values, designated `NA`. Here, every error (coded `0`, in `accuracy`) corresponds to an `NA` in the `RT` column.\n\nThe dataset also includes outlier data. In this context, $RT < 200$ are probably response errors or equipment failures. We will want to analyse `accuracy` later, so we shall need to be careful about getting rid of `NAs`.\n\nAt this point, I am going to exclude two sets of observations only.\n\n-   observations corresponding to correct response reaction times that are too short: $RT < 200$.\n-   plus observations corresponding to the word *false* which (because of stupid Excel auto-formatting) dropped item attribute data.\n\nWe can do this using the `filter()` function, setting conditions on rows, as arguments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# step 1\nlong.all.select.filter <- long.all.select %>% \n                            filter(item_name != 'FALSE')\n\n# step 2\nlong.all.select.filter <- long.all.select.filter %>%\n                            filter(RT >= 200)\n```\n:::\n\n\nHere, I am using the function `filter()` to ...\n\n-   Create a new dataset `long.all.select.filter <- ...` by\n-   Using functions to work on the data named immediately to the right of the assignment arrow: `long.all.select`\n-   An observation is included in the new dataset if it matches the condition specified as an argument in the `filter()` function call, thus:\n\n1.  `filter(item_name !='FALSE')` means: include in the new dataset `long.all.select.filter` all observations from the old dataset `long.all.select` that are not `!=` (`!` not `=` equal to) the value `FALSE` in the variable `item_name`,\n2.  then recreate the `long.all.select.filter` as a version of itself (with no name change) by including in the new version only those observations where RT was greater than or equal to 200ms using `RT >= 200`.\n\n::: callout-warning\nThe difference between = and ==\n\nYou need to be careful to distinguish these signs.\n\n-   `=` assigns a value, so `x = 2` means \"x equals 2\"\n-   `==` tests a match so `x == 2` means: \"is x equal to 2?\"\n:::\n\n#### Using multiple arguments in filtering {#sec-intro-mixed-data-filter-conditions}\n\nYou can supply multiple arguments to `filter()` and this may be helpful if (1.) you want to filter observations according to a match on condition-A **and** condition-B (logical \"and\" is coded with `&`) or (2.) you want to filter observations according to a match on condition-A or condition-B (logical \"or\" is coded `|`).\n\nYou can read more about using multiple arguments to filter observations [here](https://dplyr.tidyverse.org/reference/filter.html).\n\n#### Exercise {#sec-intro-mixed-data-exx-filter}\n\n-   Vary the filter conditions: in different ways\n\n1.  Change the threshold for including RTs from `RT >= 200` to something else\n2.  Can you assess what impact the change has? Note that you can count the number of observations (rows) in a dataset using e.g. `length(data.set.name$variable.name)`\n\nFiltering or re-coding observations is an important element of the research workflow in psychological science, as I discuss [here](extra_ToBeOrganised/intro.qmd#sec-multiversedata). How we do or do not remove observations from original data may have an impact on our results (as explored by, e.g., Steegen et al., 2014). It is important, therefore, that we learn how to do this reproducibly using R scripts that we can share with our research reports.\n\nYou can read further information about filtering [here](https://r4ds.had.co.nz/transform.html?q=filter#filter-rows-with-filter).\n\n### Remove missing values {#sec-intro-mixed-data-omit-na}\n\nWe will be working with the `long.all.select.filter.csv` dataset collated from the experimental, subject ability scores, and item property data collected for the CP word naming study.\n\nFor convenience, I am going to remove missing values before we go any further, using the `na.omit()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong.all.noNAs <- na.omit(long.all.select.filter)\n```\n:::\n\n\n::: callout-tip\nThe `na.omit()` function is powerful.\n\n-   In using this function, I am asking R to create a new dataset `long.all.noNAs` from the old dataset `long.all.select.filter` in a process in which the new dataset will have *no* rows in which there is a missing value `NA` in *any* column.\n-   You need to be reasonably sure, when you use this function, where your `NAs` may be because, otherwise, you may end the process with a new filtered dataset that has many fewer rows in it than you expected.\n:::\n\n### Now we have some tidy data {#sec-intro-mixed-data-tidy-conclusions}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(long.all.noNAs, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10  26\n   item_n subje    RT accur Lg.UK brook AoA_K Ortho_N regul Length\n   <chr>    <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 act      Aislin  595.       1    4.03       4    6.42       5       1      3\n 2 act      AlexB    586        1    4.03       4    6.42       5       1      3\n 3 act      AmyR     693        1    4.03       4    6.42       5       1      3\n 4 act      AndyD    597        1    4.03       4    6.42       5       1      3\n 5 act      AnnaF    627        1    4.03       4    6.42       5       1      3\n 6 act      AoifeH   649        1    4.03       4    6.42       5       1      3\n 7 act      ChloeB 1081        1    4.03       4    6.42       5       1      3\n 8 act      ChloeF   642        1    4.03       4    6.42       5       1      3\n 9 act      ChloeS   623.       1    4.03       4    6.42       5       1      3\n10 act      CianR    701        1    4.03       4    6.42       5       1      3\n#  with 16 more variables: BG_Mean <dbl>, Voice <dbl>, Nasal <dbl>,\n#   Fricative <dbl>, Liquid_SV <dbl>, Bilabials <dbl>, Labiodentals <dbl>,\n#   Alveolars <dbl>, Palatals <dbl>, Velars <dbl>, Glottals <dbl>,\n#   age.months <dbl>, TOWREW_skill <dbl>, TOWRENW_skill <dbl>,\n#   spoonerisms <dbl>, CART_score <dbl>, and abbreviated variable names\n#   item_name, subjectID, accuracy, Lg.UK.CDcount, brookesIMG,\n#   AoA_Kup_lem, regularity\n```\n:::\n:::\n\n\nIf we inspect `long.all.noNAs`, we can see that we have now got a tidy dataset with all the data we need for our analyses:\n\n-   One observation per row, corresponding to data about a response made by a participant to a stimulus in an experimental trial\n-   One variable per column\n-   We have information about the speed and accuracy of responses\n-   And we have information about the children and about the words.\n\nWe have removed the missing values and we have filtered outliers.\n\n### We can output the data as a .csv file {#sec-intro-mixed-data-output-csv}\n\nHaving produced the tidy dataset, we may wish to share it, or save ourselves the trouble of going through the process again. We can do this by creating a .csv file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(long.all.noNAs, \"long.all.noNAs.csv\")\n```\n:::\n\n\nThis function will create a `.csv` file from the dataset you name `long.all.noNAs` which R will put in your working directory.\n\n### Data tidying -- conclusions {#sec-intro-mixed-data-conclusions}\n\nMost research work involving quantitative evidence requires a *big* chunk of data tidying or processing before you get to the statistics. Most of the time, this is work *you* will have to do. The lessons you can learn about the process will generalize to many future research scenarios.\n\n::: callout-advice\nIt is a mistake to think of data tidying or wrangling as an inconvenience or as an extra task or something you need to do to get to the 'good stuff' (your results).\n\n1.  All analysis results follow from and thus are determined by the data processing steps that precede analysis.\n2.  Analysis results can and do vary, perhaps critically, depending on different processing decisions, and reasonable people may differ on key processing decisions.\n3.  The process of data tidying is frequently instructive of your data recording quality: you find things out about your field measurements or your instrument integrity or quality of your recordings, when you pay attention, when you process your data.\n:::\n\nIt is wise to see data tidying or processing as a key part of the data analysis workflow because, as I discuss [here](extra_ToBeOrganised/intro.qmd#sec-multiversedata), the choices you make or the integrity or quality of the actions you take, will have consequences for your analysis results or, more generally, for the quality of the evidence you share with others.\n\n[Here](https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt) is a nice substack post that links to some scholarly writing and makes some excellent points.\n\n::: callout-tip\nA key recommendation is that you write code to tidy or process data, thus creating a self-documented *auditable* data tidying process in your analysis workflow.\n\n-   This is simple to do in R and *that* is one important reason why we use and teach R.\n:::\n\n## Repeated measures designs and crossed random effects {#sec-intro-mixed-crossed-random}\n\nOur focus in this chapter is on analyzing data that come from studies with **repeated-measures designs** where the experimenter presents multiple stimuli for response to each participant.\n\nIn our working example, the **CP reading study**, CP asked all participants in her study to read a selection of words. All participants read the same selection of words, and every person read every word. For each participant, we have multiple observations and these (within-participant) observations will not be independent of each other. One participant will tend to be slower or less accurate compared to another participant, on average. Likewise, one participant's responses will reveal a stronger (or weaker) impact of the effect of an experimental variable than another participant. These between-participant differences will tend to be apparent across the sample of participants.\n\nYou could say that the lowest trial-level observations can be grouped with respect to participants, that observations are nested within participant. But the data can also be grouped by stimuli. Remember that in the CP study, all participants read the same selection of words, and every person read every word. This means that for each stimulus word, there are multiple observations because all participants responded to each word, and these (within-item) observations will not be independent of each other. One word may prove to be more challenging compared to another, eliciting slower or less accurate responses, on average. Likewise, participants' responses to a word will reveal a stronger (or weaker) impact of the effect of an experimental variable than the responses to another word. Again, these between-stimulus differences will tend to be apparent when you examine observations of responses across the sample of words.\n\nUnder these circumstances, are observations about the responses made by different participants nested under words, or are observations about the responses to different words nested under participants? We do not have to make a decision.\n\nGiven this common **repeated-measures** design, we can analyze the outcome variable in relation to:\n\n-   **fixed effects**: the impact of independent variables like participant reading skill or word frequency\n-   **random effects**: the impact of random or unexplained differences between participants *and also* between stimuli\n\nIn this situation, we can say that the random effects are crossed [@baayen2008]. When multilevel models require the specification of crossed random effects, they tend to be called **mixed-effects models**.\n\n## Working with mixed-effects models {#sec-intro-mixed-working-models}\n\nTo illustrate the approach, we examine observations from the CP study. We begin, as we did previously, by ignoring differences due to grouping variables (like participant or stimulus). We pretend that all observations are independent. In this fantasy situation, we address our research question.\n\n::: callout-note\n-   Research question: What word properties influence responses to words in a test of reading aloud?\n:::\n\n### Load the data if you need to {#sec-intro-mixed-data-load}\n\nIf you have not completed the process of tidying the CP study data then you can import the pre-tidied data here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong.all.noNAs <- read_csv(\"data/long.all.noNAs.csv\", \n                  col_types = cols(\n                    subjectID = col_factor(),\n                    item_name = col_factor()\n                    )\n                  ) \n```\n:::\n\n\nNotice that I am using `read_csv()` with an additional argument `col_types = cols(...)`.\n\n-   Here, I am requesting that `read_csv()` treats `subjectID` and `item_name` as factors.\n\n::: callout-tip\nWe can use `col_types = cols(...)` to control how `read_csv()` interprets specific column variables in the data.\n:::\n\nControlling the way that `read_csv()` handles variables is a very useful capacity, and a more efficient way to work than, say, first reading in the data and then using *coercion* to ensure that variables are assigned appropriate types. You can read more about it [here](https://readr.tidyverse.org/articles/readr.html).\n\n### Linear model for multilevel data -- ignoring the hierarchical structure {#sec-intro-mixed-lm}\n\nWe begin our data analysis by asking if reading reaction time (RT) varies in association with word frequency. A scatterplot shows that response latencies decrease with increasing word frequency (@fig-freq-all).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong.all.noNAs %>%\nggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5, colour=\"red\") + \n  theme_bw() + \n  xlab(\"Word frequency: log context distinctiveness (CD) count\")\n```\n\n::: {.cell-output-display}\n![Reading reaction time compared to word frequency, all data](Week17_files/figure-html/fig-freq-all-1.png){#fig-freq-all fig-alt='The figure shows a scatterplot: each point represents the lexical frequency (on the x-axis) and the reaction time (RT, on the y-axis) of responses made by children to a set of words. A red line is drawn through the points, indicating that RT decreases for higher values of frequency.' width=432}\n:::\n:::\n\n\nIn the plot, we see that the best fit line drawn with `geom_smooth()` trends downward for higher values of word frequency. This means that @fig-freq-all suggests that RT decreases with increasing word frequency. (I know there is a weird looking line of points around 0 but we can ignore that here.)\n\nWe can estimate the relationship between RT and word frequency using a linear model in which we ignore the possibility that there may be differences (between subjects, or between items) in the intercept or (between subjects) in the slope of the frequency effect.\nThis simplified model can be stated as:\n\n$$\nY_{ij} = \\beta_0 + \\beta_1X_j + e_{ij} \n$$\n\n-   where $Y_{ij}$ is the value of the observed outcome variable, the RT of the response made by the $i$ participant to the $j$ word;\n-   $\\beta_1X_j$ refers to the fixed effect of the explanatory variable (here, word frequency), where the frequency value $X_j$ is different for different words $j$, and $\\beta_1$ is the estimated coefficient of the effect due to the relationship between response RT and word frequency;\n-   $e_{ij}$ is the residual error term, representing the differences between observed $Y_{ij}$ and predicted values (given the model).\n\nThe linear model can be fit in R using the `lm()` function, as we have done previously.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# label: lm-all-freq\nlm.all.1 <- lm(RT ~  Lg.UK.CDcount, data = long.all.noNAs)\n\nsummary(lm.all.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = RT ~ Lg.UK.CDcount, data = long.all.noNAs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-346.62 -116.03  -38.37   62.05 1981.58 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    882.983     11.901   74.19   <2e-16 ***\nLg.UK.CDcount  -53.375      3.067  -17.40   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.9 on 9083 degrees of freedom\nMultiple R-squared:  0.03227,\tAdjusted R-squared:  0.03216 \nF-statistic: 302.8 on 1 and 9083 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIn the estimates from this linear model, we see an approximate first answer to our research question.\n\n::: callout-note\n-   Research question: What word properties influence responses to words in a test of reading aloud?\n-   Result: Our analysis shows that the estimated effect of word frequency is $\\beta = -53.375$. This means that, according to the linear model, RT decreases by about 54 milliseconds for each unit increase in log word frequency.\n:::\n\nNotice that, here, word frequency information is located in the `Lg.UK.CDcount` variable. In a common move for reading data analyses, we transformed the frequency estimate to the Log base 10 of the word frequency values, prior to analysis, in part because word frequency estimates are usually highly skewed.\n\nThe model does not explain much variance, as $Adjusted \\space R^2 = .03$ but, no doubt due to the large sample, the regression model is significant overall $F(1,9083) = 302.8, p < .001$.\n\n#### Exercise {#sec-intro-mixed-exx-lm}\n\n-   Vary the linear model: using different outcomes or different predictor variables.\n\nThe CP study dataset is rich with possibility. It would be useful to experiment with it.\n\n1.  Change the predictor from frequency to something else: what do you see when you visualize the relationship between variables using scatterplots?\n2.  Specify linear models with different predictors: do the relationships you see in plots match the coefficients you see in the model estimates?\n\n### Can we ignore the hierarchical structure? {#sec-intro-mixed-ignore-hierarchy}\n\nThe problem is that, as we have discussed, we assume that observations are independent for the linear model yet we can suppose in advance that that assumption of independence will be questionable given the expectation that participants' responses will differ in predictable ways: one participant's responses will perhaps be slower or less accurate than another, perhaps more or less affected by word frequency than another.\n\nWe can examine that variation by estimating the intercept and the slope of the frequency effect separately using the data for each participant alone. We can start by visualizing the frequency effect for each child in a grid of plots, with each plot representing the $\\text{RT} \\sim \\text{frequency}$ relationship for the data for a child (@fig-freqperchildtrellis).\n\nWe looked at how the plotting code works in [Week 16](Week16.qmd#sec-multi-plot-lattice).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlong.all.noNAs %>%\n  ggplot(aes(x = Lg.UK.CDcount, y = RT)) +\n    geom_point(alpha = .2) + \n    geom_smooth(method = \"lm\", se = FALSE, size = 1.25, colour = \"red\") + \n    theme_bw() + \n    xlab(\"Word frequency (log10 UK SUBTLEX CD count)\") + \n    facet_wrap(~ subjectID)\n```\n\n::: {.cell-output-display}\n![RT vs. word frequency, considered separately for data for each child](Week17_files/figure-html/fig-freqperchildtrellis-1.png){#fig-freqperchildtrellis fig-alt='The figure shows a grid of scatterplots. In each plot, each point represents the lexical frequency (on the x-axis) and the reaction time (RT, on the y-axis) of responses made by children to a set of words. A red line is drawn through the points, indicating that RT decreases for higher values of frequency. A different plot is shown to represent the data for each child. Between plots, we can see that the potential association between RT and frequency can vary.' width=720}\n:::\n:::\n\n\n@fig-freqperchildtrellis shows how, on average, more frequent words are associated with shorter reaction time: faster responses. The plot further shows, however, that the effect of frequency varies considerably between children.\n\n-   Some children show little or no effect; the best fit line is practically level.\n-   Some children show a marked effect, with a steep fit line indicating a strong frequency effect.\n\nWe can get more insight into the differences between children if we plot the estimated intercept and frequency effect coefficients for each child directly. This allows more insight because it focuses the eye on the differences between children in the estimates. We do this next: see @fig-freqperchildlm. (I work through the code for generating the plot in: `02-mixed-workbook-with-answers.R`)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Estimated intercepts and frequency effect slopes calculated for each child's data, with child data analysed separately for all children. Points represent estimates. Lines represent standard errors for estimates. Point estimates are presented in order of size.](Week17_files/figure-html/fig-freqperchildlm-1.png){#fig-freqperchildlm fig-alt='The figure presents two scatterplots: on the left, a plot presenting estimates of intercepts (shown as points, with lines representing standard errors); on the right, a plot presenting estimates of slopes for the effect of frequency on RT (shown as points, with lines representing standard errors). The estimates clearly vary between children.' width=768}\n:::\n:::\n\n\n@fig-freqperchildlm presents two plots showing the estimates of the intercept and the coefficient of the effect of word frequency on reading RT, calculated separately for each child. This means that we fitted a separate linear model, for the association between RT and frequency, using the data for just one child, for each child in our sample, for all the children.\n\nThe estimate for each child is shown as a black dot. The standard error of the estimate is shown as a black vertical line, shown above and below a point. You can say that where there is a longer line there we have more uncertainty about the location of the estimate.\n\nThe estimates calculated for each child are shown ordered from left to right in the plot by the size of the estimate. This adjustment to the plot reveals how the estimates of both the intercept and the slope of the frequency effect vary substantially between children.\n\n::: callout-important\nThe first **key observation** is that if there is an average intercept for everyone in the sample or, better, an intercept we could estimate for everyone in the population, then the different intercepts we have estimated for each child would be distributed around that population-level average:\n\n-   Some children will have slower (here, larger) intercepts\n-   and other children will have faster (shorter) intercepts.\n:::\n\nHere, the intercept can be taken to be the average RT when all other effects in the model are set to zero. RT varies for this sample around somewhere like $\\beta_0 = 883ms$ so a slower larger intercept might be e.g. $\\beta_0 = 1000ms$.\n\n::: callout-important\nThe second **key observation** is that if there is an average slope for the frequency effect, an effect of frequency on reading RT, averaged across everyone in the population, then, again, the different slopes we have estimated for each child would be distributed around that population-level effect.\n\n-   Some children will have larger (here, more negative) frequency effects\n-   and other children will have smaller (less negative) frequency effects.\n:::\n\nHere, the frequency effect is associated with a negative coefficient e.g. $\\beta_1 = -53$ so a larger frequency effect will be a bigger negative number e.g. $\\beta_1 = -100$.\n\n### Multilevel -- here, more appropriately known as -- mixed-effects models {#sec-intro-mixed-effects-models}\n\nIn a mixed-effects model, we account for this variation: the differences between participants in intercepts and slopes. \n\nWe do this by modeling the intercept as two terms:\n\n$$\n\\beta_{0i} = \\gamma_0 + U_{0i}\n$$\n\n-   where $\\gamma_0$ is the average intercept and $U_{0i}$ is the difference for each $i$ child between their intercept and the average intercept.\n\nWe model the frequency effect as two terms:\n\n$$\n\\beta_{1i} = \\gamma_1 + U_{1i}\n$$\n\n-   where $\\gamma_1$ is the average slope and $U_{1i}$ represents the difference for each $i$ child between the slope of their frequency effect and the average slope.\n\nWe can then incorporate in a single model the **fixed effects** due to the average intercept and the average frequency effect, as well as the **random effects**, error variance due to unexplained differences between participants in intercepts and frequency effects:\n\n$$\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n$$\n\n-   where the outcome $Y_{ij}$ is related to ...\n-   the average intercept $\\gamma_0$ and differences between $i$ children in the intercept $U_{0i}$;\n-   the average effect of the explanatory variable frequency $\\gamma_1X_j$ and differences between $i$ participants in the slope $U_{1i}X_j$;\n-   in addition to residual error variance $e_{ij}$.\n\n#### What are we doing with these random effects terms? {#sec-intro-mixed-random-effects-terms}\n\nIn sections @sec-intro-mixed-BLUPs and @sec-intro-mixed-variance-covariance, we look at what *exactly* is captured in these random effects terms $U_{0i}, U_{1i}$. Let's first look at the practicalities of analysis then come back to deepen our understanding a bit more.\n\nRight now, it is important to understand that in our analysis we do not care about the differences between *specific* children. We care that there are differences. And we care how widely spread are the differences between child A and the average intercept (or slope), or between child B and the average intercept (or slope), or between child C ... (you get the idea). Therefore, in our analysis, we estimate the spread of the differences as a *variance term*. We can see this when we look at the results of the mixed-effects model we specify, next.\n\n### Fitting a mixed-effect model using the lmer() function {#sec-intro-mixed-lmer}\n\nWe can fit a mixed-effects model of the $\\text{RT} \\sim \\text{frequency}$ relationship, taking into account the random differences between participants. I first go through the model fitting code bit by bit. (I go through the output, the results, in @sec-intro-mixed-lmer-results.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmer.all.1 <- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n```\n:::\n\n\nYou have seen the `lmer()` function code before but *practice makes perfect* so we shall go through the code step by step, as we did previously.\n\nFirst, we have a chunk of code mostly similar to what we do when we do a regression analysis.\n\n1.  `lmer.all.1 <- lmer(...)` creates a *linear mixed-effects model* object using the `lmer()` function.\n2.  `RT ~  Lg.UK.CDcount` is a formula expressing the model in which we estimate the fixed effect on the outcome or dependent variable `RT` (reaction time, in milliseconds) as predicted $\\sim$ by the independent or predictor variable `Lg.UK.CDcount` (word frequency).\n3.  `...(..., data = long.all.noNAs)` specifies the dataset in which you can find the variables named in the model fitting code.\n4.  `summary(lmer.all.1)` gets a summary of the fitted model object, showing you the results.\n\nSecond, we have the bit that is specific to multilevel or mixed-effects models.\n\n-   We add `(...||subjectID)` to tell R about the random effects corresponding to random differences between sample groups (here, observations grouped by child) that are coded by the `subjectID` variable.\n-   `(...1 ||subjectID)` says that we want to estimate random differences between sample groups (observations by child) in intercepts, where the intercept is coded by `1`.\n-   `(Lg.UK.CDcount... ||subjectID)` adds random differences between sample groups (observations by child) in slopes of the frequency effect coded using the `Lg.UK.CDcount` variable name.\n\n#### Exercise {#sec-intro-mixed-exx-lmer}\n\nIt will help your learning if you now go back and compare this model with the model you saw in [Week 16](Week16.qmd#sec-multi-lmer).\n\n-   Identify what is different: dataset, variable names, and model formula.\n-   Identify what stays the same: function name ... the specification of both model and random effects.\n\nIf you can see what is different versus what stays the same then you learn what *you can change* when the time comes for your analysis with your data.\n\n::: callout-tip\nLearning to look at example code *so that you can identify how to adapt it* for your own purposes is a key skill in psychological data science.\n:::\n\n#### What does \\|\\| mean? {#sec-intro-mixed-double-bar}\n\nBefore we move on, I want you to notice something that looks like nothing much: `||`.\n\nWe are going to need to defer until later a (necessary) discussion of exactly why we need the two double lines. In short, the use of `||` asks R to fit a model in which we estimate random effects associated with:\n\n-   variance due to differences in intercepts\n-   variance due to differences in slopes\n-   but *not* covariance between the two sets of differences\n\nI do this because otherwise the model I specify *will not converge*.\n\nWe shall need to discuss these things: **convergence**, and failures to converge; as well as random effects specification and simplification. We will discuss random effects covariance in @sec-intro-mixed-variance-covariance. For now, the most important lesson is learnt by seeing how the analysis approach we saw last week can be extended to examining the effects of experimental variables in data from repeated measures design studies.\n\n### Reading the lmer() results {#sec-intro-mixed-lmer-results}\n\nThe `lmer()` model code we discussed in @sec-intro-mixed-lmer gives us the following output.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID))\n   Data: long.all.noNAs\n\nREML criterion at convergence: 117805.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7839 -0.5568 -0.1659  0.3040 12.4850 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n subjectID   (Intercept)   87575    295.93  \n subjectID.1 Lg.UK.CDcount  2657     51.55  \n Residual                  23734    154.06  \nNumber of obs: 9085, groups:  subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)    950.913     39.216  24.248\nLg.UK.CDcount  -67.980      7.092  -9.586\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.093\n```\n:::\n:::\n\n\nWe discussed the major elements of the results output last week. We expand on that discussion, a little, here.\n\nThe output from the model summary first gives us **information about the model**.\n\n1.  First, we see information about the function used to fit the model, and the model object created by the `lmer()` function call.\n2.  Then, we see the model formula `RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID)`.\n3.  Then, we see `REML criterion at convergence` about the model fitting process, which we can usually ignore.\n4.  Then, we see information about the distribution of the model residuals.\n5.  We then see information listed under **Random effects**: this is where you can see information about the error variance terms estimated by the model.\n\n-   The information is listed in four columns: 1. `Groups`; 2. `Name`; 3. `Variance`; and 4. `Std.Dev.`\n\nWe have discussed how observations can be *grouped* by participant (because we have multiple response observations for each person in the study) just as previously we identified how observations could be *grouped* by class (because we saw that children were nested under class). That is what we mean when we refer to `Groups`: we are identifying the grouping variables that give hierarchical structure to the data.\n\n-   The `Name` lists whether the estimate we are looking at corresponds to, here, random differences between participants in intercepts (listed as `(Intercept)`), or in slopes (listed as `Lg.UK.CDcount`).\n\nAs we discuss later, in @sec-intro-mixed-variance-covariance, mixed-effects models estimate the spread in random differences. We are not interested in the specific differences in intercept or slope between specific individuals. What we want is to be able to take into account the variance associated with those differences.\n\nThus, we see in the `Random Effects` section, the variances associated with:\n\n-   `subjectID Intercept) 87575`, differences between participants in the intercepts;\n-   `subjectID.1 Lg.UK.CDcount 2657`, differences between participants in the slopes of the frequency effect;\n-   Alongside `Residual  23734`, residuals where, just like a linear model, we have variance associated with differences between model estimates and observed RT, here, at the trial level.\n\nWe do not usually discuss the specific variance estimates in research reports. However, the relative size of the variances does provide useful information [@meteyard2020a], as we shall see when we discuss the different estimates we get when we include a random effect due to differences between items (@sec-intro-mixed-random-effect-items).\n\n6.  Lastly, we see estimates of the coefficients (of the slopes) of the fixed effects.\n\nIn this model, we see estimates of the fixed effects of the intercept and the slope of the `RT ~ Lg.UK.CDcount` model. We discuss these estimates next.\n\n#### Is there a difference between linear model and linear mixed-effects model results? {#sec-intro-mixed-lm-lmer-results}\n\nRecall that the linear model yields the estimate for the frequency effect on reading RT such that RT decreases by about 53 ms for unit increase in log word frequency ($\\beta = -53.375$). Now, when we have taken random differences between participants into account, we see that the estimate of the effect **for the mixed-effects model** is $\\beta = -67.980$. Taking into account random differences clearly has an impact on results.\n\n::: callout-note\n-   Research question: What word properties influence responses to words in a test of reading aloud?\n-   Result: The mixed-effect analysis shows that RT decreases by about $68$ milliseconds for each unit increase in log word frequency.\n:::\n\nWhich coefficient estimate should you trust? Well, it is obvious that the linear model and the linear mixed-effects model estimate are relatively similar. However, it is also obvious that the linear model makes an assumption -- the *assumption of independence of observations* -- that does not make sense theoretically (because reading responses will be similar for each child) and does not make sense empirically (because responses will differ between children, see @fig-freqperchildlm). Thus, we have good grounds for supposing that the linear mixed-effects model estimate for the frequency effect is likely to be closer to the true underlying population effect.\n\nIt is important to remember, however, that whatever estimate we can produce is the estimate *given* the sample of words we used, our information about word frequency, and our measurement of reading RT responses. How far our estimate actually generalizes to the wider population is not something we can judge in the context of a single study.\n\nFurther, we have not finished in our consideration of the random effects that the account should include. We need to do more work by thinking about the differences between stimuli (@sec-intro-mixed-fixed-effect-fallacy).\n\n::: callout-warning\nWhy aren't there p-values?\n\nWe will come back to this ([here](Week18.qmd#sec-dev-mixed-discussion-p-values)) but note that if $t > 2$ we can suppose that an effect is significant at the $.05$ significance level.\n:::\n\n### What we estimate when we estimate random effects {#sec-intro-mixed-BLUPs}\n\nWe have said that we can incorporate, in a mixed-effects model, **fixed effects** (e.g., the average frequency effect) and **random effects**, error variance due to unexplained differences between participants in intercepts and in frequency effects:\n\n$$\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + e_{ij}\n$$\n\nSo we distinguish:\n\n-   the average intercept $\\gamma_0$ and differences between $i$ children in the intercept $U_{0i}$;\n-   the average effect of the explanatory variable frequency $\\gamma_1X_j$ and differences between $i$ participants in the slope $U_{1i}X_j$.\n\nWhen we think about the differences between participants (or between the units of any grouping variable), in intercepts or in slopes, we typically assume that the differences are:\n\n-   random;\n-   should be normally distributed;\n-   and are distributed around the population or average fixed effects.\n\nWe can say that the mixed-effects model sees the differences between participants **relative to the fixed effect intercept or slope**, that is, relative to the population level or average effects.\n\nWe can illustrate this by plotting, in @fig-BLUPS-prediction, the differences as estimated -- technically, *predicted* -- by the mixed-effects model that we have been examining. (The code for producing the plot can be found in `02-mixed-workbook-answers.R`.)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plot showing histograms indicating the distribution of participant adjustments to account for between-child differences in intercept or slope (the Best Linear Unbiased Predictions).](Week17_files/figure-html/fig-BLUPS-prediction-1.png){#fig-BLUPS-prediction fig-alt='The figure shows two histograms (a.) on the left represents the distribution of adjustments to account for per-child deviations between the overall intercept and the intercept estimated for a child while (b.) on the right represents the distribution of adjustments to account for per-child deviations between the overall slope and the slope estimated for a child. The histograms indicate widely spread deviations about the overall estimate.' width=960}\n:::\n:::\n\n\nWhat you can see in @fig-BLUPS-prediction are distributions, presented using histograms. The centers of the distributions are located at zero (shown by a red line). For each distribution (a. and b.), that is where the model estimate of the intercept or the slope of the frequency effect is located. Spread around that central point, you see the adjustments the model makes to account for differences between participants.\n\nNotice how, in @fig-BLUPS-prediction (a.):\n\n1.  Some children have intercepts that are smaller than the population-level or average intercept -- so their adjustments are negative (to decrease their intercepts).\n2.  Some children have intercepts that are larger than the population-level or average intercept -- so their adjustments are positive (to increase their intercepts).\n\n-   Strikingly, you can see that a few children have intercepts that are as much as 1000ms larger than the population-level or average intercept: the bars representing the estimates for these children are far out on the right of the x-axis in @fig-BLUPS-prediction (a.).\n\nNow notice how, in @fig-BLUPS-prediction (b.):\n\n3.  Some children have frequency effects (coefficients) that are smaller than the population-level or average frequency effect -- so their adjustments are positive (to decrease their frequency effect, by making it *less* negative).\n4.  Some children have frequency effects that are larger than the population-level or average frequency effect -- so their adjustments are negative (to increase their frequency effect, by making it *more* negative).\n\n(When you look at @fig-BLUPS-prediction (b.), remember that the estimated $\\beta$ coefficient for the frequency effect is negative because higher word frequency is associated with smaller RT.)\n\n-   Strikingly, you can see that a few children have frequency effects that are as much as 200ms larger (see plot (b.) around $x = -200$) than the population-level or average effect.\n\nWhen a mixed-effects model is fitted to a dataset, its set of estimated parameters includes the coefficients for the fixed effects as well as the standard deviations for the random effects [@baayen2008]. If you read the literature on mixed-effects models, you will see that the adjustments are called *Best Linear Unbiased Predictors* (BLUPs).\n\n#### Exercise: Random effects in mixed-effects models {#sec-intro-mixed-exx-lmer-random-effects}\n\nMixed-effects modeling is hard to get used to *at first*. A bit more practice helps to show you how the different parts of the model work. We again focus on the random effects.\n\nIn the model we have seen so far, we specify `(Lg.UK.CDcount + 1||subjectID)`\n\n-   We can change this part -- and only this part -- to see what happens to the results. **Do it:** rerun the model code, having changed the random effects part:\n\n1.  `lmer(RT ~  Lg.UK.CDcount + (1|subjectID)...)` gives us a *random intercepts* model accounting for just random differences between participants in the intercept\n2.  `lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 0|subjectID)...)` gives us a *random slope* model accounting for just random differences between participants in the slope of the frequency effect\n3.  `lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID)...)` gives us a *random intercepts and slopes* model accounting for both random differences between participants in the intercept and in the slope, as well as covariance in these differences.\n\n::: callout-tip\nTry out these variations and *look carefully* at the different results. Look, especially, at what happens to the `Random effects` part of the summary.\n:::\n\nThis will be an important and revealing exercise.\n\nWe can *visualize* the differences between the models in a plot showing the different predictions that the different models give us. @fig-indiv-prediction shows what a mixed-effects model predicts are the effects of frequency on RT *for different children* in the CP study.\n\n-   The predictions vary depending on the nature of the random effects we specify in the model.\n\n(Note that I figured out how to produce the plot from the information [here](https://bbolker.github.io/morelia_2018/notes/mixedlab.html).)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Plot showing model predictions of the effect, for each individual, of word frequency on reading reaction time -- predictions vary between models incorporating (a.) random effect of participants on intercepts only; (b.) random effect of participants on slopes only and (c.) random effect of participants on intercepts and on slopes.](Week17_files/figure-html/fig-indiv-prediction-1.png){#fig-indiv-prediction fig-alt='The figure shows a grid of scatterplots. In all scatterplots, the same scatter is shown. Points in grey represent the RT and word frequency information for children\\'s responses in reading words in the experimental task. Superimposed on the points are sets of red lines. The lines represent the predictions of different models (a.) assuming a random effect of participants on intercepts, all slopes are the same  but intercepts vary (b.) assuming a random effect of participants on slopes only, slopes vary but intercepts are the same (c.) and assuming random effect of participants on intercepts and on slopes both intercepts and slopes can be seen to vary.' width=960}\n:::\n:::\n\n\nWe can see that:\n\n1.  If the model includes the random effect of *participants on intercepts only* then all the slopes are the same (the lines in the figure are parallel) because this model assumes that the only differences between participants are differences in the intercepts.\n2.  If the model includes the random effect of *participants on slopes only* then the slopes vary but they all have the same intercept. The plot does not show this but you can see how all the slopes are converging on one point somewhere on the left. This happens because this model assumes that the only differences between participants are differences in the slopes.\n3.  If the model includes the random effect of *participants on intercepts and on slopes* then we can see how the intercepts and the slopes vary. Given what we saw when we looked at the relation between frequency and RT for each participant considered separately we might argue that this model is much more realistic about the data.\n\n#### Exercise: Error messages for mixed-effects models {#sec-intro-mixed-exx-lmer-error}\n\n::: callout-important\nIt is key to your skills development that you learn to make effective use of the warnings and error messages R can produce.\n:::\n\nYou do not have to just believe me when I say that `||` is in the model code to stop a problem appearing.\n\n-   Experiment -- and see what happens when you change the code. Try this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmer.all.1 <- lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1|subjectID),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.1)\n```\n:::\n\n\nDo you get an error message?\n\nA very useful trick is to learn to copy the error message you get into a search engine on your web browser. Do this and you will find useful help, as [here](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html)\n\n## Variation between stimuli: the \"language as fixed-effect fallacy\" {#sec-intro-mixed-fixed-effect-fallacy}\n\n::: callout-important\nExperimental psychologists will often collect data in studies where they present some stimuli to a sample of participants.\n:::\n\n@clark1973 showed that the appropriate analysis of experimental effects for such data requires the researcher to take into account the error variance due to unexplained or random differences between sampled participants *and also* to random differences between sampled stimuli. This is true in the context of psycholinguistics but it is also true in the context of work in any field where the presented stimuli can be understood to constitute a sample from a wider population of potential stimuli [e.g., stories about social situations, @judd2012].\n\nIf we were to estimate the average latency of the responses made by different children to each word, in the CP study data, we would see that there is considerable variation between words. We do this in @fig-pitemsints. (I work through the code for producing the plot in `02-mixed-workbook-answers.R`.)\n\n-   Some words elicit slower and some elicit faster responses on average.\n-   We can also see that there is, again, variation in the uncertainty of estimates, as reflected in differences in the lengths of the error bars corresponding to the standard errors of the estimates.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Estimated intercepts (with SEs) calculated for each stimulus word, with coefficients ordered by average latency for each word](Week17_files/figure-html/fig-pitemsints-1.png){#fig-pitemsints fig-alt='The figure shows a scatterplot: each point represents the estimate for the deviation from the average intercept predicted for responses to each word, for a sample of words. The points are ordered by the estimate and the plot presents a curve of points from left to right from (low) small to (high) large deviations.' width=768}\n:::\n:::\n\n\nIn general, psychologists have been aware since @clark1973 (if not earlier) that responses to experimental stimuli can vary because of random or unexplained differences between the stimuli: whether the stimuli are words, pictures or stories, etc. And researchers have been aware that if we did not take such variation into account, we might mistakenly detect an experimental effect, for example, as a significant difference between mean response in different conditions, simply because different stimuli presented in different conditions varied in some unknown way, randomly.\n\nFor many years, psychologists tried to take random differences between stimuli into account, alongside random differences between participants, using a variety of strategies with important limitations (see @baayen2008, for discussion). @clark1973 suggested that researchers could calculate $minF'$ (not F) when doing Analyses of Variance of experimental data\n\nThis involves a series of steps.\n\n1.  You start by *aggregating* your data\n\n-   By-subjects data -- for each subject, take the average of their responses to all the items\n-   By-items data -- for each item, take the average of all subjects' responses to that item\n\n2.  You do separate ANOVAs, one for by-subjects (F1) data and one for by-items (F2) data\n3.  You put F1 and F2 together, calculating minF'\n\nAveraging data by-subjects or by-items is relatively simple. It is *very common* to see, in the literature, psychological reports in which F1 and F2 analysis results are presented (that is why I am explaining this).\n\nCalculating $minF'$ is also relatively simple:\n\n$$\nminF' = \\frac{MS_{effect}}{MS_{\\text{random-subject-effects}} + MS_{\\text{random-word-differences}}} = \\frac{F_1F_2}{F_1 + F_2}\n$$\n\nHowever, after a while, psychologists stopped doing the extra step of the $minF'$ calculation [@raaijmakers1999]. They carried on calculating and reporting F1 and F2 ANOVA results but, as @baayen2008 discuss, that approach risks a high *false positive error rate*.\n\nPsychologists also found that while the $minF'$ approach allowed them to take into account between-participant and between-stimulus differences it could not be applied where ANOVA could not be used. This stopped researchers from taking a comprehensive approach to error variance where they wanted to conduct multiple regression analyses.\n\nIn the psychological literature, you will often see multiple regression analyses of by-items data, where a sample of participants has been asked to respond to a sample of stimuli, and the analysis is of the effects of stimulus properties on outcomes averaged (over participants' responses) to the mean outcome by item. The problem is that analyzing data only by-items ensures that we lose track of participant differences.\n\n@lorch1990 warn that analyzing only by-items mean RTs just assumes wrongly that *subjects are a fixed effect*. This approach, again, risks a higher rate of false positive errors.\n\n### Include the random effect of stimulus {#sec-intro-mixed-random-effect-stimulus}\n\nThe good thing is that, thanks to the advent of mixed-effects models, we now no longer need to tolerate these problems.\n\nIn the context of our working example, in our analysis of the CP study data, we can build up our mixed-effects model by adding a random effect to capture the impact of unexplained differences between stimuli.\n\nWe model the random effect of items on intercepts by modeling the intercept as two terms:\n\n$$\n\\beta_{0j} = \\gamma_0 + W_{0j}\n$$\n\n-   where $\\gamma_0$ is the average intercept and $W_{0j}$ represents the deviation, for each word, between the average intercept and the per-word intercept.\n\nOur model can now incorporate the additional random effect of items on intercepts:\n\n$$\nY_{ij} = \\gamma_0 + \\gamma_1X_j + U_{0i}+ U_{1i}X_j + W_{0j} + e_{ij}\n$$\n\nIn this model, the outcome $Y_{ij}$ is related to:\n\n-   the average intercept $\\gamma_0$ and the word frequency effect $\\gamma_1X_j$\n-   plus random effects due to unexplained differences between participants in intercepts $U_{0i}$ and the slope of the frequency effect $U_{1i}X_j$\n-   *as well as* random differences between items in intercepts $W_{0j}$,\n-   in addition to the residual term $e_{ij}$.\n\n::: callout-warning\nWhat about random effects associated with differences between stimulus items in the slopes of effects?\n\n- Just as we may expect there to be between-participant differences in the slope of the word frequency effect, we may expect there to be between-stimulus differences in the slope of the effect of, e.g., participant age.\n- Rest assured, we *will* look at this question.\n:::\n\n### Fitting a mixed-effect model -- now with random effects of subjects and items {#sec-intro-mixed-random-effect-items}\n\nWe can fit a mixed-effects model of the $\\text{RT} \\sim \\text{frequency}$ relationship, taking into account the random differences between participants *and now also* the random differences between stimulus words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmer.all.2 <- lmer(RT ~  Lg.UK.CDcount + \n                         (Lg.UK.CDcount + 1||subjectID) +\n                         (1|item_name),\n             \n             data = long.all.noNAs)\n\nsummary(lmer.all.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ Lg.UK.CDcount + ((1 | subjectID) + (0 + Lg.UK.CDcount |  \n    subjectID)) + (1 | item_name)\n   Data: long.all.noNAs\n\nREML criterion at convergence: 116976.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1795 -0.5474 -0.1646  0.3058 12.9485 \n\nRandom effects:\n Groups      Name          Variance Std.Dev.\n item_name   (Intercept)     3397    58.29  \n subjectID   Lg.UK.CDcount   3624    60.20  \n subjectID.1 (Intercept)   112314   335.13  \n Residual                   20704   143.89  \nNumber of obs: 9085, groups:  item_name, 159; subjectID, 61\n\nFixed effects:\n              Estimate Std. Error t value\n(Intercept)     971.07      51.87  18.723\nLg.UK.CDcount   -72.33      10.79  -6.703\n\nCorrelation of Fixed Effects:\n            (Intr)\nLg.UK.CDcnt -0.388\n```\n:::\n:::\n\n\nThis is the same mixed-effects model as the one we discussed in @sec-intro-mixed-lmer and @sec-intro-mixed-lmer-results but with one important addition.\n\n-   We add `(1|item_name)` to take into account random differences between between words in intercepts.\n\n#### Reading the results {#sec-intro-mixed-random-effect-items-results}\n\nTake a look at the model results. You should notice three changes.\n\n1.  You can see that the estimate for the effect of word frequency on reading reaction time has changed again, it is now $\\beta = -72.33$\n2.  `item_name   (Intercept)     3397` there is now an additional term in the list of random effects, giving the model estimate for variance associated with random differences between words in intercepts\n3.  And you can see that the residual variance has changed. In the first model `lmer.all.1` it was `23734`, now it is `20704`\n\n::: callout-tip\nThe reduction in residual variance is one way in which we can judge how good a job the model is doing in accounting for the variance in the outcome, observed response reaction time.\n:::\n\nWe can see that by adding a term to account for differences between items we can reduce the amount by which the model estimates deviate from observed outcomes. This difference in error variance is, essentially, one basis for estimating how well the model fits the data, and a basis for estimating the *variance explained* by a model in terms of the $R^2$ statistic you have seen before.\n\nWe will come back to this.\n\n## Variances and covariances of random effects {#sec-intro-mixed-variance-covariance}\n\nAs I have said, we usually do not aim to examine the specific deviation from the average intercept or the average fixed effect slope for a participant or stimulus. We estimate just the spread of deviations by-participants or by-items.\n\nA mixed-effects model like our final model includes fixed effects corresponding to the intercept and the slope of the word frequency effect plus the *variances*:\n\n-   $var(U_{0i})$ variance of deviations by-participants from the average intercept;\n-   $var(U_{1i}X_j)$ variance of deviations by-participants from the average slope of the frequency effect;\n-   $var(W_{0j})$ variance of deviations by-items from the average intercept;\n-   $var(e_{ij})$ residuals, at the response level, after taking into account all other terms.\n\nBecause we have variances, we may expect the random effects of participants or items to covary, e.g., participants who are slow to respond may also be more susceptible to the frequency effect, as can be seen in @fig-covarp.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Scatterplot showing the relationship between estimated coefficients for the intercept and for the frequency effect, for each child analysed separately](Week17_files/figure-html/fig-covarp-1.png){#fig-covarp fig-alt='The figure shows a scatterplot: each point represents the estimate of the intercept and the slope of the frequency effect, estimated in separate linear models for the data for each child. The plot shows a narrow cloud of points trending downwards such that larger intercepts (rightwards on the x-axis) are associated with more negative slopes (lower points on the y-axis).' width=384}\n:::\n:::\n\n\nThis is why it would often make sense to specify, among the random effects of the model, terms corresponding to the covariance of the random effects:\n\n-   $covar(U_{0i}, U_{1i}X_j)$\n\n::: callout-warning\nRemember we excluded random effects covariance {#sec-intro-mixed-exclude-covariance}\n:::\n\nIn @sec-intro-mixed-double-bar, I noted how we used the `||` notation to stop the model estimating the covariance between differences between participants in intercepts and in slopes. The reason I did this is that if I had requested that the model estimate the covariance the model would have failed to converge. What this means depends on understanding how mixed-effects models are estimated. We shall have to return to a development of that understanding later. For now, it is enough to note that mixed-effects models fitted with `lmer()` often have more difficulty with random effects covariance estimates.\n\n## Reporting the results of a mixed-effects model {#sec-intro-mixed-lmer-results-reporting}\n\nThere is no official convention on what or how to report the results of a mixed-effects model. Lotte Meteyard and I suggest what psychologists should report in an article [@meteyard2020a] that has been downloaded a few thousand times so, maybe, our advice will help to influence practice.\n\n::: callout-advice\n-   Explain what you did, and why.\n-   Explain what you found, not just whether effects are significant or not.\n:::\n\n1.  We argue that researchers should explain what analysis they have done and, where space allows, should report both the estimates of the **fixed effects** and the estimates of the **random effects**.\n\n2.  We think you can report the model code (maybe in an appendix, maybe in a note under a tabled summary of results).\n\nA table summary presenting model results can look like this.\n\n| Coefficients     | Estimate | SE   | t    |     |\n|------------------|----------|------|------|-----|\n| (Intercept)      | 971.1    | 51.9 | 18.7 |     |\n| Frequency effect | -72.3    | 10.8 | -6.7 |     |\n\n| Groups      | Name        | Variance | SD    |     |\n|-------------|-------------|----------|-------|-----|\n| item        | (Intercept) | 3397     | 58.3  |     |\n| participant | (Intercept) | 112314   | 335.1 |     |\n| participant | Frequency   | 3624     | 60.2  |     |\n| residual    |             | 20704    | 143.9 |     |\n\nNote: `lmer(RT ~  Lg.UK.CDcount + (Lg.UK.CDcount + 1||subjectID) + (1|item_name)`\n\n@Barr2013a argued that choices about random effects structure affect the generalizability of the estimates of fixed effects. In particular, it seems sensible to examine the possibility that the slope of the effect of an explanatory variable may vary at random between participants or between stimuli. Correspondingly, researchers should report and explain their decisions about the inclusion of random effects.\n\n::: callout-tip\nResearchers should report their modelling in sufficient detail that their results can be reproduced by others.\n:::\n\nIt is normal practice in psychology to report the p-values associated with null hypothesis significance tests of effects when reporting analysis. Performing hypothesis tests using t- or F-distributions depends on the calculation of degrees of freedom yet it is uncertain how degrees of freedom should be counted when analyzing multilevel data (@baayen2008). In most software applications, however, p-values associated with fixed effects may be calculated using an approximation for denominator degrees of freedom.\n\nWe will come back to how we should report the results of mixed-effects models because, here, too, in learning about writing, we can benefit by developing our approach, in depth, step by step.\n\n## Conclusions {#sec-intro-mixed-conclusions}\n\nA large proportion of psychological studies involves scenarios in which the researcher samples both participants and some kind of stimuli. Often, the researcher will present the stimuli to the participants for response in some version of a range of possible designs:\n\n-   all participants see and respond to all stimuli;\n-   participants respond to different sub-sets of stimuli in different conditions (or in different groups) but they see and respond to all stimuli in a sub-set;\n-   participants are allocated to respond to stimulus sub-sets according to a counter balancing scheme (e.g., through the use of Latin squares).\n\nWhatever version of this scenario, *if* participants are responding to multiple stimuli and *if* multiple partcipants respond to each stimulus, then the data will have a multilevel structure such that each observation can be grouped both by participant and by stimulus.\n\nWe are interested in taking into account the random effects associated with unexplained or random differences between participants or between stimuli. We often discuss the accounting of these effects in terms of the estimation of error variances associated with the random differences, calling the effects of the differences *random effects*. Where we have to deal with both samples of participants and samples of stimuli, we can talk about *crossed random effects*.\n\nThe terms are not that important. The insight is.\n\n::: callout-important\nIn general, in experimental psychological science, when we do data analysis, if we want to estimate effects of experimental variables more accurately then our models need to incorporate terms to capture the impact on observed outcomes of sampled participants and sampled stimuli.\n:::\n\nHistorically, we have, as a field, learned to take into account these sampling effects. Now, and most likely, more and more commonly in the future, we are learning to use multilevel or mixed-effects models to do this.\n\n### Summary {#sec-intro-mixed-summary}\n\n#### Summary: concepts {#sec-intro-mixed-summary-concepts}\n\nWe discussed the way that data are structured when they come from studies with repeated measures designs. Critically, we examined data from a common study design where a sample of stimulus items are presented for response to members of a participant sample. This means that each observation can be grouped by participant and, also, by stimulus. The possibility that observations can be grouped means that the data have a multilevel structure.\n\nThe multilevel structure requires the use of linear mixed-effects models when we seek to estimate the effects of experimental variables. The fact that data can be grouped both by participant and by stimulus means that the model can incorporate random effects to capture random between-participant differences as well as between-stimulus differences.\n\nThe use of mixed-effects models has meant that psychologists no longer need to adopt compromise solutions which have important limitations, like by-items and by-subjects analyses.\n\n#### Summary: skills {#sec-intro-mixed-summary-skills}\n\nWe reviewed the ways that experimental data can be untidy. And we outlined the steps that may be required to process untidy data into a tidy format suitable for analysis. As is typical for the data analysis we need to do for experimental psychological science, getting data ready for analysis requires a series of steps including: access; import; restructure; select variables; and filter observations.\n\nWe then developed a mixed-effects model to answer the question:\n\n::: callout-note\n-   Research question: What word properties influence responses to words in a test of reading aloud?\n:::\n\nOur analysis focused on the relationship between reading response reaction time (RT, in ms) and the predictor word frequency. We examined how the effect of word frequency was estimated in a linear model ignoring the multilevel structure and then in mixed-effects models which incorporated terms to capture:\n\n-   variance associated with random differences between participants in intercepts or in the slope of the frequency effect,\n-   variance associated with random differences between items in intercepts.\n\nWe saw that estimates of the frequency effect differed between different models.\n\n### Glossary: useful functions {#sec-intro-mixed-glossary-useful-functions}\n\nWe used a number of functions to tidy and visualize the CP study data.\n\n-   `read_csv()` and `read_csv()` to load source data files into the R workspace\n-   `pivot_longer()` to restructure data from wide to long\n-   `full_join()` to put together data from separate datasets; in our example, from datasets holding information about participant attributes, stimulus word properties, and participant behaviours\n-   `select()` to select the variables we need\n-   `filter()` to filter observations based on conditions\n-   `na.omit()` to remove missing values\n-   For visualisation, we used `facet_wrap()` to show plots of the relationship between outcome and predictor variables separately for different groups (by participant, or by item)\n\nFor our analyses:\n\n-   We used `lmer()` to fit a multilevel model.\n\nWe used the `summary()` function to get model results for both linear models and for the mulilevel or liner mixed-effects model.\n\n## Recommended reading {#sec-intro-mixed-recommended-reading}\n\n[@baayen2008; see also @Barr2013a; @judd2012] discuss mixed-effects models with crossed random effects in a variety of contexts in psychological science. The explanations are clear and the examples are often helpful.\n\nI wrote a tutorial article on mixed-effects models with Lotte Meteyard [@meteyard2020a]. We discuss how important the approach now is for psychological science, what researchers worry about when they use it, and what they should do and report when they use the method.\n",
    "supporting": [
      "Week17_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}